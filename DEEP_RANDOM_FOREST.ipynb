{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DEEP-RANDOM-FOREST.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "aLBMRPjGdMKc",
        "SUYZs2YFdb2I",
        "sSSZTXYW-eEc"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/filippotessaro/deep-neural-decision-forest/blob/master/DEEP_RANDOM_FOREST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58nhvUY6eiKH",
        "colab_type": "text"
      },
      "source": [
        "# Deep Neural Decision Forest"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gcM24wD6emvk",
        "colab_type": "text"
      },
      "source": [
        "This is the code for the Deep Learning Project (2019). Emotion recognition has been intesively exploited in the most recent years, due to the low-cost technologies involved and the interest derived from different fields of research, like psychology, psychiatry, neuroscience and computer science in general. Even if the interest in the topic had been constantly growing, the available datasets are still a small number. For this research we decided to test our network on the RAVDESS dataset, which contains audio and video data coming from 24 authors who emulate different emotions. \n",
        "\n",
        "Our problem is doing an audio emotion recognition using a particular kind of architecture: the deep random forest. This architecture has been tested on the MNIST dataset and on other image dataset while here we apply the network on audio data, which are converted in coefficient through an MFCC and then processed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NTFMzOTLfkMe",
        "colab_type": "text"
      },
      "source": [
        "This project follows the guide lines of the paper: [ Deep Neural Decision Forests](https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Kontschieder_Deep_Neural_Decision_ICCV_2015_paper.pdf).\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aLBMRPjGdMKc",
        "colab_type": "text"
      },
      "source": [
        "## Dataset Preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "97nC8ndfdnb4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import librosa\n",
        "import librosa.display\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.pyplot import specgram\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8SNuRFiQdK2v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "68TtYiRsdVXW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir dataset\n",
        "!cp \"gdrive/My Drive/RAVDESS/RAVDESS_DEFINITIVE_EDITION.zip\" dataset/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B5ru1I0sdVJD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!unzip dataset/RAVDESS_DEFINITIVE_EDITION.zip\n",
        "mylist= os.listdir('RAVDESS_DEFINITIVE_EDITION/')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SUYZs2YFdb2I",
        "colab_type": "text"
      },
      "source": [
        "### Creating Training + Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vVlJe2ZRdbBQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import librosa\n",
        "from librosa import display\n",
        "import time\n",
        "\n",
        "path = 'RAVDESS_DEFINITIVE_EDITION/'\n",
        "lst = []\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "for subdir, dirs, files in os.walk(path):\n",
        "  for file in files:\n",
        "      try:\n",
        "        print(file)\n",
        "        #Load librosa array, obtain mfcss, store the file and the mcss information in a new array\n",
        "        X, sample_rate = librosa.load(os.path.join(subdir,file), res_type='kaiser_fast')\n",
        "        mfccs = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=40).T,axis=0) \n",
        "        # The instruction below converts the labels (from 1 to 8) to a series from 0 to 7\n",
        "        # This is because our predictor needs to start from 0 otherwise it will try to predict also 0.\n",
        "        file = int(file[7:8]) - 1  \n",
        "        arr = mfccs, file\n",
        "        lst.append(arr)\n",
        "      # If the file is not valid, skip it\n",
        "      except ValueError:\n",
        "        continue\n",
        "\n",
        "print(\"--- Data loaded. Loading time: %s seconds ---\" % (time.time() - start_time))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XIlBGNc-dlsq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Creating X and y: zip makes a list of all the first elements, and a list of all the second elements.\n",
        "X, y = zip(*lst)\n",
        "\n",
        "import numpy as np\n",
        "X = np.asarray(X)\n",
        "y = np.asarray(y)\n",
        "\n",
        "\n",
        "X.shape, y.shape\n",
        "\n",
        "import joblib\n",
        "\n",
        "X_name = 'X.joblib'\n",
        "y_name = 'y.joblib'\n",
        "save_dir = '/content/drive/My Drive/Ravdess_model'\n",
        "save_dir = 'gdrive/My Drive/RAVDESS/'\n",
        "\n",
        "savedX = joblib.dump(X, os.path.join(save_dir, X_name))\n",
        "savedy = joblib.dump(y, os.path.join(save_dir, y_name))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sSSZTXYW-eEc",
        "colab_type": "text"
      },
      "source": [
        "## Plotting the audio file's waveform and its spectrogram"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_GPar4nk-P0k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Load libraries\n",
        "import librosa\n",
        "import librosa.display\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.pyplot import specgram\n",
        "from sklearn.metrics import confusion_matrix\n",
        "% pylab inline\n",
        "import pandas as pd\n",
        "import glob "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HCq6jBUt-tPN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#file audio loading with librosa\n",
        "data, sampling_rate = librosa.load('RawData/f11 (01).wav')\n",
        "\n",
        "plt.figure(figsize=(15, 5))\n",
        "librosa.display.waveplot(data, sr=sampling_rate)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ylxTtPXc_BM6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import scipy.io.wavfile\n",
        "import sys\n",
        "\n",
        "sr,x = scipy.io.wavfile.read('RawData/f10 (01).wav')\n",
        "\n",
        "## Parameters: 10ms step, 30ms window\n",
        "nstep = int(sr * 0.01)\n",
        "nwin  = int(sr * 0.03)\n",
        "nfft = nwin\n",
        "\n",
        "window = np.hamming(nwin)\n",
        "\n",
        "## will take windows x[n1:n2].  generate\n",
        "## and loop over n2 such that all frames\n",
        "## fit within the waveform\n",
        "nn = range(nwin, len(x), nstep)\n",
        "\n",
        "X = np.zeros( (len(nn), nfft//2) )\n",
        "\n",
        "for i,n in enumerate(nn):\n",
        "    xseg = x[n-nwin:n]\n",
        "    z = np.fft.fft(window * xseg, nfft)\n",
        "    X[i,:] = np.log(np.abs(z[:nfft//2]))\n",
        "\n",
        "plt.imshow(X.T, interpolation='nearest',\n",
        "    origin='lower',\n",
        "    aspect='auto')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNOgZZZW_QK6",
        "colab_type": "text"
      },
      "source": [
        "## Getting Data from joblib files\n",
        "\n",
        "Joblib is a powerful Python package for management of computation: parallel computing, caching, and primitives for out-of-core computing. It is handy when working on so called big data, that can consume more than the available RAM (several GB nowadays). In such situations, objects in the working space must be persisted to disk, for out-of-core computing, distribution of jobs, or caching.\n",
        "\n",
        "For data management, joblib provides transparent disk persistence that is very efficient with such objects. The internal mechanism relies on specializing pickle to handle better numpy arrays.\n",
        "\n",
        "On this project, we have decided to store our features and our output labels in two distinct joblib files. The main reason in doing this, is for the fact that with this file format we have reduced the loading time of the audio files."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yQSkaOf-AY6l",
        "colab_type": "code",
        "outputId": "a313abde-5f59-4a2c-e51d-7924ca2d9434",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "#PROVA NUOVO DATASET\n",
        "import joblib\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "from tensorboardcolab import TensorBoardColab\n",
        "\n",
        "# Loading saved models\n",
        "X = joblib.load('X.joblib')\n",
        "print(len(X))\n",
        "y = joblib.load('y.joblib')\n",
        "print(len(y))\n",
        "\n",
        "\n",
        "#Add new dataset\n",
        "'''X_Sentelli = joblib.load('X_Sentelli.joblib')\n",
        "y_Sentelli = joblib.load('y_Sentelli.joblib')\n",
        "\n",
        "print(len(y_Sentelli))# 4904 + 9237\n",
        "\n",
        "X_new = np.append(X, X_Sentelli, axis=0)\n",
        "print(len(X_new))\n",
        "y_new = np.append (y, y_Sentelli, axis=0)'''\n",
        "\n",
        "#Train 70 - Test 30 random splitting\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4904\n",
            "4904\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bt1rqj5uYroo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''#Expanding dimensions (for possible CNN)\n",
        "X_train = np.expand_dims(X_train, axis=2)\n",
        "X_test = np.expand_dims(X_test, axis=2)'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kIpzGNqJAigo",
        "colab_type": "text"
      },
      "source": [
        "## Simple Sklearn Random Forest Implementation\n",
        "Our target is to beat the accuracy of this model with a deep neural network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mefb4apfA7Bg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sklearn\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "def random_forest_classifier(features, target):\n",
        "    \"\"\"\n",
        "    To train the random forest classifier with features and target data\n",
        "    :param features:\n",
        "    :param target:\n",
        "    :return: trained random forest classifier\n",
        "    \"\"\"\n",
        "    clf = RandomForestClassifier()\n",
        "    clf.fit(features, target)\n",
        "    return clf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbLre3FyBB4E",
        "colab_type": "text"
      },
      "source": [
        "Train and Test the random forest model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V7SKRRgXBABx",
        "colab_type": "code",
        "outputId": "babc2ec6-8c6a-44e8-9097-be9a77a36047",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 262
        }
      },
      "source": [
        "# Create random forest classifier instance\n",
        "trained_model = random_forest_classifier(X_train, y_train)\n",
        "predictions = trained_model.predict(X_test)\n",
        "\n",
        "print(\"Train Accuracy :: \", accuracy_score(y_train, trained_model.predict(X_train)))\n",
        "print(\"Test Accuracy  :: \", accuracy_score(y_test, predictions))\n",
        "print(\" Confusion matrix \\n\", confusion_matrix(y_test, predictions))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
            "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Accuracy ::  0.9990867579908675\n",
            "Test Accuracy  ::  0.8567016676961087\n",
            " Confusion matrix \n",
            " [[117   4   0   4   2   0   0   0]\n",
            " [  9 220   2   4   0   1   1   0]\n",
            " [  1  16 221   2   6   5   7   5]\n",
            " [  5  14   6 199   1  10   5   4]\n",
            " [  0   2   5   5 228   0   0   0]\n",
            " [  6   7   9  17   6 208   2   1]\n",
            " [  5   3   6   4   2   2  92   4]\n",
            " [  3   3   7   3   9   1   6 102]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cP8NuotvBQ3s",
        "colab_type": "text"
      },
      "source": [
        "Target Results:\n",
        "\n",
        "*   Train Accuracy :  0.99\n",
        "*   Test Accuracy  :  0.87\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Bzv9jv-BmOp",
        "colab_type": "text"
      },
      "source": [
        "## Deep Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N3EcQy8VDVKT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from numpy import genfromtxt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.datasets as dsets\n",
        "import torchvision.transforms as transforms\n",
        "from torch.autograd import Variable\n",
        "import torch.utils.data as Data\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from torch.nn.parameter import Parameter\n",
        "import torch.nn.functional as F\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WInE1L2YDk6M",
        "colab_type": "text"
      },
      "source": [
        "### Network Model Definition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S6fWsv6GDU5R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DeepNeuralDecisionForest(nn.Module):\n",
        "    def __init__(self, p_keep_conv, p_keep_hidden, n_leaf, n_label, n_tree, n_depth, batch_size):\n",
        "        super(DeepNeuralDecisionForest, self).__init__()\n",
        "\n",
        "        #Case CNN as input to Deep Random Forest\n",
        "        '''self.conv = nn.Sequential()\n",
        "        self.conv.add_module('conv1', nn.Conv1d(in_channels = 40,out_channels = 64,kernel_size = 1,stride = 1))\n",
        "        self.conv.add_module('relu1', nn.ReLU())\n",
        "        self.conv.add_module('pool1', nn.MaxPool1d(kernel_size=5))\n",
        "        self.conv.add_module('drop1', nn.Dropout(1-p_keep_conv))\n",
        "        self.conv.add_module('conv2', nn.Conv1d(64, 128, kernel_size=1))\n",
        "        self.conv.add_module('relu2', nn.ReLU())\n",
        "        self.conv.add_module('pool2', nn.BatchNorm1d(128))\n",
        "        self.conv.add_module('drop2', nn.Dropout(1-p_keep_conv))'''\n",
        "        \n",
        "        self.fc1 = nn.Sequential(\n",
        "\t\t\t   nn.Linear(40, 64),\n",
        "\t\t\t   nn.ReLU(),\n",
        "         nn.BatchNorm1d(64),\n",
        "         nn.Dropout(1-p_keep_conv))\n",
        "\n",
        "        self._nleaf = n_leaf\n",
        "        self._nlabel = n_label\n",
        "        self._ntree = n_tree\n",
        "        self._ndepth = n_depth\n",
        "        self._batchsize = batch_size\n",
        "\n",
        "        self.treelayers = nn.ModuleList()\n",
        "        self.pi_e = nn.ParameterList()\n",
        "        for i in range(self._ntree):\n",
        "            treelayer = nn.Sequential()\n",
        "            treelayer.add_module('sub_linear1', nn.Linear(64, 128))\n",
        "            treelayer.add_module('sub_relu', nn.ReLU())\n",
        "            treelayer.add_module('sub_drop1', nn.Dropout(1-p_keep_hidden))\n",
        "            treelayer.add_module('sub_batchNorm', nn.BatchNorm1d(128))\n",
        "            treelayer.add_module('sub_linear2', nn.Linear(128, self._nleaf))\n",
        "            treelayer.add_module('sub_sigmoid', nn.Sigmoid())\n",
        "            \n",
        "            self.treelayers.append(treelayer)\n",
        "            self.pi_e.append(Parameter(self.init_prob_weights([self._nleaf, self._nlabel], -2, 2)))\n",
        "\n",
        "    def init_pi(self):\n",
        "        return torch.ones(self._nleaf, self._nlabel)/float(self._nlabel)\n",
        "\n",
        "    def init_weights(self, shape):\n",
        "        return torch.randn(shape).uniform(-0.01,0.01)\n",
        "\n",
        "    def init_prob_weights(self, shape, minval=-5, maxval=5):\n",
        "        return torch.Tensor(shape[0], shape[1]).uniform_(minval, maxval)\n",
        "\n",
        "    def compute_mu(self, flat_decision_p_e):\n",
        "        n_batch = self._batchsize\n",
        "        batch_0_indices = torch.range(0, n_batch * self._nleaf - 1, self._nleaf).unsqueeze(1).repeat(1, self._nleaf).long()\n",
        "\n",
        "        in_repeat = self._nleaf // 2\n",
        "        out_repeat = n_batch\n",
        "\n",
        "        batch_complement_indices = torch.LongTensor(\n",
        "            np.array([[0] * in_repeat, [n_batch * self._nleaf] * in_repeat] * out_repeat).reshape(n_batch, self._nleaf))\n",
        "\n",
        "        # First define the routing probabilistics d for root nodes\n",
        "        mu_e = []\n",
        "        indices_var = Variable((batch_0_indices + batch_complement_indices).view(-1)) \n",
        "        indices_var = indices_var.cuda()\n",
        "        # iterate over each tree\n",
        "        for i, flat_decision_p in enumerate(flat_decision_p_e):\n",
        "            mu = torch.gather(flat_decision_p, 0, indices_var).view(n_batch, self._nleaf)\n",
        "            mu_e.append(mu)\n",
        "\n",
        "        # from the scond layer to the last layer, we make the decison nodes\n",
        "        for d in range(1, self._ndepth + 1):\n",
        "            indices = torch.range(2 ** d, 2 ** (d + 1) - 1) - 1\n",
        "            tile_indices = indices.unsqueeze(1).repeat(1, 2 ** (self._ndepth - d + 1)).view(1, -1)\n",
        "            batch_indices = batch_0_indices + tile_indices.repeat(n_batch, 1).long()\n",
        "\n",
        "            in_repeat = in_repeat // 2\n",
        "            out_repeat = out_repeat * 2\n",
        "            # Again define the indices that picks d and 1-d for the nodes\n",
        "            batch_complement_indices = torch.LongTensor(\n",
        "                np.array([[0] * in_repeat, [n_batch * self._nleaf] * in_repeat] * out_repeat).reshape(n_batch, self._nleaf))\n",
        "\n",
        "            mu_e_update = []\n",
        "            indices_var = Variable((batch_indices + batch_complement_indices).view(-1))\n",
        "            indices_var = indices_var.cuda()\n",
        "            for mu, flat_decision_p in zip(mu_e, flat_decision_p_e):\n",
        "                mu = torch.mul(mu, torch.gather(flat_decision_p, 0, indices_var).view(\n",
        "                    n_batch, self._nleaf))\n",
        "                mu_e_update.append(mu)\n",
        "            mu_e = mu_e_update\n",
        "        return mu_e\n",
        "\n",
        "    def compute_py_x(self, mu_e, leaf_p_e):\n",
        "        py_x_e = []\n",
        "        n_batch = self._batchsize\n",
        "\n",
        "        for i in range(len(mu_e)):\n",
        "            py_x_tree = mu_e[i].unsqueeze(2).repeat(1, 1, self._nlabel).mul(leaf_p_e[i].unsqueeze(0).repeat(n_batch, 1, 1)).mean(1)\n",
        "            py_x_e.append(py_x_tree.squeeze().unsqueeze(0))\n",
        "\n",
        "        py_x_e = torch.cat(py_x_e, 0)\n",
        "        py_x = py_x_e.mean(0).squeeze()\n",
        "        \n",
        "        return py_x\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 40)#DELETE\n",
        "        feat = self.fc1.forward(x)#DELETE\n",
        "        #feat = self.conv.forward(x)\n",
        "        #print('out 8', feat.shape)\n",
        "        \n",
        "        feat = feat.view(-1, 64)\n",
        "        self._batchsize = x.size(0)\n",
        "\n",
        "        flat_decision_p_e = []\n",
        "        leaf_p_e = []\n",
        "        \n",
        "        for i in range(len(self.treelayers)):\n",
        "            decision_p = self.treelayers[i].forward(feat)\n",
        "            decision_p_comp = 1 - decision_p\n",
        "            decision_p_pack = torch.cat((decision_p, decision_p_comp))\n",
        "            flat_decision_p = decision_p_pack.view(-1)\n",
        "            flat_decision_p_e.append(flat_decision_p)\n",
        "            leaf_p = F.softmax(self.pi_e[i])\n",
        "            leaf_p_e.append(leaf_p)\n",
        "        \n",
        "        mu_e = self.compute_mu(flat_decision_p_e)\n",
        "        \n",
        "        py_x = self.compute_py_x(mu_e, leaf_p_e)\n",
        "        return torch.log(py_x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2edT-L4xDUNA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(net,data_loader,optimizer,cost_function, device='cuda'):\n",
        "  samples = 0.\n",
        "  cumulative_loss = 0.\n",
        "  cumulative_accuracy = 0.\n",
        "  \n",
        "  # Set the network in train mode\n",
        "  net.train()\n",
        "  \n",
        "  # Loop over the dataset\n",
        "  for batch_idx, (inputs, targets) in enumerate(data_loader):\n",
        "    # Load data into GPU\n",
        "    inputs = inputs.to(device)\n",
        "    \n",
        "    targets = targets.to(device)\n",
        "    \n",
        "    # Forward pass\n",
        "    outputs = net(inputs)\n",
        "\n",
        "    # Apply the loss\n",
        "    loss = cost_function(outputs, targets)\n",
        "      \n",
        "    # Backward pass\n",
        "    loss.backward()\n",
        "    \n",
        "    # Update parameters\n",
        "    optimizer.step()\n",
        "    \n",
        "    # Reset the optimizer\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Better print something, no?\n",
        "    samples+=inputs.shape[0]\n",
        "    cumulative_loss += loss.item()\n",
        "    _, predicted = outputs.max(1)\n",
        "    cumulative_accuracy += predicted.eq(targets).sum().item()\n",
        "\n",
        "  return cumulative_loss/samples, cumulative_accuracy/samples*100\n",
        "\n",
        "\n",
        "def test(net, data_loader, cost_function, device='cuda'):\n",
        "  samples = 0.\n",
        "  cumulative_loss = 0.\n",
        "  cumulative_accuracy = 0.\n",
        "\n",
        "  #Set the network in eval mode\n",
        "  net.eval()\n",
        "  with torch.no_grad(): # torch.no_grad() disables the autograd machinery, thus not saving the intermediate activations\n",
        "    # Loop over the dataset\n",
        "    for batch_idx, (inputs, targets) in enumerate(data_loader):\n",
        "      # Load data into GPU\n",
        "      inputs = inputs.to(device)\n",
        "\n",
        "      targets = targets.to(device)\n",
        "\n",
        "      # Forward pass\n",
        "      outputs = net(inputs)\n",
        "\n",
        "      # Apply the loss\n",
        "      loss = cost_function(outputs, targets)\n",
        "\n",
        "      # Better print something\n",
        "      samples+=inputs.shape[0]\n",
        "      cumulative_loss += loss.item() # Note: the .item() is needed to extract scalars from tensors\n",
        "      _, predicted = outputs.max(1)\n",
        "      cumulative_accuracy += predicted.eq(targets).sum().item()\n",
        "\n",
        "  return cumulative_loss/samples, cumulative_accuracy/samples*100\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aqxGk1uWDxP1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_cost_function():\n",
        "  cost_function = torch.nn.CrossEntropyLoss()\n",
        "  return cost_function\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pxi8mQ8jDxDg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_optimizer(net, lr, wd, momentum):\n",
        "  optimizer = torch.optim.Adam(net.parameters(), lr=lr)#, betas=(0.9, 0.999), eps=1e-08, amsgrad=False)\n",
        "  return optimizer\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "20PzWbGJDuOe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_data(batch_size, test_batch_size=128): \n",
        "  # Load data\n",
        "  #X_train, X_test, y_train, y_test\n",
        "  train_data = Data.TensorDataset(\n",
        "    torch.from_numpy(X_train).float(), \n",
        "    torch.from_numpy(y_train).long())\n",
        "\n",
        "  test_data = Data.TensorDataset(\n",
        "    torch.from_numpy(X_test).float(), \n",
        "    torch.from_numpy(y_test).long())\n",
        "\n",
        "  # Initialize dataloaders\n",
        "  train_loader = torch.utils.data.DataLoader(train_data, batch_size = batch_size, shuffle = True)\n",
        "  test_loader = torch.utils.data.DataLoader(test_data, batch_size = batch_size, shuffle = False)\n",
        "  \n",
        "  return train_loader, test_loader\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UrU8UUZDBqdA",
        "colab_type": "code",
        "outputId": "1ad328b7-d5a8-42f0-b67b-11210e8714fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Instantiate visualizer\n",
        "tb = TensorBoardColab(graph_path='./log')\n",
        "visualization_name='Deep Random Forest'\n",
        "\n",
        "device='cuda:0'\n",
        "momentum = 0.95\n",
        "learning_rate = 0.001 \n",
        "num_epochs = 1000\n",
        "weight_decay = 1e-5\n",
        "\n",
        "DEPTH = 3  # Depth of a tree\n",
        "N_LEAF = 2 ** (DEPTH + 1)  # Number of leaf node\n",
        "N_LABEL = 8  # Number of classes\n",
        "N_TREE = 3  # Number of trees\n",
        "batch_size = 128\n",
        "\n",
        "# Dropout Network hyperparameters\n",
        "p_conv_keep = 0.9\n",
        "p_full_keep = 0.6\n",
        "\n",
        "    \n",
        "#DataLoader creation\n",
        "train_loader, test_loader = get_data(batch_size)\n",
        "\n",
        "#Network initialization\n",
        "net = DeepNeuralDecisionForest(p_keep_conv = p_conv_keep, p_keep_hidden = p_full_keep, n_leaf= N_LEAF, n_label= N_LABEL, n_tree= N_TREE, n_depth= DEPTH, batch_size=batch_size)\n",
        "net.to(device)\n",
        "\n",
        "optimizer = get_optimizer(net, learning_rate, weight_decay, momentum)\n",
        "cost_function = get_cost_function()\n",
        "\n",
        "train_accuracy_list = list()\n",
        "test_accuracy_list = list()\n",
        "\n",
        "train_loss_list = list()\n",
        "test_loss_list = list()\n",
        "\n",
        "print('Before training:')\n",
        "train_loss, train_accuracy = test(net, train_loader, cost_function)\n",
        "test_loss, test_accuracy = test(net, test_loader, cost_function)\n",
        "train_accuracy_list.append(train_accuracy)\n",
        "test_accuracy_list.append(test_accuracy)\n",
        "train_loss_list.append(train_loss)\n",
        "test_loss_list.append(test_loss)\n",
        "\n",
        "\n",
        "print('\\t Training loss {:.5f}, Training accuracy {:.2f}'.format(train_loss, train_accuracy))\n",
        "print('\\t Test loss {:.5f}, Test accuracy {:.2f}'.format(test_loss, test_accuracy))\n",
        "print('-----------------------------------------------------')\n",
        "\n",
        "\n",
        "# Add values to plots\n",
        "tb.save_value('Loss/train_loss', visualization_name, 0, train_loss)\n",
        "tb.save_value('Loss/test_loss', visualization_name, 0, test_loss)\n",
        "tb.save_value('Accuracy/train_accuracy', visualization_name, 0, train_accuracy)\n",
        "tb.save_value('Accuracy/test_accuracy', visualization_name, 0, test_accuracy)\n",
        "\n",
        "# Update plots \n",
        "tb.flush_line(visualization_name)\n",
        "\n",
        "for e in range(num_epochs):\n",
        "  train_loss, train_accuracy = train(net, train_loader, optimizer, cost_function)\n",
        "  val_loss, val_accuracy = test(net, test_loader, cost_function)\n",
        "  print('Epoch: {:d}'.format(e+1))\n",
        "  print('\\t Training loss {:.5f}, Training accuracy {:.2f}'.format(train_loss, train_accuracy))\n",
        "  print('\\t Test loss {:.5f}, Test accuracy {:.2f}'.format(val_loss, val_accuracy))\n",
        "  print('-----------------------------------------------------')\n",
        "  \n",
        "  train_accuracy_list.append(train_accuracy)\n",
        "  test_accuracy_list.append(val_accuracy)\n",
        "  train_loss_list.append(train_loss)\n",
        "  test_loss_list.append(val_loss)\n",
        "  \n",
        "  # Add values to plots\n",
        "  tb.save_value('Loss/train_loss', visualization_name, e + 1, train_loss)\n",
        "  tb.save_value('Loss/test_loss', visualization_name, e + 1, val_loss)\n",
        "  tb.save_value('Accuracy/train_accuracy', visualization_name, e + 1, train_accuracy)\n",
        "  tb.save_value('Accuracy/test_accuracy', visualization_name, e + 1, val_accuracy)\n",
        "  # Update plots \n",
        "  tb.flush_line(visualization_name)\n",
        "  \n",
        "print('After training:')\n",
        "train_loss, train_accuracy = test(net, train_loader, cost_function)\n",
        "test_loss, test_accuracy = test(net, test_loader, cost_function)\n",
        "\n",
        "print('\\t Training loss {:.5f}, Training accuracy {:.2f}'.format(train_loss, train_accuracy))\n",
        "print('\\t Test loss {:.5f}, Test accuracy {:.2f}'.format(test_loss, test_accuracy))\n",
        "print('-----------------------------------------------------')"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wait for 8 seconds...\n",
            "TensorBoard link:\n",
            "https://d1356e1d.ngrok.io\n",
            "Before training:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:123: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:53: UserWarning: torch.range is deprecated in favor of torch.arange and will be removed in 0.5. Note that arange generates values in [start; end), not [start; end].\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:72: UserWarning: torch.range is deprecated in favor of torch.arange and will be removed in 0.5. Note that arange generates values in [start; end), not [start; end].\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\t Training loss 0.01694, Training accuracy 15.10\n",
            "\t Test loss 0.01705, Test accuracy 15.81\n",
            "-----------------------------------------------------\n",
            "Epoch: 1\n",
            "\t Training loss 0.01614, Training accuracy 19.36\n",
            "\t Test loss 0.01605, Test accuracy 19.64\n",
            "-----------------------------------------------------\n",
            "Epoch: 2\n",
            "\t Training loss 0.01519, Training accuracy 23.14\n",
            "\t Test loss 0.01524, Test accuracy 22.30\n",
            "-----------------------------------------------------\n",
            "Epoch: 3\n",
            "\t Training loss 0.01447, Training accuracy 31.39\n",
            "\t Test loss 0.01480, Test accuracy 26.74\n",
            "-----------------------------------------------------\n",
            "Epoch: 4\n",
            "\t Training loss 0.01388, Training accuracy 37.02\n",
            "\t Test loss 0.01434, Test accuracy 30.39\n",
            "-----------------------------------------------------\n",
            "Epoch: 5\n",
            "\t Training loss 0.01336, Training accuracy 40.91\n",
            "\t Test loss 0.01348, Test accuracy 40.95\n",
            "-----------------------------------------------------\n",
            "Epoch: 6\n",
            "\t Training loss 0.01295, Training accuracy 43.56\n",
            "\t Test loss 0.01297, Test accuracy 44.47\n",
            "-----------------------------------------------------\n",
            "Epoch: 7\n",
            "\t Training loss 0.01262, Training accuracy 45.48\n",
            "\t Test loss 0.01285, Test accuracy 45.95\n",
            "-----------------------------------------------------\n",
            "Epoch: 8\n",
            "\t Training loss 0.01232, Training accuracy 46.82\n",
            "\t Test loss 0.01241, Test accuracy 50.46\n",
            "-----------------------------------------------------\n",
            "Epoch: 9\n",
            "\t Training loss 0.01203, Training accuracy 50.41\n",
            "\t Test loss 0.01250, Test accuracy 45.15\n",
            "-----------------------------------------------------\n",
            "Epoch: 10\n",
            "\t Training loss 0.01180, Training accuracy 51.14\n",
            "\t Test loss 0.01189, Test accuracy 52.87\n",
            "-----------------------------------------------------\n",
            "Epoch: 11\n",
            "\t Training loss 0.01157, Training accuracy 52.02\n",
            "\t Test loss 0.01168, Test accuracy 53.24\n",
            "-----------------------------------------------------\n",
            "Epoch: 12\n",
            "\t Training loss 0.01128, Training accuracy 55.92\n",
            "\t Test loss 0.01142, Test accuracy 54.97\n",
            "-----------------------------------------------------\n",
            "Epoch: 13\n",
            "\t Training loss 0.01110, Training accuracy 56.07\n",
            "\t Test loss 0.01151, Test accuracy 54.91\n",
            "-----------------------------------------------------\n",
            "Epoch: 14\n",
            "\t Training loss 0.01095, Training accuracy 56.74\n",
            "\t Test loss 0.01142, Test accuracy 52.38\n",
            "-----------------------------------------------------\n",
            "Epoch: 15\n",
            "\t Training loss 0.01066, Training accuracy 59.51\n",
            "\t Test loss 0.01081, Test accuracy 59.23\n",
            "-----------------------------------------------------\n",
            "Epoch: 16\n",
            "\t Training loss 0.01055, Training accuracy 59.57\n",
            "\t Test loss 0.01077, Test accuracy 58.99\n",
            "-----------------------------------------------------\n",
            "Epoch: 17\n",
            "\t Training loss 0.01028, Training accuracy 61.74\n",
            "\t Test loss 0.01068, Test accuracy 58.31\n",
            "-----------------------------------------------------\n",
            "Epoch: 18\n",
            "\t Training loss 0.01015, Training accuracy 61.46\n",
            "\t Test loss 0.01048, Test accuracy 58.99\n",
            "-----------------------------------------------------\n",
            "Epoch: 19\n",
            "\t Training loss 0.00991, Training accuracy 62.44\n",
            "\t Test loss 0.01110, Test accuracy 52.32\n",
            "-----------------------------------------------------\n",
            "Epoch: 20\n",
            "\t Training loss 0.00983, Training accuracy 62.50\n",
            "\t Test loss 0.01013, Test accuracy 61.03\n",
            "-----------------------------------------------------\n",
            "Epoch: 21\n",
            "\t Training loss 0.00968, Training accuracy 63.47\n",
            "\t Test loss 0.00990, Test accuracy 61.77\n",
            "-----------------------------------------------------\n",
            "Epoch: 22\n",
            "\t Training loss 0.00951, Training accuracy 63.71\n",
            "\t Test loss 0.00980, Test accuracy 61.89\n",
            "-----------------------------------------------------\n",
            "Epoch: 23\n",
            "\t Training loss 0.00938, Training accuracy 64.90\n",
            "\t Test loss 0.00992, Test accuracy 60.53\n",
            "-----------------------------------------------------\n",
            "Epoch: 24\n",
            "\t Training loss 0.00927, Training accuracy 65.48\n",
            "\t Test loss 0.00943, Test accuracy 64.92\n",
            "-----------------------------------------------------\n",
            "Epoch: 25\n",
            "\t Training loss 0.00913, Training accuracy 65.18\n",
            "\t Test loss 0.00988, Test accuracy 60.41\n",
            "-----------------------------------------------------\n",
            "Epoch: 26\n",
            "\t Training loss 0.00903, Training accuracy 65.97\n",
            "\t Test loss 0.00973, Test accuracy 62.14\n",
            "-----------------------------------------------------\n",
            "Epoch: 27\n",
            "\t Training loss 0.00894, Training accuracy 66.27\n",
            "\t Test loss 0.00912, Test accuracy 63.80\n",
            "-----------------------------------------------------\n",
            "Epoch: 28\n",
            "\t Training loss 0.00880, Training accuracy 66.21\n",
            "\t Test loss 0.00892, Test accuracy 65.84\n",
            "-----------------------------------------------------\n",
            "Epoch: 29\n",
            "\t Training loss 0.00864, Training accuracy 67.25\n",
            "\t Test loss 0.00923, Test accuracy 62.88\n",
            "-----------------------------------------------------\n",
            "Epoch: 30\n",
            "\t Training loss 0.00854, Training accuracy 67.79\n",
            "\t Test loss 0.00885, Test accuracy 66.46\n",
            "-----------------------------------------------------\n",
            "Epoch: 31\n",
            "\t Training loss 0.00840, Training accuracy 68.65\n",
            "\t Test loss 0.00895, Test accuracy 65.16\n",
            "-----------------------------------------------------\n",
            "Epoch: 32\n",
            "\t Training loss 0.00829, Training accuracy 68.65\n",
            "\t Test loss 0.00884, Test accuracy 64.67\n",
            "-----------------------------------------------------\n",
            "Epoch: 33\n",
            "\t Training loss 0.00824, Training accuracy 68.80\n",
            "\t Test loss 0.00905, Test accuracy 64.11\n",
            "-----------------------------------------------------\n",
            "Epoch: 34\n",
            "\t Training loss 0.00827, Training accuracy 67.98\n",
            "\t Test loss 0.00862, Test accuracy 66.58\n",
            "-----------------------------------------------------\n",
            "Epoch: 35\n",
            "\t Training loss 0.00811, Training accuracy 69.28\n",
            "\t Test loss 0.00834, Test accuracy 68.75\n",
            "-----------------------------------------------------\n",
            "Epoch: 36\n",
            "\t Training loss 0.00806, Training accuracy 69.53\n",
            "\t Test loss 0.00904, Test accuracy 63.50\n",
            "-----------------------------------------------------\n",
            "Epoch: 37\n",
            "\t Training loss 0.00796, Training accuracy 69.25\n",
            "\t Test loss 0.00868, Test accuracy 65.72\n",
            "-----------------------------------------------------\n",
            "Epoch: 38\n",
            "\t Training loss 0.00784, Training accuracy 69.77\n",
            "\t Test loss 0.00831, Test accuracy 66.58\n",
            "-----------------------------------------------------\n",
            "Epoch: 39\n",
            "\t Training loss 0.00784, Training accuracy 69.74\n",
            "\t Test loss 0.00877, Test accuracy 64.30\n",
            "-----------------------------------------------------\n",
            "Epoch: 40\n",
            "\t Training loss 0.00773, Training accuracy 70.11\n",
            "\t Test loss 0.00810, Test accuracy 69.73\n",
            "-----------------------------------------------------\n",
            "Epoch: 41\n",
            "\t Training loss 0.00756, Training accuracy 70.47\n",
            "\t Test loss 0.00829, Test accuracy 66.83\n",
            "-----------------------------------------------------\n",
            "Epoch: 42\n",
            "\t Training loss 0.00748, Training accuracy 72.18\n",
            "\t Test loss 0.00792, Test accuracy 69.18\n",
            "-----------------------------------------------------\n",
            "Epoch: 43\n",
            "\t Training loss 0.00737, Training accuracy 71.99\n",
            "\t Test loss 0.00855, Test accuracy 64.24\n",
            "-----------------------------------------------------\n",
            "Epoch: 44\n",
            "\t Training loss 0.00731, Training accuracy 72.33\n",
            "\t Test loss 0.00790, Test accuracy 68.87\n",
            "-----------------------------------------------------\n",
            "Epoch: 45\n",
            "\t Training loss 0.00724, Training accuracy 72.66\n",
            "\t Test loss 0.00791, Test accuracy 67.82\n",
            "-----------------------------------------------------\n",
            "Epoch: 46\n",
            "\t Training loss 0.00710, Training accuracy 73.67\n",
            "\t Test loss 0.00807, Test accuracy 67.26\n",
            "-----------------------------------------------------\n",
            "Epoch: 47\n",
            "\t Training loss 0.00712, Training accuracy 72.82\n",
            "\t Test loss 0.00877, Test accuracy 60.84\n",
            "-----------------------------------------------------\n",
            "Epoch: 48\n",
            "\t Training loss 0.00712, Training accuracy 72.91\n",
            "\t Test loss 0.00754, Test accuracy 71.96\n",
            "-----------------------------------------------------\n",
            "Epoch: 49\n",
            "\t Training loss 0.00692, Training accuracy 73.76\n",
            "\t Test loss 0.00729, Test accuracy 73.13\n",
            "-----------------------------------------------------\n",
            "Epoch: 50\n",
            "\t Training loss 0.00686, Training accuracy 73.27\n",
            "\t Test loss 0.00750, Test accuracy 70.60\n",
            "-----------------------------------------------------\n",
            "Epoch: 51\n",
            "\t Training loss 0.00679, Training accuracy 74.19\n",
            "\t Test loss 0.00762, Test accuracy 68.93\n",
            "-----------------------------------------------------\n",
            "Epoch: 52\n",
            "\t Training loss 0.00667, Training accuracy 74.55\n",
            "\t Test loss 0.00762, Test accuracy 69.98\n",
            "-----------------------------------------------------\n",
            "Epoch: 53\n",
            "\t Training loss 0.00676, Training accuracy 74.34\n",
            "\t Test loss 0.00722, Test accuracy 72.14\n",
            "-----------------------------------------------------\n",
            "Epoch: 54\n",
            "\t Training loss 0.00659, Training accuracy 74.76\n",
            "\t Test loss 0.00736, Test accuracy 70.17\n",
            "-----------------------------------------------------\n",
            "Epoch: 55\n",
            "\t Training loss 0.00643, Training accuracy 75.95\n",
            "\t Test loss 0.00725, Test accuracy 73.07\n",
            "-----------------------------------------------------\n",
            "Epoch: 56\n",
            "\t Training loss 0.00659, Training accuracy 74.40\n",
            "\t Test loss 0.00731, Test accuracy 69.24\n",
            "-----------------------------------------------------\n",
            "Epoch: 57\n",
            "\t Training loss 0.00646, Training accuracy 75.43\n",
            "\t Test loss 0.00738, Test accuracy 71.53\n",
            "-----------------------------------------------------\n",
            "Epoch: 58\n",
            "\t Training loss 0.00646, Training accuracy 75.16\n",
            "\t Test loss 0.00721, Test accuracy 71.83\n",
            "-----------------------------------------------------\n",
            "Epoch: 59\n",
            "\t Training loss 0.00639, Training accuracy 74.89\n",
            "\t Test loss 0.00725, Test accuracy 72.51\n",
            "-----------------------------------------------------\n",
            "Epoch: 60\n",
            "\t Training loss 0.00636, Training accuracy 75.31\n",
            "\t Test loss 0.00800, Test accuracy 68.13\n",
            "-----------------------------------------------------\n",
            "Epoch: 61\n",
            "\t Training loss 0.00640, Training accuracy 75.77\n",
            "\t Test loss 0.00687, Test accuracy 73.26\n",
            "-----------------------------------------------------\n",
            "Epoch: 62\n",
            "\t Training loss 0.00638, Training accuracy 75.43\n",
            "\t Test loss 0.00710, Test accuracy 70.23\n",
            "-----------------------------------------------------\n",
            "Epoch: 63\n",
            "\t Training loss 0.00626, Training accuracy 76.04\n",
            "\t Test loss 0.00694, Test accuracy 73.26\n",
            "-----------------------------------------------------\n",
            "Epoch: 64\n",
            "\t Training loss 0.00614, Training accuracy 76.23\n",
            "\t Test loss 0.00707, Test accuracy 70.35\n",
            "-----------------------------------------------------\n",
            "Epoch: 65\n",
            "\t Training loss 0.00625, Training accuracy 75.37\n",
            "\t Test loss 0.00701, Test accuracy 71.16\n",
            "-----------------------------------------------------\n",
            "Epoch: 66\n",
            "\t Training loss 0.00608, Training accuracy 77.11\n",
            "\t Test loss 0.00785, Test accuracy 69.18\n",
            "-----------------------------------------------------\n",
            "Epoch: 67\n",
            "\t Training loss 0.00611, Training accuracy 75.77\n",
            "\t Test loss 0.00663, Test accuracy 73.32\n",
            "-----------------------------------------------------\n",
            "Epoch: 68\n",
            "\t Training loss 0.00592, Training accuracy 77.66\n",
            "\t Test loss 0.00685, Test accuracy 70.11\n",
            "-----------------------------------------------------\n",
            "Epoch: 69\n",
            "\t Training loss 0.00597, Training accuracy 77.63\n",
            "\t Test loss 0.00693, Test accuracy 73.63\n",
            "-----------------------------------------------------\n",
            "Epoch: 70\n",
            "\t Training loss 0.00595, Training accuracy 77.20\n",
            "\t Test loss 0.00683, Test accuracy 75.60\n",
            "-----------------------------------------------------\n",
            "Epoch: 71\n",
            "\t Training loss 0.00581, Training accuracy 77.99\n",
            "\t Test loss 0.00632, Test accuracy 75.42\n",
            "-----------------------------------------------------\n",
            "Epoch: 72\n",
            "\t Training loss 0.00585, Training accuracy 77.96\n",
            "\t Test loss 0.00623, Test accuracy 75.29\n",
            "-----------------------------------------------------\n",
            "Epoch: 73\n",
            "\t Training loss 0.00572, Training accuracy 77.72\n",
            "\t Test loss 0.00665, Test accuracy 72.88\n",
            "-----------------------------------------------------\n",
            "Epoch: 74\n",
            "\t Training loss 0.00574, Training accuracy 78.33\n",
            "\t Test loss 0.00631, Test accuracy 72.82\n",
            "-----------------------------------------------------\n",
            "Epoch: 75\n",
            "\t Training loss 0.00584, Training accuracy 77.66\n",
            "\t Test loss 0.00614, Test accuracy 75.54\n",
            "-----------------------------------------------------\n",
            "Epoch: 76\n",
            "\t Training loss 0.00561, Training accuracy 78.63\n",
            "\t Test loss 0.00654, Test accuracy 75.66\n",
            "-----------------------------------------------------\n",
            "Epoch: 77\n",
            "\t Training loss 0.00564, Training accuracy 78.81\n",
            "\t Test loss 0.00604, Test accuracy 76.84\n",
            "-----------------------------------------------------\n",
            "Epoch: 78\n",
            "\t Training loss 0.00556, Training accuracy 78.72\n",
            "\t Test loss 0.00636, Test accuracy 76.10\n",
            "-----------------------------------------------------\n",
            "Epoch: 79\n",
            "\t Training loss 0.00554, Training accuracy 79.27\n",
            "\t Test loss 0.00615, Test accuracy 77.27\n",
            "-----------------------------------------------------\n",
            "Epoch: 80\n",
            "\t Training loss 0.00557, Training accuracy 79.21\n",
            "\t Test loss 0.00631, Test accuracy 73.63\n",
            "-----------------------------------------------------\n",
            "Epoch: 81\n",
            "\t Training loss 0.00556, Training accuracy 78.60\n",
            "\t Test loss 0.00608, Test accuracy 76.10\n",
            "-----------------------------------------------------\n",
            "Epoch: 82\n",
            "\t Training loss 0.00543, Training accuracy 79.45\n",
            "\t Test loss 0.00634, Test accuracy 73.01\n",
            "-----------------------------------------------------\n",
            "Epoch: 83\n",
            "\t Training loss 0.00543, Training accuracy 79.79\n",
            "\t Test loss 0.00597, Test accuracy 76.03\n",
            "-----------------------------------------------------\n",
            "Epoch: 84\n",
            "\t Training loss 0.00545, Training accuracy 79.06\n",
            "\t Test loss 0.00608, Test accuracy 76.84\n",
            "-----------------------------------------------------\n",
            "Epoch: 85\n",
            "\t Training loss 0.00549, Training accuracy 79.00\n",
            "\t Test loss 0.00617, Test accuracy 76.03\n",
            "-----------------------------------------------------\n",
            "Epoch: 86\n",
            "\t Training loss 0.00546, Training accuracy 79.45\n",
            "\t Test loss 0.00599, Test accuracy 76.53\n",
            "-----------------------------------------------------\n",
            "Epoch: 87\n",
            "\t Training loss 0.00547, Training accuracy 78.51\n",
            "\t Test loss 0.00602, Test accuracy 75.79\n",
            "-----------------------------------------------------\n",
            "Epoch: 88\n",
            "\t Training loss 0.00546, Training accuracy 78.69\n",
            "\t Test loss 0.00631, Test accuracy 73.69\n",
            "-----------------------------------------------------\n",
            "Epoch: 89\n",
            "\t Training loss 0.00543, Training accuracy 78.17\n",
            "\t Test loss 0.00598, Test accuracy 76.28\n",
            "-----------------------------------------------------\n",
            "Epoch: 90\n",
            "\t Training loss 0.00539, Training accuracy 79.24\n",
            "\t Test loss 0.00643, Test accuracy 74.49\n",
            "-----------------------------------------------------\n",
            "Epoch: 91\n",
            "\t Training loss 0.00531, Training accuracy 80.21\n",
            "\t Test loss 0.00596, Test accuracy 76.28\n",
            "-----------------------------------------------------\n",
            "Epoch: 92\n",
            "\t Training loss 0.00530, Training accuracy 80.40\n",
            "\t Test loss 0.00595, Test accuracy 76.90\n",
            "-----------------------------------------------------\n",
            "Epoch: 93\n",
            "\t Training loss 0.00536, Training accuracy 79.60\n",
            "\t Test loss 0.00589, Test accuracy 76.03\n",
            "-----------------------------------------------------\n",
            "Epoch: 94\n",
            "\t Training loss 0.00520, Training accuracy 79.51\n",
            "\t Test loss 0.00635, Test accuracy 75.54\n",
            "-----------------------------------------------------\n",
            "Epoch: 95\n",
            "\t Training loss 0.00524, Training accuracy 80.33\n",
            "\t Test loss 0.00550, Test accuracy 78.51\n",
            "-----------------------------------------------------\n",
            "Epoch: 96\n",
            "\t Training loss 0.00519, Training accuracy 80.12\n",
            "\t Test loss 0.00604, Test accuracy 74.98\n",
            "-----------------------------------------------------\n",
            "Epoch: 97\n",
            "\t Training loss 0.00532, Training accuracy 79.82\n",
            "\t Test loss 0.00615, Test accuracy 75.05\n",
            "-----------------------------------------------------\n",
            "Epoch: 98\n",
            "\t Training loss 0.00520, Training accuracy 80.30\n",
            "\t Test loss 0.00589, Test accuracy 76.28\n",
            "-----------------------------------------------------\n",
            "Epoch: 99\n",
            "\t Training loss 0.00507, Training accuracy 81.16\n",
            "\t Test loss 0.00565, Test accuracy 76.90\n",
            "-----------------------------------------------------\n",
            "Epoch: 100\n",
            "\t Training loss 0.00499, Training accuracy 81.28\n",
            "\t Test loss 0.00601, Test accuracy 75.60\n",
            "-----------------------------------------------------\n",
            "Epoch: 101\n",
            "\t Training loss 0.00520, Training accuracy 79.76\n",
            "\t Test loss 0.00594, Test accuracy 75.48\n",
            "-----------------------------------------------------\n",
            "Epoch: 102\n",
            "\t Training loss 0.00514, Training accuracy 79.85\n",
            "\t Test loss 0.00577, Test accuracy 75.36\n",
            "-----------------------------------------------------\n",
            "Epoch: 103\n",
            "\t Training loss 0.00513, Training accuracy 79.76\n",
            "\t Test loss 0.00603, Test accuracy 74.43\n",
            "-----------------------------------------------------\n",
            "Epoch: 104\n",
            "\t Training loss 0.00513, Training accuracy 80.33\n",
            "\t Test loss 0.00626, Test accuracy 76.96\n",
            "-----------------------------------------------------\n",
            "Epoch: 105\n",
            "\t Training loss 0.00504, Training accuracy 81.00\n",
            "\t Test loss 0.00564, Test accuracy 79.06\n",
            "-----------------------------------------------------\n",
            "Epoch: 106\n",
            "\t Training loss 0.00499, Training accuracy 80.82\n",
            "\t Test loss 0.00563, Test accuracy 77.83\n",
            "-----------------------------------------------------\n",
            "Epoch: 107\n",
            "\t Training loss 0.00497, Training accuracy 80.70\n",
            "\t Test loss 0.00590, Test accuracy 73.93\n",
            "-----------------------------------------------------\n",
            "Epoch: 108\n",
            "\t Training loss 0.00496, Training accuracy 80.49\n",
            "\t Test loss 0.00554, Test accuracy 77.33\n",
            "-----------------------------------------------------\n",
            "Epoch: 109\n",
            "\t Training loss 0.00486, Training accuracy 81.61\n",
            "\t Test loss 0.00597, Test accuracy 75.29\n",
            "-----------------------------------------------------\n",
            "Epoch: 110\n",
            "\t Training loss 0.00492, Training accuracy 81.04\n",
            "\t Test loss 0.00555, Test accuracy 79.00\n",
            "-----------------------------------------------------\n",
            "Epoch: 111\n",
            "\t Training loss 0.00481, Training accuracy 81.95\n",
            "\t Test loss 0.00541, Test accuracy 79.00\n",
            "-----------------------------------------------------\n",
            "Epoch: 112\n",
            "\t Training loss 0.00476, Training accuracy 82.47\n",
            "\t Test loss 0.00537, Test accuracy 78.38\n",
            "-----------------------------------------------------\n",
            "Epoch: 113\n",
            "\t Training loss 0.00480, Training accuracy 82.34\n",
            "\t Test loss 0.00555, Test accuracy 78.07\n",
            "-----------------------------------------------------\n",
            "Epoch: 114\n",
            "\t Training loss 0.00490, Training accuracy 80.67\n",
            "\t Test loss 0.00549, Test accuracy 77.76\n",
            "-----------------------------------------------------\n",
            "Epoch: 115\n",
            "\t Training loss 0.00474, Training accuracy 81.61\n",
            "\t Test loss 0.00583, Test accuracy 76.41\n",
            "-----------------------------------------------------\n",
            "Epoch: 116\n",
            "\t Training loss 0.00474, Training accuracy 82.40\n",
            "\t Test loss 0.00513, Test accuracy 78.88\n",
            "-----------------------------------------------------\n",
            "Epoch: 117\n",
            "\t Training loss 0.00477, Training accuracy 81.55\n",
            "\t Test loss 0.00522, Test accuracy 78.26\n",
            "-----------------------------------------------------\n",
            "Epoch: 118\n",
            "\t Training loss 0.00483, Training accuracy 81.83\n",
            "\t Test loss 0.00578, Test accuracy 76.41\n",
            "-----------------------------------------------------\n",
            "Epoch: 119\n",
            "\t Training loss 0.00482, Training accuracy 81.77\n",
            "\t Test loss 0.00553, Test accuracy 77.39\n",
            "-----------------------------------------------------\n",
            "Epoch: 120\n",
            "\t Training loss 0.00477, Training accuracy 81.92\n",
            "\t Test loss 0.00527, Test accuracy 78.94\n",
            "-----------------------------------------------------\n",
            "Epoch: 121\n",
            "\t Training loss 0.00476, Training accuracy 82.22\n",
            "\t Test loss 0.00526, Test accuracy 78.69\n",
            "-----------------------------------------------------\n",
            "Epoch: 122\n",
            "\t Training loss 0.00483, Training accuracy 81.25\n",
            "\t Test loss 0.00529, Test accuracy 77.64\n",
            "-----------------------------------------------------\n",
            "Epoch: 123\n",
            "\t Training loss 0.00459, Training accuracy 82.80\n",
            "\t Test loss 0.00542, Test accuracy 77.89\n",
            "-----------------------------------------------------\n",
            "Epoch: 124\n",
            "\t Training loss 0.00468, Training accuracy 82.07\n",
            "\t Test loss 0.00506, Test accuracy 79.62\n",
            "-----------------------------------------------------\n",
            "Epoch: 125\n",
            "\t Training loss 0.00468, Training accuracy 82.62\n",
            "\t Test loss 0.00506, Test accuracy 81.22\n",
            "-----------------------------------------------------\n",
            "Epoch: 126\n",
            "\t Training loss 0.00456, Training accuracy 82.68\n",
            "\t Test loss 0.00565, Test accuracy 76.41\n",
            "-----------------------------------------------------\n",
            "Epoch: 127\n",
            "\t Training loss 0.00479, Training accuracy 81.64\n",
            "\t Test loss 0.00488, Test accuracy 80.36\n",
            "-----------------------------------------------------\n",
            "Epoch: 128\n",
            "\t Training loss 0.00455, Training accuracy 82.10\n",
            "\t Test loss 0.00496, Test accuracy 79.80\n",
            "-----------------------------------------------------\n",
            "Epoch: 129\n",
            "\t Training loss 0.00461, Training accuracy 82.37\n",
            "\t Test loss 0.00546, Test accuracy 77.83\n",
            "-----------------------------------------------------\n",
            "Epoch: 130\n",
            "\t Training loss 0.00452, Training accuracy 81.61\n",
            "\t Test loss 0.00488, Test accuracy 81.10\n",
            "-----------------------------------------------------\n",
            "Epoch: 131\n",
            "\t Training loss 0.00456, Training accuracy 82.22\n",
            "\t Test loss 0.00479, Test accuracy 81.96\n",
            "-----------------------------------------------------\n",
            "Epoch: 132\n",
            "\t Training loss 0.00456, Training accuracy 82.68\n",
            "\t Test loss 0.00506, Test accuracy 78.26\n",
            "-----------------------------------------------------\n",
            "Epoch: 133\n",
            "\t Training loss 0.00465, Training accuracy 81.31\n",
            "\t Test loss 0.00510, Test accuracy 78.94\n",
            "-----------------------------------------------------\n",
            "Epoch: 134\n",
            "\t Training loss 0.00446, Training accuracy 82.62\n",
            "\t Test loss 0.00510, Test accuracy 78.69\n",
            "-----------------------------------------------------\n",
            "Epoch: 135\n",
            "\t Training loss 0.00437, Training accuracy 83.53\n",
            "\t Test loss 0.00482, Test accuracy 80.42\n",
            "-----------------------------------------------------\n",
            "Epoch: 136\n",
            "\t Training loss 0.00443, Training accuracy 83.35\n",
            "\t Test loss 0.00518, Test accuracy 79.25\n",
            "-----------------------------------------------------\n",
            "Epoch: 137\n",
            "\t Training loss 0.00436, Training accuracy 83.35\n",
            "\t Test loss 0.00537, Test accuracy 77.33\n",
            "-----------------------------------------------------\n",
            "Epoch: 138\n",
            "\t Training loss 0.00442, Training accuracy 82.50\n",
            "\t Test loss 0.00483, Test accuracy 79.37\n",
            "-----------------------------------------------------\n",
            "Epoch: 139\n",
            "\t Training loss 0.00427, Training accuracy 83.96\n",
            "\t Test loss 0.00509, Test accuracy 76.41\n",
            "-----------------------------------------------------\n",
            "Epoch: 140\n",
            "\t Training loss 0.00444, Training accuracy 82.19\n",
            "\t Test loss 0.00505, Test accuracy 79.62\n",
            "-----------------------------------------------------\n",
            "Epoch: 141\n",
            "\t Training loss 0.00443, Training accuracy 82.95\n",
            "\t Test loss 0.00483, Test accuracy 79.68\n",
            "-----------------------------------------------------\n",
            "Epoch: 142\n",
            "\t Training loss 0.00428, Training accuracy 83.81\n",
            "\t Test loss 0.00504, Test accuracy 78.32\n",
            "-----------------------------------------------------\n",
            "Epoch: 143\n",
            "\t Training loss 0.00417, Training accuracy 83.99\n",
            "\t Test loss 0.00491, Test accuracy 79.12\n",
            "-----------------------------------------------------\n",
            "Epoch: 144\n",
            "\t Training loss 0.00431, Training accuracy 82.92\n",
            "\t Test loss 0.00472, Test accuracy 80.85\n",
            "-----------------------------------------------------\n",
            "Epoch: 145\n",
            "\t Training loss 0.00417, Training accuracy 82.92\n",
            "\t Test loss 0.00500, Test accuracy 79.74\n",
            "-----------------------------------------------------\n",
            "Epoch: 146\n",
            "\t Training loss 0.00433, Training accuracy 83.50\n",
            "\t Test loss 0.00474, Test accuracy 80.30\n",
            "-----------------------------------------------------\n",
            "Epoch: 147\n",
            "\t Training loss 0.00421, Training accuracy 83.38\n",
            "\t Test loss 0.00506, Test accuracy 79.86\n",
            "-----------------------------------------------------\n",
            "Epoch: 148\n",
            "\t Training loss 0.00430, Training accuracy 83.23\n",
            "\t Test loss 0.00466, Test accuracy 80.42\n",
            "-----------------------------------------------------\n",
            "Epoch: 149\n",
            "\t Training loss 0.00434, Training accuracy 82.95\n",
            "\t Test loss 0.00491, Test accuracy 78.94\n",
            "-----------------------------------------------------\n",
            "Epoch: 150\n",
            "\t Training loss 0.00413, Training accuracy 83.96\n",
            "\t Test loss 0.00503, Test accuracy 79.80\n",
            "-----------------------------------------------------\n",
            "Epoch: 151\n",
            "\t Training loss 0.00425, Training accuracy 82.86\n",
            "\t Test loss 0.00490, Test accuracy 78.13\n",
            "-----------------------------------------------------\n",
            "Epoch: 152\n",
            "\t Training loss 0.00431, Training accuracy 82.47\n",
            "\t Test loss 0.00453, Test accuracy 81.10\n",
            "-----------------------------------------------------\n",
            "Epoch: 153\n",
            "\t Training loss 0.00412, Training accuracy 84.32\n",
            "\t Test loss 0.00424, Test accuracy 82.64\n",
            "-----------------------------------------------------\n",
            "Epoch: 154\n",
            "\t Training loss 0.00411, Training accuracy 83.35\n",
            "\t Test loss 0.00479, Test accuracy 80.23\n",
            "-----------------------------------------------------\n",
            "Epoch: 155\n",
            "\t Training loss 0.00409, Training accuracy 83.62\n",
            "\t Test loss 0.00435, Test accuracy 80.79\n",
            "-----------------------------------------------------\n",
            "Epoch: 156\n",
            "\t Training loss 0.00413, Training accuracy 83.14\n",
            "\t Test loss 0.00467, Test accuracy 80.05\n",
            "-----------------------------------------------------\n",
            "Epoch: 157\n",
            "\t Training loss 0.00415, Training accuracy 83.17\n",
            "\t Test loss 0.00446, Test accuracy 82.09\n",
            "-----------------------------------------------------\n",
            "Epoch: 158\n",
            "\t Training loss 0.00412, Training accuracy 83.68\n",
            "\t Test loss 0.00473, Test accuracy 80.48\n",
            "-----------------------------------------------------\n",
            "Epoch: 159\n",
            "\t Training loss 0.00395, Training accuracy 84.17\n",
            "\t Test loss 0.00444, Test accuracy 81.22\n",
            "-----------------------------------------------------\n",
            "Epoch: 160\n",
            "\t Training loss 0.00406, Training accuracy 83.77\n",
            "\t Test loss 0.00463, Test accuracy 81.10\n",
            "-----------------------------------------------------\n",
            "Epoch: 161\n",
            "\t Training loss 0.00396, Training accuracy 84.17\n",
            "\t Test loss 0.00521, Test accuracy 78.75\n",
            "-----------------------------------------------------\n",
            "Epoch: 162\n",
            "\t Training loss 0.00421, Training accuracy 83.23\n",
            "\t Test loss 0.00462, Test accuracy 80.98\n",
            "-----------------------------------------------------\n",
            "Epoch: 163\n",
            "\t Training loss 0.00423, Training accuracy 83.56\n",
            "\t Test loss 0.00448, Test accuracy 80.67\n",
            "-----------------------------------------------------\n",
            "Epoch: 164\n",
            "\t Training loss 0.00413, Training accuracy 83.14\n",
            "\t Test loss 0.00461, Test accuracy 78.69\n",
            "-----------------------------------------------------\n",
            "Epoch: 165\n",
            "\t Training loss 0.00429, Training accuracy 83.96\n",
            "\t Test loss 0.00490, Test accuracy 79.37\n",
            "-----------------------------------------------------\n",
            "Epoch: 166\n",
            "\t Training loss 0.00409, Training accuracy 83.77\n",
            "\t Test loss 0.00478, Test accuracy 78.81\n",
            "-----------------------------------------------------\n",
            "Epoch: 167\n",
            "\t Training loss 0.00397, Training accuracy 84.26\n",
            "\t Test loss 0.00493, Test accuracy 78.69\n",
            "-----------------------------------------------------\n",
            "Epoch: 168\n",
            "\t Training loss 0.00393, Training accuracy 84.26\n",
            "\t Test loss 0.00451, Test accuracy 79.37\n",
            "-----------------------------------------------------\n",
            "Epoch: 169\n",
            "\t Training loss 0.00395, Training accuracy 84.63\n",
            "\t Test loss 0.00462, Test accuracy 79.86\n",
            "-----------------------------------------------------\n",
            "Epoch: 170\n",
            "\t Training loss 0.00390, Training accuracy 85.11\n",
            "\t Test loss 0.00507, Test accuracy 79.31\n",
            "-----------------------------------------------------\n",
            "Epoch: 171\n",
            "\t Training loss 0.00393, Training accuracy 84.05\n",
            "\t Test loss 0.00462, Test accuracy 80.79\n",
            "-----------------------------------------------------\n",
            "Epoch: 172\n",
            "\t Training loss 0.00390, Training accuracy 84.69\n",
            "\t Test loss 0.00459, Test accuracy 80.11\n",
            "-----------------------------------------------------\n",
            "Epoch: 173\n",
            "\t Training loss 0.00383, Training accuracy 85.36\n",
            "\t Test loss 0.00485, Test accuracy 78.20\n",
            "-----------------------------------------------------\n",
            "Epoch: 174\n",
            "\t Training loss 0.00387, Training accuracy 84.08\n",
            "\t Test loss 0.00438, Test accuracy 81.53\n",
            "-----------------------------------------------------\n",
            "Epoch: 175\n",
            "\t Training loss 0.00384, Training accuracy 85.18\n",
            "\t Test loss 0.00439, Test accuracy 81.53\n",
            "-----------------------------------------------------\n",
            "Epoch: 176\n",
            "\t Training loss 0.00373, Training accuracy 85.51\n",
            "\t Test loss 0.00415, Test accuracy 82.58\n",
            "-----------------------------------------------------\n",
            "Epoch: 177\n",
            "\t Training loss 0.00387, Training accuracy 85.21\n",
            "\t Test loss 0.00426, Test accuracy 81.59\n",
            "-----------------------------------------------------\n",
            "Epoch: 178\n",
            "\t Training loss 0.00391, Training accuracy 84.54\n",
            "\t Test loss 0.00441, Test accuracy 80.98\n",
            "-----------------------------------------------------\n",
            "Epoch: 179\n",
            "\t Training loss 0.00386, Training accuracy 84.84\n",
            "\t Test loss 0.00459, Test accuracy 81.84\n",
            "-----------------------------------------------------\n",
            "Epoch: 180\n",
            "\t Training loss 0.00391, Training accuracy 83.93\n",
            "\t Test loss 0.00418, Test accuracy 81.41\n",
            "-----------------------------------------------------\n",
            "Epoch: 181\n",
            "\t Training loss 0.00379, Training accuracy 85.24\n",
            "\t Test loss 0.00425, Test accuracy 82.03\n",
            "-----------------------------------------------------\n",
            "Epoch: 182\n",
            "\t Training loss 0.00387, Training accuracy 84.20\n",
            "\t Test loss 0.00452, Test accuracy 79.86\n",
            "-----------------------------------------------------\n",
            "Epoch: 183\n",
            "\t Training loss 0.00385, Training accuracy 84.02\n",
            "\t Test loss 0.00433, Test accuracy 81.41\n",
            "-----------------------------------------------------\n",
            "Epoch: 184\n",
            "\t Training loss 0.00385, Training accuracy 84.54\n",
            "\t Test loss 0.00398, Test accuracy 83.32\n",
            "-----------------------------------------------------\n",
            "Epoch: 185\n",
            "\t Training loss 0.00377, Training accuracy 85.27\n",
            "\t Test loss 0.00414, Test accuracy 82.27\n",
            "-----------------------------------------------------\n",
            "Epoch: 186\n",
            "\t Training loss 0.00372, Training accuracy 84.75\n",
            "\t Test loss 0.00463, Test accuracy 80.42\n",
            "-----------------------------------------------------\n",
            "Epoch: 187\n",
            "\t Training loss 0.00373, Training accuracy 84.90\n",
            "\t Test loss 0.00398, Test accuracy 83.01\n",
            "-----------------------------------------------------\n",
            "Epoch: 188\n",
            "\t Training loss 0.00368, Training accuracy 84.60\n",
            "\t Test loss 0.00524, Test accuracy 77.02\n",
            "-----------------------------------------------------\n",
            "Epoch: 189\n",
            "\t Training loss 0.00392, Training accuracy 84.17\n",
            "\t Test loss 0.00445, Test accuracy 80.54\n",
            "-----------------------------------------------------\n",
            "Epoch: 190\n",
            "\t Training loss 0.00373, Training accuracy 85.27\n",
            "\t Test loss 0.00462, Test accuracy 78.26\n",
            "-----------------------------------------------------\n",
            "Epoch: 191\n",
            "\t Training loss 0.00375, Training accuracy 85.02\n",
            "\t Test loss 0.00424, Test accuracy 81.66\n",
            "-----------------------------------------------------\n",
            "Epoch: 192\n",
            "\t Training loss 0.00384, Training accuracy 84.96\n",
            "\t Test loss 0.00439, Test accuracy 80.73\n",
            "-----------------------------------------------------\n",
            "Epoch: 193\n",
            "\t Training loss 0.00375, Training accuracy 85.63\n",
            "\t Test loss 0.00452, Test accuracy 80.67\n",
            "-----------------------------------------------------\n",
            "Epoch: 194\n",
            "\t Training loss 0.00371, Training accuracy 85.14\n",
            "\t Test loss 0.00430, Test accuracy 81.84\n",
            "-----------------------------------------------------\n",
            "Epoch: 195\n",
            "\t Training loss 0.00377, Training accuracy 85.45\n",
            "\t Test loss 0.00424, Test accuracy 82.33\n",
            "-----------------------------------------------------\n",
            "Epoch: 196\n",
            "\t Training loss 0.00368, Training accuracy 85.24\n",
            "\t Test loss 0.00464, Test accuracy 79.99\n",
            "-----------------------------------------------------\n",
            "Epoch: 197\n",
            "\t Training loss 0.00369, Training accuracy 85.21\n",
            "\t Test loss 0.00508, Test accuracy 77.95\n",
            "-----------------------------------------------------\n",
            "Epoch: 198\n",
            "\t Training loss 0.00374, Training accuracy 84.47\n",
            "\t Test loss 0.00421, Test accuracy 82.03\n",
            "-----------------------------------------------------\n",
            "Epoch: 199\n",
            "\t Training loss 0.00371, Training accuracy 84.90\n",
            "\t Test loss 0.00434, Test accuracy 82.33\n",
            "-----------------------------------------------------\n",
            "Epoch: 200\n",
            "\t Training loss 0.00380, Training accuracy 84.99\n",
            "\t Test loss 0.00469, Test accuracy 80.17\n",
            "-----------------------------------------------------\n",
            "Epoch: 201\n",
            "\t Training loss 0.00374, Training accuracy 84.57\n",
            "\t Test loss 0.00426, Test accuracy 81.47\n",
            "-----------------------------------------------------\n",
            "Epoch: 202\n",
            "\t Training loss 0.00358, Training accuracy 85.27\n",
            "\t Test loss 0.00397, Test accuracy 81.96\n",
            "-----------------------------------------------------\n",
            "Epoch: 203\n",
            "\t Training loss 0.00366, Training accuracy 85.48\n",
            "\t Test loss 0.00420, Test accuracy 82.09\n",
            "-----------------------------------------------------\n",
            "Epoch: 204\n",
            "\t Training loss 0.00354, Training accuracy 86.36\n",
            "\t Test loss 0.00408, Test accuracy 83.26\n",
            "-----------------------------------------------------\n",
            "Epoch: 205\n",
            "\t Training loss 0.00359, Training accuracy 85.57\n",
            "\t Test loss 0.00387, Test accuracy 84.25\n",
            "-----------------------------------------------------\n",
            "Epoch: 206\n",
            "\t Training loss 0.00364, Training accuracy 85.21\n",
            "\t Test loss 0.00433, Test accuracy 81.28\n",
            "-----------------------------------------------------\n",
            "Epoch: 207\n",
            "\t Training loss 0.00346, Training accuracy 85.75\n",
            "\t Test loss 0.00432, Test accuracy 81.04\n",
            "-----------------------------------------------------\n",
            "Epoch: 208\n",
            "\t Training loss 0.00365, Training accuracy 85.18\n",
            "\t Test loss 0.00457, Test accuracy 79.86\n",
            "-----------------------------------------------------\n",
            "Epoch: 209\n",
            "\t Training loss 0.00368, Training accuracy 84.63\n",
            "\t Test loss 0.00423, Test accuracy 81.72\n",
            "-----------------------------------------------------\n",
            "Epoch: 210\n",
            "\t Training loss 0.00364, Training accuracy 86.00\n",
            "\t Test loss 0.00411, Test accuracy 82.77\n",
            "-----------------------------------------------------\n",
            "Epoch: 211\n",
            "\t Training loss 0.00352, Training accuracy 86.12\n",
            "\t Test loss 0.00380, Test accuracy 84.43\n",
            "-----------------------------------------------------\n",
            "Epoch: 212\n",
            "\t Training loss 0.00342, Training accuracy 86.70\n",
            "\t Test loss 0.00384, Test accuracy 84.31\n",
            "-----------------------------------------------------\n",
            "Epoch: 213\n",
            "\t Training loss 0.00354, Training accuracy 85.97\n",
            "\t Test loss 0.00383, Test accuracy 84.00\n",
            "-----------------------------------------------------\n",
            "Epoch: 214\n",
            "\t Training loss 0.00353, Training accuracy 85.88\n",
            "\t Test loss 0.00459, Test accuracy 78.75\n",
            "-----------------------------------------------------\n",
            "Epoch: 215\n",
            "\t Training loss 0.00352, Training accuracy 85.91\n",
            "\t Test loss 0.00372, Test accuracy 83.76\n",
            "-----------------------------------------------------\n",
            "Epoch: 216\n",
            "\t Training loss 0.00350, Training accuracy 85.48\n",
            "\t Test loss 0.00407, Test accuracy 83.57\n",
            "-----------------------------------------------------\n",
            "Epoch: 217\n",
            "\t Training loss 0.00349, Training accuracy 86.51\n",
            "\t Test loss 0.00382, Test accuracy 84.00\n",
            "-----------------------------------------------------\n",
            "Epoch: 218\n",
            "\t Training loss 0.00351, Training accuracy 85.57\n",
            "\t Test loss 0.00374, Test accuracy 85.42\n",
            "-----------------------------------------------------\n",
            "Epoch: 219\n",
            "\t Training loss 0.00354, Training accuracy 86.33\n",
            "\t Test loss 0.00370, Test accuracy 84.37\n",
            "-----------------------------------------------------\n",
            "Epoch: 220\n",
            "\t Training loss 0.00351, Training accuracy 86.03\n",
            "\t Test loss 0.00439, Test accuracy 82.52\n",
            "-----------------------------------------------------\n",
            "Epoch: 221\n",
            "\t Training loss 0.00349, Training accuracy 86.03\n",
            "\t Test loss 0.00446, Test accuracy 80.91\n",
            "-----------------------------------------------------\n",
            "Epoch: 222\n",
            "\t Training loss 0.00356, Training accuracy 86.00\n",
            "\t Test loss 0.00378, Test accuracy 83.26\n",
            "-----------------------------------------------------\n",
            "Epoch: 223\n",
            "\t Training loss 0.00362, Training accuracy 85.60\n",
            "\t Test loss 0.00429, Test accuracy 81.72\n",
            "-----------------------------------------------------\n",
            "Epoch: 224\n",
            "\t Training loss 0.00365, Training accuracy 85.66\n",
            "\t Test loss 0.00411, Test accuracy 82.83\n",
            "-----------------------------------------------------\n",
            "Epoch: 225\n",
            "\t Training loss 0.00359, Training accuracy 86.24\n",
            "\t Test loss 0.00373, Test accuracy 85.18\n",
            "-----------------------------------------------------\n",
            "Epoch: 226\n",
            "\t Training loss 0.00345, Training accuracy 85.94\n",
            "\t Test loss 0.00404, Test accuracy 81.84\n",
            "-----------------------------------------------------\n",
            "Epoch: 227\n",
            "\t Training loss 0.00352, Training accuracy 85.84\n",
            "\t Test loss 0.00401, Test accuracy 83.76\n",
            "-----------------------------------------------------\n",
            "Epoch: 228\n",
            "\t Training loss 0.00351, Training accuracy 86.51\n",
            "\t Test loss 0.00391, Test accuracy 84.00\n",
            "-----------------------------------------------------\n",
            "Epoch: 229\n",
            "\t Training loss 0.00345, Training accuracy 85.63\n",
            "\t Test loss 0.00425, Test accuracy 83.01\n",
            "-----------------------------------------------------\n",
            "Epoch: 230\n",
            "\t Training loss 0.00350, Training accuracy 86.33\n",
            "\t Test loss 0.00374, Test accuracy 83.57\n",
            "-----------------------------------------------------\n",
            "Epoch: 231\n",
            "\t Training loss 0.00346, Training accuracy 85.91\n",
            "\t Test loss 0.00376, Test accuracy 84.13\n",
            "-----------------------------------------------------\n",
            "Epoch: 232\n",
            "\t Training loss 0.00328, Training accuracy 86.54\n",
            "\t Test loss 0.00413, Test accuracy 82.89\n",
            "-----------------------------------------------------\n",
            "Epoch: 233\n",
            "\t Training loss 0.00348, Training accuracy 85.63\n",
            "\t Test loss 0.00423, Test accuracy 81.35\n",
            "-----------------------------------------------------\n",
            "Epoch: 234\n",
            "\t Training loss 0.00343, Training accuracy 85.91\n",
            "\t Test loss 0.00390, Test accuracy 82.52\n",
            "-----------------------------------------------------\n",
            "Epoch: 235\n",
            "\t Training loss 0.00348, Training accuracy 85.69\n",
            "\t Test loss 0.00381, Test accuracy 83.01\n",
            "-----------------------------------------------------\n",
            "Epoch: 236\n",
            "\t Training loss 0.00337, Training accuracy 86.64\n",
            "\t Test loss 0.00408, Test accuracy 82.64\n",
            "-----------------------------------------------------\n",
            "Epoch: 237\n",
            "\t Training loss 0.00340, Training accuracy 86.58\n",
            "\t Test loss 0.00495, Test accuracy 79.18\n",
            "-----------------------------------------------------\n",
            "Epoch: 238\n",
            "\t Training loss 0.00350, Training accuracy 84.93\n",
            "\t Test loss 0.00411, Test accuracy 83.26\n",
            "-----------------------------------------------------\n",
            "Epoch: 239\n",
            "\t Training loss 0.00355, Training accuracy 85.39\n",
            "\t Test loss 0.00415, Test accuracy 82.27\n",
            "-----------------------------------------------------\n",
            "Epoch: 240\n",
            "\t Training loss 0.00349, Training accuracy 85.75\n",
            "\t Test loss 0.00407, Test accuracy 84.00\n",
            "-----------------------------------------------------\n",
            "Epoch: 241\n",
            "\t Training loss 0.00360, Training accuracy 84.32\n",
            "\t Test loss 0.00439, Test accuracy 80.85\n",
            "-----------------------------------------------------\n",
            "Epoch: 242\n",
            "\t Training loss 0.00339, Training accuracy 86.91\n",
            "\t Test loss 0.00409, Test accuracy 82.77\n",
            "-----------------------------------------------------\n",
            "Epoch: 243\n",
            "\t Training loss 0.00330, Training accuracy 86.61\n",
            "\t Test loss 0.00446, Test accuracy 81.59\n",
            "-----------------------------------------------------\n",
            "Epoch: 244\n",
            "\t Training loss 0.00344, Training accuracy 85.88\n",
            "\t Test loss 0.00392, Test accuracy 82.95\n",
            "-----------------------------------------------------\n",
            "Epoch: 245\n",
            "\t Training loss 0.00334, Training accuracy 86.24\n",
            "\t Test loss 0.00394, Test accuracy 84.25\n",
            "-----------------------------------------------------\n",
            "Epoch: 246\n",
            "\t Training loss 0.00327, Training accuracy 86.61\n",
            "\t Test loss 0.00376, Test accuracy 84.56\n",
            "-----------------------------------------------------\n",
            "Epoch: 247\n",
            "\t Training loss 0.00325, Training accuracy 87.40\n",
            "\t Test loss 0.00377, Test accuracy 85.24\n",
            "-----------------------------------------------------\n",
            "Epoch: 248\n",
            "\t Training loss 0.00333, Training accuracy 85.84\n",
            "\t Test loss 0.00459, Test accuracy 79.99\n",
            "-----------------------------------------------------\n",
            "Epoch: 249\n",
            "\t Training loss 0.00336, Training accuracy 86.03\n",
            "\t Test loss 0.00385, Test accuracy 84.13\n",
            "-----------------------------------------------------\n",
            "Epoch: 250\n",
            "\t Training loss 0.00343, Training accuracy 86.42\n",
            "\t Test loss 0.00357, Test accuracy 85.92\n",
            "-----------------------------------------------------\n",
            "Epoch: 251\n",
            "\t Training loss 0.00326, Training accuracy 87.28\n",
            "\t Test loss 0.00374, Test accuracy 85.18\n",
            "-----------------------------------------------------\n",
            "Epoch: 252\n",
            "\t Training loss 0.00341, Training accuracy 86.54\n",
            "\t Test loss 0.00408, Test accuracy 83.14\n",
            "-----------------------------------------------------\n",
            "Epoch: 253\n",
            "\t Training loss 0.00338, Training accuracy 86.39\n",
            "\t Test loss 0.00418, Test accuracy 83.20\n",
            "-----------------------------------------------------\n",
            "Epoch: 254\n",
            "\t Training loss 0.00335, Training accuracy 86.18\n",
            "\t Test loss 0.00388, Test accuracy 83.14\n",
            "-----------------------------------------------------\n",
            "Epoch: 255\n",
            "\t Training loss 0.00343, Training accuracy 86.36\n",
            "\t Test loss 0.00374, Test accuracy 83.82\n",
            "-----------------------------------------------------\n",
            "Epoch: 256\n",
            "\t Training loss 0.00338, Training accuracy 87.03\n",
            "\t Test loss 0.00378, Test accuracy 84.37\n",
            "-----------------------------------------------------\n",
            "Epoch: 257\n",
            "\t Training loss 0.00328, Training accuracy 86.61\n",
            "\t Test loss 0.00389, Test accuracy 82.89\n",
            "-----------------------------------------------------\n",
            "Epoch: 258\n",
            "\t Training loss 0.00338, Training accuracy 85.78\n",
            "\t Test loss 0.00397, Test accuracy 83.76\n",
            "-----------------------------------------------------\n",
            "Epoch: 259\n",
            "\t Training loss 0.00331, Training accuracy 86.79\n",
            "\t Test loss 0.00406, Test accuracy 83.38\n",
            "-----------------------------------------------------\n",
            "Epoch: 260\n",
            "\t Training loss 0.00343, Training accuracy 86.06\n",
            "\t Test loss 0.00424, Test accuracy 82.95\n",
            "-----------------------------------------------------\n",
            "Epoch: 261\n",
            "\t Training loss 0.00330, Training accuracy 86.82\n",
            "\t Test loss 0.00377, Test accuracy 84.50\n",
            "-----------------------------------------------------\n",
            "Epoch: 262\n",
            "\t Training loss 0.00322, Training accuracy 86.54\n",
            "\t Test loss 0.00373, Test accuracy 85.24\n",
            "-----------------------------------------------------\n",
            "Epoch: 263\n",
            "\t Training loss 0.00323, Training accuracy 87.61\n",
            "\t Test loss 0.00351, Test accuracy 86.35\n",
            "-----------------------------------------------------\n",
            "Epoch: 264\n",
            "\t Training loss 0.00321, Training accuracy 87.46\n",
            "\t Test loss 0.00402, Test accuracy 82.83\n",
            "-----------------------------------------------------\n",
            "Epoch: 265\n",
            "\t Training loss 0.00331, Training accuracy 86.94\n",
            "\t Test loss 0.00367, Test accuracy 86.84\n",
            "-----------------------------------------------------\n",
            "Epoch: 266\n",
            "\t Training loss 0.00330, Training accuracy 87.55\n",
            "\t Test loss 0.00367, Test accuracy 85.79\n",
            "-----------------------------------------------------\n",
            "Epoch: 267\n",
            "\t Training loss 0.00327, Training accuracy 86.48\n",
            "\t Test loss 0.00385, Test accuracy 83.94\n",
            "-----------------------------------------------------\n",
            "Epoch: 268\n",
            "\t Training loss 0.00334, Training accuracy 86.12\n",
            "\t Test loss 0.00379, Test accuracy 84.99\n",
            "-----------------------------------------------------\n",
            "Epoch: 269\n",
            "\t Training loss 0.00332, Training accuracy 86.51\n",
            "\t Test loss 0.00408, Test accuracy 82.95\n",
            "-----------------------------------------------------\n",
            "Epoch: 270\n",
            "\t Training loss 0.00322, Training accuracy 87.06\n",
            "\t Test loss 0.00365, Test accuracy 85.36\n",
            "-----------------------------------------------------\n",
            "Epoch: 271\n",
            "\t Training loss 0.00314, Training accuracy 87.98\n",
            "\t Test loss 0.00433, Test accuracy 81.90\n",
            "-----------------------------------------------------\n",
            "Epoch: 272\n",
            "\t Training loss 0.00334, Training accuracy 87.28\n",
            "\t Test loss 0.00386, Test accuracy 83.69\n",
            "-----------------------------------------------------\n",
            "Epoch: 273\n",
            "\t Training loss 0.00332, Training accuracy 87.15\n",
            "\t Test loss 0.00368, Test accuracy 84.25\n",
            "-----------------------------------------------------\n",
            "Epoch: 274\n",
            "\t Training loss 0.00335, Training accuracy 87.06\n",
            "\t Test loss 0.00385, Test accuracy 84.68\n",
            "-----------------------------------------------------\n",
            "Epoch: 275\n",
            "\t Training loss 0.00330, Training accuracy 87.18\n",
            "\t Test loss 0.00375, Test accuracy 84.00\n",
            "-----------------------------------------------------\n",
            "Epoch: 276\n",
            "\t Training loss 0.00334, Training accuracy 86.51\n",
            "\t Test loss 0.00363, Test accuracy 85.24\n",
            "-----------------------------------------------------\n",
            "Epoch: 277\n",
            "\t Training loss 0.00307, Training accuracy 88.01\n",
            "\t Test loss 0.00372, Test accuracy 85.67\n",
            "-----------------------------------------------------\n",
            "Epoch: 278\n",
            "\t Training loss 0.00329, Training accuracy 87.06\n",
            "\t Test loss 0.00403, Test accuracy 83.82\n",
            "-----------------------------------------------------\n",
            "Epoch: 279\n",
            "\t Training loss 0.00316, Training accuracy 87.82\n",
            "\t Test loss 0.00353, Test accuracy 85.48\n",
            "-----------------------------------------------------\n",
            "Epoch: 280\n",
            "\t Training loss 0.00315, Training accuracy 87.91\n",
            "\t Test loss 0.00385, Test accuracy 83.94\n",
            "-----------------------------------------------------\n",
            "Epoch: 281\n",
            "\t Training loss 0.00338, Training accuracy 86.15\n",
            "\t Test loss 0.00377, Test accuracy 84.25\n",
            "-----------------------------------------------------\n",
            "Epoch: 282\n",
            "\t Training loss 0.00323, Training accuracy 87.34\n",
            "\t Test loss 0.00350, Test accuracy 85.98\n",
            "-----------------------------------------------------\n",
            "Epoch: 283\n",
            "\t Training loss 0.00320, Training accuracy 86.73\n",
            "\t Test loss 0.00375, Test accuracy 85.73\n",
            "-----------------------------------------------------\n",
            "Epoch: 284\n",
            "\t Training loss 0.00314, Training accuracy 87.64\n",
            "\t Test loss 0.00374, Test accuracy 84.56\n",
            "-----------------------------------------------------\n",
            "Epoch: 285\n",
            "\t Training loss 0.00317, Training accuracy 87.76\n",
            "\t Test loss 0.00370, Test accuracy 84.43\n",
            "-----------------------------------------------------\n",
            "Epoch: 286\n",
            "\t Training loss 0.00332, Training accuracy 86.24\n",
            "\t Test loss 0.00408, Test accuracy 83.20\n",
            "-----------------------------------------------------\n",
            "Epoch: 287\n",
            "\t Training loss 0.00319, Training accuracy 87.58\n",
            "\t Test loss 0.00377, Test accuracy 84.93\n",
            "-----------------------------------------------------\n",
            "Epoch: 288\n",
            "\t Training loss 0.00319, Training accuracy 88.28\n",
            "\t Test loss 0.00433, Test accuracy 83.26\n",
            "-----------------------------------------------------\n",
            "Epoch: 289\n",
            "\t Training loss 0.00311, Training accuracy 87.31\n",
            "\t Test loss 0.00400, Test accuracy 82.64\n",
            "-----------------------------------------------------\n",
            "Epoch: 290\n",
            "\t Training loss 0.00320, Training accuracy 87.88\n",
            "\t Test loss 0.00392, Test accuracy 83.69\n",
            "-----------------------------------------------------\n",
            "Epoch: 291\n",
            "\t Training loss 0.00324, Training accuracy 87.64\n",
            "\t Test loss 0.00399, Test accuracy 85.11\n",
            "-----------------------------------------------------\n",
            "Epoch: 292\n",
            "\t Training loss 0.00321, Training accuracy 87.03\n",
            "\t Test loss 0.00358, Test accuracy 85.36\n",
            "-----------------------------------------------------\n",
            "Epoch: 293\n",
            "\t Training loss 0.00326, Training accuracy 87.58\n",
            "\t Test loss 0.00467, Test accuracy 80.79\n",
            "-----------------------------------------------------\n",
            "Epoch: 294\n",
            "\t Training loss 0.00333, Training accuracy 87.46\n",
            "\t Test loss 0.00397, Test accuracy 83.38\n",
            "-----------------------------------------------------\n",
            "Epoch: 295\n",
            "\t Training loss 0.00315, Training accuracy 87.46\n",
            "\t Test loss 0.00368, Test accuracy 84.62\n",
            "-----------------------------------------------------\n",
            "Epoch: 296\n",
            "\t Training loss 0.00311, Training accuracy 87.91\n",
            "\t Test loss 0.00353, Test accuracy 85.86\n",
            "-----------------------------------------------------\n",
            "Epoch: 297\n",
            "\t Training loss 0.00312, Training accuracy 87.58\n",
            "\t Test loss 0.00366, Test accuracy 85.36\n",
            "-----------------------------------------------------\n",
            "Epoch: 298\n",
            "\t Training loss 0.00312, Training accuracy 88.10\n",
            "\t Test loss 0.00382, Test accuracy 84.37\n",
            "-----------------------------------------------------\n",
            "Epoch: 299\n",
            "\t Training loss 0.00309, Training accuracy 87.49\n",
            "\t Test loss 0.00388, Test accuracy 83.45\n",
            "-----------------------------------------------------\n",
            "Epoch: 300\n",
            "\t Training loss 0.00308, Training accuracy 87.79\n",
            "\t Test loss 0.00380, Test accuracy 84.99\n",
            "-----------------------------------------------------\n",
            "Epoch: 301\n",
            "\t Training loss 0.00319, Training accuracy 88.01\n",
            "\t Test loss 0.00367, Test accuracy 83.94\n",
            "-----------------------------------------------------\n",
            "Epoch: 302\n",
            "\t Training loss 0.00319, Training accuracy 87.82\n",
            "\t Test loss 0.00359, Test accuracy 85.11\n",
            "-----------------------------------------------------\n",
            "Epoch: 303\n",
            "\t Training loss 0.00321, Training accuracy 87.55\n",
            "\t Test loss 0.00392, Test accuracy 83.94\n",
            "-----------------------------------------------------\n",
            "Epoch: 304\n",
            "\t Training loss 0.00304, Training accuracy 87.64\n",
            "\t Test loss 0.00384, Test accuracy 84.81\n",
            "-----------------------------------------------------\n",
            "Epoch: 305\n",
            "\t Training loss 0.00317, Training accuracy 87.34\n",
            "\t Test loss 0.00391, Test accuracy 84.74\n",
            "-----------------------------------------------------\n",
            "Epoch: 306\n",
            "\t Training loss 0.00303, Training accuracy 88.07\n",
            "\t Test loss 0.00435, Test accuracy 81.28\n",
            "-----------------------------------------------------\n",
            "Epoch: 307\n",
            "\t Training loss 0.00301, Training accuracy 88.01\n",
            "\t Test loss 0.00360, Test accuracy 84.81\n",
            "-----------------------------------------------------\n",
            "Epoch: 308\n",
            "\t Training loss 0.00319, Training accuracy 87.58\n",
            "\t Test loss 0.00403, Test accuracy 82.40\n",
            "-----------------------------------------------------\n",
            "Epoch: 309\n",
            "\t Training loss 0.00318, Training accuracy 87.88\n",
            "\t Test loss 0.00400, Test accuracy 83.82\n",
            "-----------------------------------------------------\n",
            "Epoch: 310\n",
            "\t Training loss 0.00303, Training accuracy 87.79\n",
            "\t Test loss 0.00356, Test accuracy 85.67\n",
            "-----------------------------------------------------\n",
            "Epoch: 311\n",
            "\t Training loss 0.00305, Training accuracy 88.25\n",
            "\t Test loss 0.00372, Test accuracy 85.24\n",
            "-----------------------------------------------------\n",
            "Epoch: 312\n",
            "\t Training loss 0.00302, Training accuracy 88.55\n",
            "\t Test loss 0.00361, Test accuracy 84.37\n",
            "-----------------------------------------------------\n",
            "Epoch: 313\n",
            "\t Training loss 0.00301, Training accuracy 88.74\n",
            "\t Test loss 0.00361, Test accuracy 85.30\n",
            "-----------------------------------------------------\n",
            "Epoch: 314\n",
            "\t Training loss 0.00305, Training accuracy 87.12\n",
            "\t Test loss 0.00350, Test accuracy 85.18\n",
            "-----------------------------------------------------\n",
            "Epoch: 315\n",
            "\t Training loss 0.00322, Training accuracy 88.46\n",
            "\t Test loss 0.00374, Test accuracy 84.87\n",
            "-----------------------------------------------------\n",
            "Epoch: 316\n",
            "\t Training loss 0.00303, Training accuracy 88.86\n",
            "\t Test loss 0.00370, Test accuracy 84.37\n",
            "-----------------------------------------------------\n",
            "Epoch: 317\n",
            "\t Training loss 0.00306, Training accuracy 88.34\n",
            "\t Test loss 0.00356, Test accuracy 84.87\n",
            "-----------------------------------------------------\n",
            "Epoch: 318\n",
            "\t Training loss 0.00309, Training accuracy 87.46\n",
            "\t Test loss 0.00384, Test accuracy 84.87\n",
            "-----------------------------------------------------\n",
            "Epoch: 319\n",
            "\t Training loss 0.00319, Training accuracy 87.95\n",
            "\t Test loss 0.00391, Test accuracy 83.82\n",
            "-----------------------------------------------------\n",
            "Epoch: 320\n",
            "\t Training loss 0.00301, Training accuracy 88.52\n",
            "\t Test loss 0.00372, Test accuracy 83.76\n",
            "-----------------------------------------------------\n",
            "Epoch: 321\n",
            "\t Training loss 0.00304, Training accuracy 88.37\n",
            "\t Test loss 0.00365, Test accuracy 86.53\n",
            "-----------------------------------------------------\n",
            "Epoch: 322\n",
            "\t Training loss 0.00316, Training accuracy 87.82\n",
            "\t Test loss 0.00443, Test accuracy 81.96\n",
            "-----------------------------------------------------\n",
            "Epoch: 323\n",
            "\t Training loss 0.00307, Training accuracy 88.65\n",
            "\t Test loss 0.00370, Test accuracy 84.87\n",
            "-----------------------------------------------------\n",
            "Epoch: 324\n",
            "\t Training loss 0.00325, Training accuracy 87.18\n",
            "\t Test loss 0.00395, Test accuracy 84.31\n",
            "-----------------------------------------------------\n",
            "Epoch: 325\n",
            "\t Training loss 0.00297, Training accuracy 88.61\n",
            "\t Test loss 0.00378, Test accuracy 84.06\n",
            "-----------------------------------------------------\n",
            "Epoch: 326\n",
            "\t Training loss 0.00307, Training accuracy 88.13\n",
            "\t Test loss 0.00370, Test accuracy 85.55\n",
            "-----------------------------------------------------\n",
            "Epoch: 327\n",
            "\t Training loss 0.00299, Training accuracy 88.10\n",
            "\t Test loss 0.00346, Test accuracy 85.79\n",
            "-----------------------------------------------------\n",
            "Epoch: 328\n",
            "\t Training loss 0.00295, Training accuracy 88.77\n",
            "\t Test loss 0.00372, Test accuracy 84.56\n",
            "-----------------------------------------------------\n",
            "Epoch: 329\n",
            "\t Training loss 0.00309, Training accuracy 87.09\n",
            "\t Test loss 0.00347, Test accuracy 85.73\n",
            "-----------------------------------------------------\n",
            "Epoch: 330\n",
            "\t Training loss 0.00317, Training accuracy 87.52\n",
            "\t Test loss 0.00379, Test accuracy 84.25\n",
            "-----------------------------------------------------\n",
            "Epoch: 331\n",
            "\t Training loss 0.00318, Training accuracy 87.18\n",
            "\t Test loss 0.00340, Test accuracy 86.29\n",
            "-----------------------------------------------------\n",
            "Epoch: 332\n",
            "\t Training loss 0.00293, Training accuracy 88.86\n",
            "\t Test loss 0.00390, Test accuracy 84.31\n",
            "-----------------------------------------------------\n",
            "Epoch: 333\n",
            "\t Training loss 0.00307, Training accuracy 88.01\n",
            "\t Test loss 0.00375, Test accuracy 84.50\n",
            "-----------------------------------------------------\n",
            "Epoch: 334\n",
            "\t Training loss 0.00293, Training accuracy 88.77\n",
            "\t Test loss 0.00343, Test accuracy 86.23\n",
            "-----------------------------------------------------\n",
            "Epoch: 335\n",
            "\t Training loss 0.00321, Training accuracy 87.03\n",
            "\t Test loss 0.00365, Test accuracy 85.11\n",
            "-----------------------------------------------------\n",
            "Epoch: 336\n",
            "\t Training loss 0.00311, Training accuracy 88.77\n",
            "\t Test loss 0.00379, Test accuracy 84.99\n",
            "-----------------------------------------------------\n",
            "Epoch: 337\n",
            "\t Training loss 0.00291, Training accuracy 88.31\n",
            "\t Test loss 0.00350, Test accuracy 85.48\n",
            "-----------------------------------------------------\n",
            "Epoch: 338\n",
            "\t Training loss 0.00307, Training accuracy 88.04\n",
            "\t Test loss 0.00351, Test accuracy 85.48\n",
            "-----------------------------------------------------\n",
            "Epoch: 339\n",
            "\t Training loss 0.00288, Training accuracy 88.77\n",
            "\t Test loss 0.00364, Test accuracy 85.42\n",
            "-----------------------------------------------------\n",
            "Epoch: 340\n",
            "\t Training loss 0.00309, Training accuracy 87.88\n",
            "\t Test loss 0.00411, Test accuracy 83.01\n",
            "-----------------------------------------------------\n",
            "Epoch: 341\n",
            "\t Training loss 0.00291, Training accuracy 89.41\n",
            "\t Test loss 0.00345, Test accuracy 85.86\n",
            "-----------------------------------------------------\n",
            "Epoch: 342\n",
            "\t Training loss 0.00301, Training accuracy 88.46\n",
            "\t Test loss 0.00333, Test accuracy 87.46\n",
            "-----------------------------------------------------\n",
            "Epoch: 343\n",
            "\t Training loss 0.00289, Training accuracy 88.55\n",
            "\t Test loss 0.00373, Test accuracy 85.61\n",
            "-----------------------------------------------------\n",
            "Epoch: 344\n",
            "\t Training loss 0.00296, Training accuracy 89.10\n",
            "\t Test loss 0.00340, Test accuracy 86.47\n",
            "-----------------------------------------------------\n",
            "Epoch: 345\n",
            "\t Training loss 0.00303, Training accuracy 87.95\n",
            "\t Test loss 0.00363, Test accuracy 85.86\n",
            "-----------------------------------------------------\n",
            "Epoch: 346\n",
            "\t Training loss 0.00307, Training accuracy 88.28\n",
            "\t Test loss 0.00373, Test accuracy 84.99\n",
            "-----------------------------------------------------\n",
            "Epoch: 347\n",
            "\t Training loss 0.00293, Training accuracy 88.77\n",
            "\t Test loss 0.00364, Test accuracy 86.04\n",
            "-----------------------------------------------------\n",
            "Epoch: 348\n",
            "\t Training loss 0.00290, Training accuracy 88.74\n",
            "\t Test loss 0.00368, Test accuracy 85.30\n",
            "-----------------------------------------------------\n",
            "Epoch: 349\n",
            "\t Training loss 0.00320, Training accuracy 87.61\n",
            "\t Test loss 0.00366, Test accuracy 84.74\n",
            "-----------------------------------------------------\n",
            "Epoch: 350\n",
            "\t Training loss 0.00326, Training accuracy 87.91\n",
            "\t Test loss 0.00343, Test accuracy 86.72\n",
            "-----------------------------------------------------\n",
            "Epoch: 351\n",
            "\t Training loss 0.00309, Training accuracy 87.79\n",
            "\t Test loss 0.00359, Test accuracy 86.10\n",
            "-----------------------------------------------------\n",
            "Epoch: 352\n",
            "\t Training loss 0.00302, Training accuracy 88.13\n",
            "\t Test loss 0.00338, Test accuracy 86.23\n",
            "-----------------------------------------------------\n",
            "Epoch: 353\n",
            "\t Training loss 0.00296, Training accuracy 88.43\n",
            "\t Test loss 0.00359, Test accuracy 85.79\n",
            "-----------------------------------------------------\n",
            "Epoch: 354\n",
            "\t Training loss 0.00293, Training accuracy 88.13\n",
            "\t Test loss 0.00387, Test accuracy 84.31\n",
            "-----------------------------------------------------\n",
            "Epoch: 355\n",
            "\t Training loss 0.00296, Training accuracy 88.01\n",
            "\t Test loss 0.00344, Test accuracy 86.84\n",
            "-----------------------------------------------------\n",
            "Epoch: 356\n",
            "\t Training loss 0.00301, Training accuracy 88.19\n",
            "\t Test loss 0.00360, Test accuracy 86.41\n",
            "-----------------------------------------------------\n",
            "Epoch: 357\n",
            "\t Training loss 0.00305, Training accuracy 87.37\n",
            "\t Test loss 0.00379, Test accuracy 85.18\n",
            "-----------------------------------------------------\n",
            "Epoch: 358\n",
            "\t Training loss 0.00289, Training accuracy 89.16\n",
            "\t Test loss 0.00346, Test accuracy 86.35\n",
            "-----------------------------------------------------\n",
            "Epoch: 359\n",
            "\t Training loss 0.00295, Training accuracy 88.31\n",
            "\t Test loss 0.00379, Test accuracy 85.92\n",
            "-----------------------------------------------------\n",
            "Epoch: 360\n",
            "\t Training loss 0.00299, Training accuracy 89.25\n",
            "\t Test loss 0.00345, Test accuracy 86.23\n",
            "-----------------------------------------------------\n",
            "Epoch: 361\n",
            "\t Training loss 0.00293, Training accuracy 88.31\n",
            "\t Test loss 0.00437, Test accuracy 81.10\n",
            "-----------------------------------------------------\n",
            "Epoch: 362\n",
            "\t Training loss 0.00299, Training accuracy 87.98\n",
            "\t Test loss 0.00336, Test accuracy 85.79\n",
            "-----------------------------------------------------\n",
            "Epoch: 363\n",
            "\t Training loss 0.00298, Training accuracy 88.31\n",
            "\t Test loss 0.00350, Test accuracy 86.23\n",
            "-----------------------------------------------------\n",
            "Epoch: 364\n",
            "\t Training loss 0.00303, Training accuracy 88.68\n",
            "\t Test loss 0.00374, Test accuracy 85.42\n",
            "-----------------------------------------------------\n",
            "Epoch: 365\n",
            "\t Training loss 0.00291, Training accuracy 89.38\n",
            "\t Test loss 0.00334, Test accuracy 86.78\n",
            "-----------------------------------------------------\n",
            "Epoch: 366\n",
            "\t Training loss 0.00289, Training accuracy 88.68\n",
            "\t Test loss 0.00351, Test accuracy 85.55\n",
            "-----------------------------------------------------\n",
            "Epoch: 367\n",
            "\t Training loss 0.00296, Training accuracy 88.71\n",
            "\t Test loss 0.00372, Test accuracy 85.61\n",
            "-----------------------------------------------------\n",
            "Epoch: 368\n",
            "\t Training loss 0.00290, Training accuracy 88.74\n",
            "\t Test loss 0.00376, Test accuracy 85.42\n",
            "-----------------------------------------------------\n",
            "Epoch: 369\n",
            "\t Training loss 0.00294, Training accuracy 89.10\n",
            "\t Test loss 0.00348, Test accuracy 86.16\n",
            "-----------------------------------------------------\n",
            "Epoch: 370\n",
            "\t Training loss 0.00288, Training accuracy 89.32\n",
            "\t Test loss 0.00363, Test accuracy 85.24\n",
            "-----------------------------------------------------\n",
            "Epoch: 371\n",
            "\t Training loss 0.00287, Training accuracy 89.10\n",
            "\t Test loss 0.00354, Test accuracy 84.87\n",
            "-----------------------------------------------------\n",
            "Epoch: 372\n",
            "\t Training loss 0.00299, Training accuracy 88.28\n",
            "\t Test loss 0.00350, Test accuracy 85.61\n",
            "-----------------------------------------------------\n",
            "Epoch: 373\n",
            "\t Training loss 0.00287, Training accuracy 89.01\n",
            "\t Test loss 0.00410, Test accuracy 83.94\n",
            "-----------------------------------------------------\n",
            "Epoch: 374\n",
            "\t Training loss 0.00281, Training accuracy 89.71\n",
            "\t Test loss 0.00340, Test accuracy 87.15\n",
            "-----------------------------------------------------\n",
            "Epoch: 375\n",
            "\t Training loss 0.00301, Training accuracy 88.04\n",
            "\t Test loss 0.00311, Test accuracy 87.65\n",
            "-----------------------------------------------------\n",
            "Epoch: 376\n",
            "\t Training loss 0.00282, Training accuracy 89.59\n",
            "\t Test loss 0.00332, Test accuracy 86.84\n",
            "-----------------------------------------------------\n",
            "Epoch: 377\n",
            "\t Training loss 0.00289, Training accuracy 88.52\n",
            "\t Test loss 0.00439, Test accuracy 82.71\n",
            "-----------------------------------------------------\n",
            "Epoch: 378\n",
            "\t Training loss 0.00293, Training accuracy 88.74\n",
            "\t Test loss 0.00366, Test accuracy 84.87\n",
            "-----------------------------------------------------\n",
            "Epoch: 379\n",
            "\t Training loss 0.00292, Training accuracy 88.43\n",
            "\t Test loss 0.00340, Test accuracy 86.16\n",
            "-----------------------------------------------------\n",
            "Epoch: 380\n",
            "\t Training loss 0.00293, Training accuracy 88.77\n",
            "\t Test loss 0.00366, Test accuracy 84.43\n",
            "-----------------------------------------------------\n",
            "Epoch: 381\n",
            "\t Training loss 0.00298, Training accuracy 88.28\n",
            "\t Test loss 0.00363, Test accuracy 85.36\n",
            "-----------------------------------------------------\n",
            "Epoch: 382\n",
            "\t Training loss 0.00304, Training accuracy 88.19\n",
            "\t Test loss 0.00354, Test accuracy 84.00\n",
            "-----------------------------------------------------\n",
            "Epoch: 383\n",
            "\t Training loss 0.00308, Training accuracy 88.04\n",
            "\t Test loss 0.00333, Test accuracy 86.97\n",
            "-----------------------------------------------------\n",
            "Epoch: 384\n",
            "\t Training loss 0.00289, Training accuracy 89.01\n",
            "\t Test loss 0.00336, Test accuracy 87.28\n",
            "-----------------------------------------------------\n",
            "Epoch: 385\n",
            "\t Training loss 0.00282, Training accuracy 89.25\n",
            "\t Test loss 0.00408, Test accuracy 83.01\n",
            "-----------------------------------------------------\n",
            "Epoch: 386\n",
            "\t Training loss 0.00291, Training accuracy 88.80\n",
            "\t Test loss 0.00338, Test accuracy 87.58\n",
            "-----------------------------------------------------\n",
            "Epoch: 387\n",
            "\t Training loss 0.00273, Training accuracy 89.04\n",
            "\t Test loss 0.00317, Test accuracy 87.28\n",
            "-----------------------------------------------------\n",
            "Epoch: 388\n",
            "\t Training loss 0.00279, Training accuracy 89.16\n",
            "\t Test loss 0.00329, Test accuracy 88.26\n",
            "-----------------------------------------------------\n",
            "Epoch: 389\n",
            "\t Training loss 0.00287, Training accuracy 88.28\n",
            "\t Test loss 0.00347, Test accuracy 87.28\n",
            "-----------------------------------------------------\n",
            "Epoch: 390\n",
            "\t Training loss 0.00291, Training accuracy 89.16\n",
            "\t Test loss 0.00355, Test accuracy 85.86\n",
            "-----------------------------------------------------\n",
            "Epoch: 391\n",
            "\t Training loss 0.00289, Training accuracy 88.34\n",
            "\t Test loss 0.00350, Test accuracy 84.93\n",
            "-----------------------------------------------------\n",
            "Epoch: 392\n",
            "\t Training loss 0.00294, Training accuracy 89.28\n",
            "\t Test loss 0.00352, Test accuracy 85.61\n",
            "-----------------------------------------------------\n",
            "Epoch: 393\n",
            "\t Training loss 0.00296, Training accuracy 88.83\n",
            "\t Test loss 0.00385, Test accuracy 84.62\n",
            "-----------------------------------------------------\n",
            "Epoch: 394\n",
            "\t Training loss 0.00307, Training accuracy 87.61\n",
            "\t Test loss 0.00335, Test accuracy 85.98\n",
            "-----------------------------------------------------\n",
            "Epoch: 395\n",
            "\t Training loss 0.00291, Training accuracy 88.86\n",
            "\t Test loss 0.00351, Test accuracy 85.24\n",
            "-----------------------------------------------------\n",
            "Epoch: 396\n",
            "\t Training loss 0.00287, Training accuracy 88.65\n",
            "\t Test loss 0.00336, Test accuracy 86.41\n",
            "-----------------------------------------------------\n",
            "Epoch: 397\n",
            "\t Training loss 0.00283, Training accuracy 88.25\n",
            "\t Test loss 0.00347, Test accuracy 86.04\n",
            "-----------------------------------------------------\n",
            "Epoch: 398\n",
            "\t Training loss 0.00288, Training accuracy 88.65\n",
            "\t Test loss 0.00368, Test accuracy 85.11\n",
            "-----------------------------------------------------\n",
            "Epoch: 399\n",
            "\t Training loss 0.00304, Training accuracy 88.83\n",
            "\t Test loss 0.00438, Test accuracy 81.59\n",
            "-----------------------------------------------------\n",
            "Epoch: 400\n",
            "\t Training loss 0.00290, Training accuracy 88.95\n",
            "\t Test loss 0.00328, Test accuracy 86.53\n",
            "-----------------------------------------------------\n",
            "Epoch: 401\n",
            "\t Training loss 0.00283, Training accuracy 89.07\n",
            "\t Test loss 0.00349, Test accuracy 86.53\n",
            "-----------------------------------------------------\n",
            "Epoch: 402\n",
            "\t Training loss 0.00279, Training accuracy 89.22\n",
            "\t Test loss 0.00348, Test accuracy 86.16\n",
            "-----------------------------------------------------\n",
            "Epoch: 403\n",
            "\t Training loss 0.00275, Training accuracy 89.68\n",
            "\t Test loss 0.00342, Test accuracy 85.55\n",
            "-----------------------------------------------------\n",
            "Epoch: 404\n",
            "\t Training loss 0.00297, Training accuracy 88.49\n",
            "\t Test loss 0.00355, Test accuracy 84.99\n",
            "-----------------------------------------------------\n",
            "Epoch: 405\n",
            "\t Training loss 0.00271, Training accuracy 89.71\n",
            "\t Test loss 0.00318, Test accuracy 86.72\n",
            "-----------------------------------------------------\n",
            "Epoch: 406\n",
            "\t Training loss 0.00306, Training accuracy 88.46\n",
            "\t Test loss 0.00332, Test accuracy 85.73\n",
            "-----------------------------------------------------\n",
            "Epoch: 407\n",
            "\t Training loss 0.00282, Training accuracy 89.44\n",
            "\t Test loss 0.00357, Test accuracy 85.42\n",
            "-----------------------------------------------------\n",
            "Epoch: 408\n",
            "\t Training loss 0.00281, Training accuracy 89.10\n",
            "\t Test loss 0.00328, Test accuracy 86.16\n",
            "-----------------------------------------------------\n",
            "Epoch: 409\n",
            "\t Training loss 0.00286, Training accuracy 89.07\n",
            "\t Test loss 0.00351, Test accuracy 85.73\n",
            "-----------------------------------------------------\n",
            "Epoch: 410\n",
            "\t Training loss 0.00297, Training accuracy 87.82\n",
            "\t Test loss 0.00441, Test accuracy 82.09\n",
            "-----------------------------------------------------\n",
            "Epoch: 411\n",
            "\t Training loss 0.00289, Training accuracy 88.74\n",
            "\t Test loss 0.00328, Test accuracy 87.09\n",
            "-----------------------------------------------------\n",
            "Epoch: 412\n",
            "\t Training loss 0.00286, Training accuracy 88.89\n",
            "\t Test loss 0.00343, Test accuracy 86.10\n",
            "-----------------------------------------------------\n",
            "Epoch: 413\n",
            "\t Training loss 0.00282, Training accuracy 89.04\n",
            "\t Test loss 0.00330, Test accuracy 86.41\n",
            "-----------------------------------------------------\n",
            "Epoch: 414\n",
            "\t Training loss 0.00284, Training accuracy 89.38\n",
            "\t Test loss 0.00331, Test accuracy 86.97\n",
            "-----------------------------------------------------\n",
            "Epoch: 415\n",
            "\t Training loss 0.00289, Training accuracy 88.83\n",
            "\t Test loss 0.00327, Test accuracy 86.41\n",
            "-----------------------------------------------------\n",
            "Epoch: 416\n",
            "\t Training loss 0.00276, Training accuracy 89.13\n",
            "\t Test loss 0.00384, Test accuracy 84.87\n",
            "-----------------------------------------------------\n",
            "Epoch: 417\n",
            "\t Training loss 0.00286, Training accuracy 88.92\n",
            "\t Test loss 0.00320, Test accuracy 87.96\n",
            "-----------------------------------------------------\n",
            "Epoch: 418\n",
            "\t Training loss 0.00275, Training accuracy 89.62\n",
            "\t Test loss 0.00316, Test accuracy 87.21\n",
            "-----------------------------------------------------\n",
            "Epoch: 419\n",
            "\t Training loss 0.00289, Training accuracy 88.52\n",
            "\t Test loss 0.00352, Test accuracy 85.05\n",
            "-----------------------------------------------------\n",
            "Epoch: 420\n",
            "\t Training loss 0.00294, Training accuracy 88.34\n",
            "\t Test loss 0.00349, Test accuracy 86.16\n",
            "-----------------------------------------------------\n",
            "Epoch: 421\n",
            "\t Training loss 0.00276, Training accuracy 89.53\n",
            "\t Test loss 0.00324, Test accuracy 87.09\n",
            "-----------------------------------------------------\n",
            "Epoch: 422\n",
            "\t Training loss 0.00273, Training accuracy 88.74\n",
            "\t Test loss 0.00336, Test accuracy 86.78\n",
            "-----------------------------------------------------\n",
            "Epoch: 423\n",
            "\t Training loss 0.00284, Training accuracy 88.83\n",
            "\t Test loss 0.00359, Test accuracy 85.42\n",
            "-----------------------------------------------------\n",
            "Epoch: 424\n",
            "\t Training loss 0.00285, Training accuracy 88.68\n",
            "\t Test loss 0.00373, Test accuracy 84.93\n",
            "-----------------------------------------------------\n",
            "Epoch: 425\n",
            "\t Training loss 0.00290, Training accuracy 88.98\n",
            "\t Test loss 0.00345, Test accuracy 86.23\n",
            "-----------------------------------------------------\n",
            "Epoch: 426\n",
            "\t Training loss 0.00295, Training accuracy 88.68\n",
            "\t Test loss 0.00359, Test accuracy 86.16\n",
            "-----------------------------------------------------\n",
            "Epoch: 427\n",
            "\t Training loss 0.00297, Training accuracy 88.65\n",
            "\t Test loss 0.00392, Test accuracy 85.36\n",
            "-----------------------------------------------------\n",
            "Epoch: 428\n",
            "\t Training loss 0.00288, Training accuracy 88.89\n",
            "\t Test loss 0.00355, Test accuracy 85.92\n",
            "-----------------------------------------------------\n",
            "Epoch: 429\n",
            "\t Training loss 0.00294, Training accuracy 88.98\n",
            "\t Test loss 0.00388, Test accuracy 84.50\n",
            "-----------------------------------------------------\n",
            "Epoch: 430\n",
            "\t Training loss 0.00295, Training accuracy 88.55\n",
            "\t Test loss 0.00348, Test accuracy 85.98\n",
            "-----------------------------------------------------\n",
            "Epoch: 431\n",
            "\t Training loss 0.00275, Training accuracy 90.20\n",
            "\t Test loss 0.00328, Test accuracy 86.66\n",
            "-----------------------------------------------------\n",
            "Epoch: 432\n",
            "\t Training loss 0.00283, Training accuracy 88.71\n",
            "\t Test loss 0.00374, Test accuracy 84.68\n",
            "-----------------------------------------------------\n",
            "Epoch: 433\n",
            "\t Training loss 0.00290, Training accuracy 88.28\n",
            "\t Test loss 0.00297, Test accuracy 88.76\n",
            "-----------------------------------------------------\n",
            "Epoch: 434\n",
            "\t Training loss 0.00288, Training accuracy 88.65\n",
            "\t Test loss 0.00332, Test accuracy 86.84\n",
            "-----------------------------------------------------\n",
            "Epoch: 435\n",
            "\t Training loss 0.00272, Training accuracy 89.65\n",
            "\t Test loss 0.00312, Test accuracy 86.66\n",
            "-----------------------------------------------------\n",
            "Epoch: 436\n",
            "\t Training loss 0.00272, Training accuracy 89.44\n",
            "\t Test loss 0.00316, Test accuracy 87.21\n",
            "-----------------------------------------------------\n",
            "Epoch: 437\n",
            "\t Training loss 0.00277, Training accuracy 89.32\n",
            "\t Test loss 0.00355, Test accuracy 86.10\n",
            "-----------------------------------------------------\n",
            "Epoch: 438\n",
            "\t Training loss 0.00267, Training accuracy 90.32\n",
            "\t Test loss 0.00318, Test accuracy 88.20\n",
            "-----------------------------------------------------\n",
            "Epoch: 439\n",
            "\t Training loss 0.00270, Training accuracy 90.29\n",
            "\t Test loss 0.00310, Test accuracy 87.89\n",
            "-----------------------------------------------------\n",
            "Epoch: 440\n",
            "\t Training loss 0.00289, Training accuracy 89.22\n",
            "\t Test loss 0.00302, Test accuracy 89.01\n",
            "-----------------------------------------------------\n",
            "Epoch: 441\n",
            "\t Training loss 0.00277, Training accuracy 89.59\n",
            "\t Test loss 0.00333, Test accuracy 87.52\n",
            "-----------------------------------------------------\n",
            "Epoch: 442\n",
            "\t Training loss 0.00275, Training accuracy 89.25\n",
            "\t Test loss 0.00312, Test accuracy 88.14\n",
            "-----------------------------------------------------\n",
            "Epoch: 443\n",
            "\t Training loss 0.00273, Training accuracy 89.68\n",
            "\t Test loss 0.00324, Test accuracy 88.45\n",
            "-----------------------------------------------------\n",
            "Epoch: 444\n",
            "\t Training loss 0.00272, Training accuracy 89.47\n",
            "\t Test loss 0.00348, Test accuracy 85.92\n",
            "-----------------------------------------------------\n",
            "Epoch: 445\n",
            "\t Training loss 0.00265, Training accuracy 90.20\n",
            "\t Test loss 0.00333, Test accuracy 86.97\n",
            "-----------------------------------------------------\n",
            "Epoch: 446\n",
            "\t Training loss 0.00269, Training accuracy 90.29\n",
            "\t Test loss 0.00330, Test accuracy 87.40\n",
            "-----------------------------------------------------\n",
            "Epoch: 447\n",
            "\t Training loss 0.00277, Training accuracy 89.13\n",
            "\t Test loss 0.00324, Test accuracy 87.09\n",
            "-----------------------------------------------------\n",
            "Epoch: 448\n",
            "\t Training loss 0.00271, Training accuracy 89.77\n",
            "\t Test loss 0.00348, Test accuracy 86.23\n",
            "-----------------------------------------------------\n",
            "Epoch: 449\n",
            "\t Training loss 0.00281, Training accuracy 88.83\n",
            "\t Test loss 0.00314, Test accuracy 87.09\n",
            "-----------------------------------------------------\n",
            "Epoch: 450\n",
            "\t Training loss 0.00275, Training accuracy 89.83\n",
            "\t Test loss 0.00353, Test accuracy 85.42\n",
            "-----------------------------------------------------\n",
            "Epoch: 451\n",
            "\t Training loss 0.00274, Training accuracy 89.53\n",
            "\t Test loss 0.00356, Test accuracy 86.91\n",
            "-----------------------------------------------------\n",
            "Epoch: 452\n",
            "\t Training loss 0.00281, Training accuracy 89.10\n",
            "\t Test loss 0.00333, Test accuracy 86.47\n",
            "-----------------------------------------------------\n",
            "Epoch: 453\n",
            "\t Training loss 0.00286, Training accuracy 89.13\n",
            "\t Test loss 0.00336, Test accuracy 87.71\n",
            "-----------------------------------------------------\n",
            "Epoch: 454\n",
            "\t Training loss 0.00288, Training accuracy 88.40\n",
            "\t Test loss 0.00332, Test accuracy 85.92\n",
            "-----------------------------------------------------\n",
            "Epoch: 455\n",
            "\t Training loss 0.00272, Training accuracy 89.22\n",
            "\t Test loss 0.00337, Test accuracy 87.09\n",
            "-----------------------------------------------------\n",
            "Epoch: 456\n",
            "\t Training loss 0.00275, Training accuracy 90.02\n",
            "\t Test loss 0.00318, Test accuracy 87.58\n",
            "-----------------------------------------------------\n",
            "Epoch: 457\n",
            "\t Training loss 0.00277, Training accuracy 89.16\n",
            "\t Test loss 0.00324, Test accuracy 88.45\n",
            "-----------------------------------------------------\n",
            "Epoch: 458\n",
            "\t Training loss 0.00287, Training accuracy 89.07\n",
            "\t Test loss 0.00317, Test accuracy 87.65\n",
            "-----------------------------------------------------\n",
            "Epoch: 459\n",
            "\t Training loss 0.00280, Training accuracy 89.56\n",
            "\t Test loss 0.00318, Test accuracy 87.52\n",
            "-----------------------------------------------------\n",
            "Epoch: 460\n",
            "\t Training loss 0.00270, Training accuracy 90.02\n",
            "\t Test loss 0.00368, Test accuracy 84.62\n",
            "-----------------------------------------------------\n",
            "Epoch: 461\n",
            "\t Training loss 0.00268, Training accuracy 89.13\n",
            "\t Test loss 0.00306, Test accuracy 87.96\n",
            "-----------------------------------------------------\n",
            "Epoch: 462\n",
            "\t Training loss 0.00271, Training accuracy 89.62\n",
            "\t Test loss 0.00334, Test accuracy 85.61\n",
            "-----------------------------------------------------\n",
            "Epoch: 463\n",
            "\t Training loss 0.00267, Training accuracy 89.19\n",
            "\t Test loss 0.00335, Test accuracy 86.29\n",
            "-----------------------------------------------------\n",
            "Epoch: 464\n",
            "\t Training loss 0.00276, Training accuracy 89.35\n",
            "\t Test loss 0.00321, Test accuracy 88.20\n",
            "-----------------------------------------------------\n",
            "Epoch: 465\n",
            "\t Training loss 0.00274, Training accuracy 89.53\n",
            "\t Test loss 0.00328, Test accuracy 86.84\n",
            "-----------------------------------------------------\n",
            "Epoch: 466\n",
            "\t Training loss 0.00271, Training accuracy 89.80\n",
            "\t Test loss 0.00304, Test accuracy 87.89\n",
            "-----------------------------------------------------\n",
            "Epoch: 467\n",
            "\t Training loss 0.00281, Training accuracy 89.89\n",
            "\t Test loss 0.00366, Test accuracy 85.73\n",
            "-----------------------------------------------------\n",
            "Epoch: 468\n",
            "\t Training loss 0.00278, Training accuracy 88.74\n",
            "\t Test loss 0.00352, Test accuracy 85.24\n",
            "-----------------------------------------------------\n",
            "Epoch: 469\n",
            "\t Training loss 0.00279, Training accuracy 88.89\n",
            "\t Test loss 0.00312, Test accuracy 88.82\n",
            "-----------------------------------------------------\n",
            "Epoch: 470\n",
            "\t Training loss 0.00280, Training accuracy 89.16\n",
            "\t Test loss 0.00326, Test accuracy 86.91\n",
            "-----------------------------------------------------\n",
            "Epoch: 471\n",
            "\t Training loss 0.00272, Training accuracy 90.29\n",
            "\t Test loss 0.00335, Test accuracy 86.29\n",
            "-----------------------------------------------------\n",
            "Epoch: 472\n",
            "\t Training loss 0.00270, Training accuracy 88.80\n",
            "\t Test loss 0.00313, Test accuracy 86.53\n",
            "-----------------------------------------------------\n",
            "Epoch: 473\n",
            "\t Training loss 0.00265, Training accuracy 90.14\n",
            "\t Test loss 0.00309, Test accuracy 87.83\n",
            "-----------------------------------------------------\n",
            "Epoch: 474\n",
            "\t Training loss 0.00268, Training accuracy 88.92\n",
            "\t Test loss 0.00304, Test accuracy 87.77\n",
            "-----------------------------------------------------\n",
            "Epoch: 475\n",
            "\t Training loss 0.00269, Training accuracy 89.74\n",
            "\t Test loss 0.00308, Test accuracy 87.96\n",
            "-----------------------------------------------------\n",
            "Epoch: 476\n",
            "\t Training loss 0.00272, Training accuracy 89.68\n",
            "\t Test loss 0.00306, Test accuracy 87.58\n",
            "-----------------------------------------------------\n",
            "Epoch: 477\n",
            "\t Training loss 0.00274, Training accuracy 89.89\n",
            "\t Test loss 0.00314, Test accuracy 88.20\n",
            "-----------------------------------------------------\n",
            "Epoch: 478\n",
            "\t Training loss 0.00276, Training accuracy 89.10\n",
            "\t Test loss 0.00315, Test accuracy 87.15\n",
            "-----------------------------------------------------\n",
            "Epoch: 479\n",
            "\t Training loss 0.00276, Training accuracy 89.19\n",
            "\t Test loss 0.00326, Test accuracy 88.20\n",
            "-----------------------------------------------------\n",
            "Epoch: 480\n",
            "\t Training loss 0.00283, Training accuracy 90.08\n",
            "\t Test loss 0.00417, Test accuracy 84.93\n",
            "-----------------------------------------------------\n",
            "Epoch: 481\n",
            "\t Training loss 0.00285, Training accuracy 88.83\n",
            "\t Test loss 0.00322, Test accuracy 88.82\n",
            "-----------------------------------------------------\n",
            "Epoch: 482\n",
            "\t Training loss 0.00282, Training accuracy 89.04\n",
            "\t Test loss 0.00329, Test accuracy 86.23\n",
            "-----------------------------------------------------\n",
            "Epoch: 483\n",
            "\t Training loss 0.00270, Training accuracy 90.02\n",
            "\t Test loss 0.00386, Test accuracy 85.36\n",
            "-----------------------------------------------------\n",
            "Epoch: 484\n",
            "\t Training loss 0.00271, Training accuracy 90.11\n",
            "\t Test loss 0.00315, Test accuracy 87.58\n",
            "-----------------------------------------------------\n",
            "Epoch: 485\n",
            "\t Training loss 0.00273, Training accuracy 89.07\n",
            "\t Test loss 0.00309, Test accuracy 87.96\n",
            "-----------------------------------------------------\n",
            "Epoch: 486\n",
            "\t Training loss 0.00270, Training accuracy 89.68\n",
            "\t Test loss 0.00367, Test accuracy 85.18\n",
            "-----------------------------------------------------\n",
            "Epoch: 487\n",
            "\t Training loss 0.00261, Training accuracy 90.59\n",
            "\t Test loss 0.00282, Test accuracy 88.51\n",
            "-----------------------------------------------------\n",
            "Epoch: 488\n",
            "\t Training loss 0.00256, Training accuracy 90.29\n",
            "\t Test loss 0.00336, Test accuracy 85.55\n",
            "-----------------------------------------------------\n",
            "Epoch: 489\n",
            "\t Training loss 0.00261, Training accuracy 89.92\n",
            "\t Test loss 0.00339, Test accuracy 87.34\n",
            "-----------------------------------------------------\n",
            "Epoch: 490\n",
            "\t Training loss 0.00272, Training accuracy 89.59\n",
            "\t Test loss 0.00471, Test accuracy 80.54\n",
            "-----------------------------------------------------\n",
            "Epoch: 491\n",
            "\t Training loss 0.00290, Training accuracy 88.74\n",
            "\t Test loss 0.00334, Test accuracy 86.53\n",
            "-----------------------------------------------------\n",
            "Epoch: 492\n",
            "\t Training loss 0.00289, Training accuracy 88.92\n",
            "\t Test loss 0.00344, Test accuracy 85.86\n",
            "-----------------------------------------------------\n",
            "Epoch: 493\n",
            "\t Training loss 0.00279, Training accuracy 88.89\n",
            "\t Test loss 0.00312, Test accuracy 87.77\n",
            "-----------------------------------------------------\n",
            "Epoch: 494\n",
            "\t Training loss 0.00284, Training accuracy 89.01\n",
            "\t Test loss 0.00367, Test accuracy 85.92\n",
            "-----------------------------------------------------\n",
            "Epoch: 495\n",
            "\t Training loss 0.00278, Training accuracy 88.95\n",
            "\t Test loss 0.00333, Test accuracy 86.23\n",
            "-----------------------------------------------------\n",
            "Epoch: 496\n",
            "\t Training loss 0.00283, Training accuracy 89.80\n",
            "\t Test loss 0.00335, Test accuracy 86.91\n",
            "-----------------------------------------------------\n",
            "Epoch: 497\n",
            "\t Training loss 0.00272, Training accuracy 89.65\n",
            "\t Test loss 0.00306, Test accuracy 88.20\n",
            "-----------------------------------------------------\n",
            "Epoch: 498\n",
            "\t Training loss 0.00267, Training accuracy 89.53\n",
            "\t Test loss 0.00358, Test accuracy 86.72\n",
            "-----------------------------------------------------\n",
            "Epoch: 499\n",
            "\t Training loss 0.00258, Training accuracy 90.38\n",
            "\t Test loss 0.00336, Test accuracy 87.34\n",
            "-----------------------------------------------------\n",
            "Epoch: 500\n",
            "\t Training loss 0.00279, Training accuracy 90.20\n",
            "\t Test loss 0.00317, Test accuracy 86.72\n",
            "-----------------------------------------------------\n",
            "Epoch: 501\n",
            "\t Training loss 0.00268, Training accuracy 90.02\n",
            "\t Test loss 0.00321, Test accuracy 87.46\n",
            "-----------------------------------------------------\n",
            "Epoch: 502\n",
            "\t Training loss 0.00266, Training accuracy 89.86\n",
            "\t Test loss 0.00327, Test accuracy 86.53\n",
            "-----------------------------------------------------\n",
            "Epoch: 503\n",
            "\t Training loss 0.00269, Training accuracy 89.28\n",
            "\t Test loss 0.00306, Test accuracy 88.20\n",
            "-----------------------------------------------------\n",
            "Epoch: 504\n",
            "\t Training loss 0.00266, Training accuracy 89.56\n",
            "\t Test loss 0.00298, Test accuracy 88.57\n",
            "-----------------------------------------------------\n",
            "Epoch: 505\n",
            "\t Training loss 0.00259, Training accuracy 90.02\n",
            "\t Test loss 0.00352, Test accuracy 86.60\n",
            "-----------------------------------------------------\n",
            "Epoch: 506\n",
            "\t Training loss 0.00257, Training accuracy 89.98\n",
            "\t Test loss 0.00326, Test accuracy 87.40\n",
            "-----------------------------------------------------\n",
            "Epoch: 507\n",
            "\t Training loss 0.00264, Training accuracy 89.83\n",
            "\t Test loss 0.00327, Test accuracy 87.77\n",
            "-----------------------------------------------------\n",
            "Epoch: 508\n",
            "\t Training loss 0.00273, Training accuracy 90.02\n",
            "\t Test loss 0.00346, Test accuracy 84.87\n",
            "-----------------------------------------------------\n",
            "Epoch: 509\n",
            "\t Training loss 0.00268, Training accuracy 89.62\n",
            "\t Test loss 0.00343, Test accuracy 87.71\n",
            "-----------------------------------------------------\n",
            "Epoch: 510\n",
            "\t Training loss 0.00258, Training accuracy 90.20\n",
            "\t Test loss 0.00321, Test accuracy 87.34\n",
            "-----------------------------------------------------\n",
            "Epoch: 511\n",
            "\t Training loss 0.00262, Training accuracy 90.17\n",
            "\t Test loss 0.00312, Test accuracy 88.26\n",
            "-----------------------------------------------------\n",
            "Epoch: 512\n",
            "\t Training loss 0.00264, Training accuracy 89.95\n",
            "\t Test loss 0.00290, Test accuracy 88.39\n",
            "-----------------------------------------------------\n",
            "Epoch: 513\n",
            "\t Training loss 0.00281, Training accuracy 89.01\n",
            "\t Test loss 0.00355, Test accuracy 87.34\n",
            "-----------------------------------------------------\n",
            "Epoch: 514\n",
            "\t Training loss 0.00266, Training accuracy 89.89\n",
            "\t Test loss 0.00288, Test accuracy 87.77\n",
            "-----------------------------------------------------\n",
            "Epoch: 515\n",
            "\t Training loss 0.00258, Training accuracy 89.74\n",
            "\t Test loss 0.00310, Test accuracy 87.15\n",
            "-----------------------------------------------------\n",
            "Epoch: 516\n",
            "\t Training loss 0.00254, Training accuracy 90.59\n",
            "\t Test loss 0.00319, Test accuracy 87.77\n",
            "-----------------------------------------------------\n",
            "Epoch: 517\n",
            "\t Training loss 0.00277, Training accuracy 89.28\n",
            "\t Test loss 0.00287, Test accuracy 89.25\n",
            "-----------------------------------------------------\n",
            "Epoch: 518\n",
            "\t Training loss 0.00265, Training accuracy 89.74\n",
            "\t Test loss 0.00333, Test accuracy 87.03\n",
            "-----------------------------------------------------\n",
            "Epoch: 519\n",
            "\t Training loss 0.00257, Training accuracy 90.65\n",
            "\t Test loss 0.00318, Test accuracy 87.83\n",
            "-----------------------------------------------------\n",
            "Epoch: 520\n",
            "\t Training loss 0.00275, Training accuracy 89.28\n",
            "\t Test loss 0.00335, Test accuracy 85.73\n",
            "-----------------------------------------------------\n",
            "Epoch: 521\n",
            "\t Training loss 0.00266, Training accuracy 89.68\n",
            "\t Test loss 0.00339, Test accuracy 86.91\n",
            "-----------------------------------------------------\n",
            "Epoch: 522\n",
            "\t Training loss 0.00260, Training accuracy 89.80\n",
            "\t Test loss 0.00284, Test accuracy 89.50\n",
            "-----------------------------------------------------\n",
            "Epoch: 523\n",
            "\t Training loss 0.00276, Training accuracy 89.77\n",
            "\t Test loss 0.00329, Test accuracy 88.45\n",
            "-----------------------------------------------------\n",
            "Epoch: 524\n",
            "\t Training loss 0.00259, Training accuracy 90.44\n",
            "\t Test loss 0.00330, Test accuracy 87.34\n",
            "-----------------------------------------------------\n",
            "Epoch: 525\n",
            "\t Training loss 0.00267, Training accuracy 89.47\n",
            "\t Test loss 0.00345, Test accuracy 85.86\n",
            "-----------------------------------------------------\n",
            "Epoch: 526\n",
            "\t Training loss 0.00273, Training accuracy 88.52\n",
            "\t Test loss 0.00321, Test accuracy 87.21\n",
            "-----------------------------------------------------\n",
            "Epoch: 527\n",
            "\t Training loss 0.00252, Training accuracy 89.83\n",
            "\t Test loss 0.00296, Test accuracy 89.01\n",
            "-----------------------------------------------------\n",
            "Epoch: 528\n",
            "\t Training loss 0.00270, Training accuracy 89.77\n",
            "\t Test loss 0.00315, Test accuracy 88.51\n",
            "-----------------------------------------------------\n",
            "Epoch: 529\n",
            "\t Training loss 0.00258, Training accuracy 90.29\n",
            "\t Test loss 0.00302, Test accuracy 88.94\n",
            "-----------------------------------------------------\n",
            "Epoch: 530\n",
            "\t Training loss 0.00266, Training accuracy 90.32\n",
            "\t Test loss 0.00319, Test accuracy 88.45\n",
            "-----------------------------------------------------\n",
            "Epoch: 531\n",
            "\t Training loss 0.00260, Training accuracy 90.17\n",
            "\t Test loss 0.00286, Test accuracy 90.24\n",
            "-----------------------------------------------------\n",
            "Epoch: 532\n",
            "\t Training loss 0.00262, Training accuracy 90.35\n",
            "\t Test loss 0.00305, Test accuracy 88.76\n",
            "-----------------------------------------------------\n",
            "Epoch: 533\n",
            "\t Training loss 0.00275, Training accuracy 89.59\n",
            "\t Test loss 0.00310, Test accuracy 88.14\n",
            "-----------------------------------------------------\n",
            "Epoch: 534\n",
            "\t Training loss 0.00254, Training accuracy 90.56\n",
            "\t Test loss 0.00290, Test accuracy 89.25\n",
            "-----------------------------------------------------\n",
            "Epoch: 535\n",
            "\t Training loss 0.00257, Training accuracy 90.35\n",
            "\t Test loss 0.00301, Test accuracy 88.63\n",
            "-----------------------------------------------------\n",
            "Epoch: 536\n",
            "\t Training loss 0.00266, Training accuracy 89.77\n",
            "\t Test loss 0.00316, Test accuracy 87.28\n",
            "-----------------------------------------------------\n",
            "Epoch: 537\n",
            "\t Training loss 0.00268, Training accuracy 89.92\n",
            "\t Test loss 0.00299, Test accuracy 88.76\n",
            "-----------------------------------------------------\n",
            "Epoch: 538\n",
            "\t Training loss 0.00264, Training accuracy 90.62\n",
            "\t Test loss 0.00312, Test accuracy 89.01\n",
            "-----------------------------------------------------\n",
            "Epoch: 539\n",
            "\t Training loss 0.00261, Training accuracy 90.50\n",
            "\t Test loss 0.00390, Test accuracy 84.62\n",
            "-----------------------------------------------------\n",
            "Epoch: 540\n",
            "\t Training loss 0.00263, Training accuracy 89.59\n",
            "\t Test loss 0.00382, Test accuracy 84.68\n",
            "-----------------------------------------------------\n",
            "Epoch: 541\n",
            "\t Training loss 0.00260, Training accuracy 90.50\n",
            "\t Test loss 0.00317, Test accuracy 88.02\n",
            "-----------------------------------------------------\n",
            "Epoch: 542\n",
            "\t Training loss 0.00256, Training accuracy 90.29\n",
            "\t Test loss 0.00290, Test accuracy 89.19\n",
            "-----------------------------------------------------\n",
            "Epoch: 543\n",
            "\t Training loss 0.00254, Training accuracy 90.47\n",
            "\t Test loss 0.00320, Test accuracy 86.78\n",
            "-----------------------------------------------------\n",
            "Epoch: 544\n",
            "\t Training loss 0.00272, Training accuracy 89.53\n",
            "\t Test loss 0.00304, Test accuracy 88.14\n",
            "-----------------------------------------------------\n",
            "Epoch: 545\n",
            "\t Training loss 0.00267, Training accuracy 89.80\n",
            "\t Test loss 0.00341, Test accuracy 87.40\n",
            "-----------------------------------------------------\n",
            "Epoch: 546\n",
            "\t Training loss 0.00263, Training accuracy 89.98\n",
            "\t Test loss 0.00307, Test accuracy 88.33\n",
            "-----------------------------------------------------\n",
            "Epoch: 547\n",
            "\t Training loss 0.00249, Training accuracy 90.90\n",
            "\t Test loss 0.00347, Test accuracy 88.26\n",
            "-----------------------------------------------------\n",
            "Epoch: 548\n",
            "\t Training loss 0.00267, Training accuracy 89.74\n",
            "\t Test loss 0.00312, Test accuracy 87.71\n",
            "-----------------------------------------------------\n",
            "Epoch: 549\n",
            "\t Training loss 0.00271, Training accuracy 89.71\n",
            "\t Test loss 0.00327, Test accuracy 87.03\n",
            "-----------------------------------------------------\n",
            "Epoch: 550\n",
            "\t Training loss 0.00264, Training accuracy 90.05\n",
            "\t Test loss 0.00316, Test accuracy 86.47\n",
            "-----------------------------------------------------\n",
            "Epoch: 551\n",
            "\t Training loss 0.00265, Training accuracy 89.98\n",
            "\t Test loss 0.00296, Test accuracy 88.26\n",
            "-----------------------------------------------------\n",
            "Epoch: 552\n",
            "\t Training loss 0.00252, Training accuracy 90.17\n",
            "\t Test loss 0.00348, Test accuracy 86.78\n",
            "-----------------------------------------------------\n",
            "Epoch: 553\n",
            "\t Training loss 0.00259, Training accuracy 90.23\n",
            "\t Test loss 0.00352, Test accuracy 85.48\n",
            "-----------------------------------------------------\n",
            "Epoch: 554\n",
            "\t Training loss 0.00254, Training accuracy 90.53\n",
            "\t Test loss 0.00324, Test accuracy 87.15\n",
            "-----------------------------------------------------\n",
            "Epoch: 555\n",
            "\t Training loss 0.00254, Training accuracy 90.87\n",
            "\t Test loss 0.00268, Test accuracy 89.38\n",
            "-----------------------------------------------------\n",
            "Epoch: 556\n",
            "\t Training loss 0.00256, Training accuracy 90.29\n",
            "\t Test loss 0.00285, Test accuracy 89.68\n",
            "-----------------------------------------------------\n",
            "Epoch: 557\n",
            "\t Training loss 0.00260, Training accuracy 90.96\n",
            "\t Test loss 0.00306, Test accuracy 87.65\n",
            "-----------------------------------------------------\n",
            "Epoch: 558\n",
            "\t Training loss 0.00256, Training accuracy 90.78\n",
            "\t Test loss 0.00352, Test accuracy 85.36\n",
            "-----------------------------------------------------\n",
            "Epoch: 559\n",
            "\t Training loss 0.00253, Training accuracy 90.84\n",
            "\t Test loss 0.00287, Test accuracy 89.68\n",
            "-----------------------------------------------------\n",
            "Epoch: 560\n",
            "\t Training loss 0.00252, Training accuracy 90.38\n",
            "\t Test loss 0.00307, Test accuracy 88.57\n",
            "-----------------------------------------------------\n",
            "Epoch: 561\n",
            "\t Training loss 0.00253, Training accuracy 90.87\n",
            "\t Test loss 0.00296, Test accuracy 88.14\n",
            "-----------------------------------------------------\n",
            "Epoch: 562\n",
            "\t Training loss 0.00247, Training accuracy 90.90\n",
            "\t Test loss 0.00291, Test accuracy 88.57\n",
            "-----------------------------------------------------\n",
            "Epoch: 563\n",
            "\t Training loss 0.00262, Training accuracy 89.35\n",
            "\t Test loss 0.00342, Test accuracy 85.24\n",
            "-----------------------------------------------------\n",
            "Epoch: 564\n",
            "\t Training loss 0.00250, Training accuracy 90.96\n",
            "\t Test loss 0.00338, Test accuracy 87.15\n",
            "-----------------------------------------------------\n",
            "Epoch: 565\n",
            "\t Training loss 0.00256, Training accuracy 91.11\n",
            "\t Test loss 0.00313, Test accuracy 87.71\n",
            "-----------------------------------------------------\n",
            "Epoch: 566\n",
            "\t Training loss 0.00259, Training accuracy 90.32\n",
            "\t Test loss 0.00324, Test accuracy 88.02\n",
            "-----------------------------------------------------\n",
            "Epoch: 567\n",
            "\t Training loss 0.00251, Training accuracy 90.44\n",
            "\t Test loss 0.00277, Test accuracy 88.88\n",
            "-----------------------------------------------------\n",
            "Epoch: 568\n",
            "\t Training loss 0.00266, Training accuracy 89.89\n",
            "\t Test loss 0.00319, Test accuracy 88.14\n",
            "-----------------------------------------------------\n",
            "Epoch: 569\n",
            "\t Training loss 0.00273, Training accuracy 89.53\n",
            "\t Test loss 0.00332, Test accuracy 86.41\n",
            "-----------------------------------------------------\n",
            "Epoch: 570\n",
            "\t Training loss 0.00261, Training accuracy 89.95\n",
            "\t Test loss 0.00294, Test accuracy 88.33\n",
            "-----------------------------------------------------\n",
            "Epoch: 571\n",
            "\t Training loss 0.00255, Training accuracy 90.50\n",
            "\t Test loss 0.00302, Test accuracy 87.83\n",
            "-----------------------------------------------------\n",
            "Epoch: 572\n",
            "\t Training loss 0.00265, Training accuracy 90.75\n",
            "\t Test loss 0.00472, Test accuracy 81.90\n",
            "-----------------------------------------------------\n",
            "Epoch: 573\n",
            "\t Training loss 0.00281, Training accuracy 88.86\n",
            "\t Test loss 0.00311, Test accuracy 87.65\n",
            "-----------------------------------------------------\n",
            "Epoch: 574\n",
            "\t Training loss 0.00270, Training accuracy 89.25\n",
            "\t Test loss 0.00297, Test accuracy 88.82\n",
            "-----------------------------------------------------\n",
            "Epoch: 575\n",
            "\t Training loss 0.00253, Training accuracy 90.08\n",
            "\t Test loss 0.00325, Test accuracy 87.40\n",
            "-----------------------------------------------------\n",
            "Epoch: 576\n",
            "\t Training loss 0.00263, Training accuracy 89.98\n",
            "\t Test loss 0.00327, Test accuracy 86.72\n",
            "-----------------------------------------------------\n",
            "Epoch: 577\n",
            "\t Training loss 0.00273, Training accuracy 89.47\n",
            "\t Test loss 0.00293, Test accuracy 88.88\n",
            "-----------------------------------------------------\n",
            "Epoch: 578\n",
            "\t Training loss 0.00255, Training accuracy 90.20\n",
            "\t Test loss 0.00277, Test accuracy 88.57\n",
            "-----------------------------------------------------\n",
            "Epoch: 579\n",
            "\t Training loss 0.00253, Training accuracy 90.65\n",
            "\t Test loss 0.00299, Test accuracy 88.57\n",
            "-----------------------------------------------------\n",
            "Epoch: 580\n",
            "\t Training loss 0.00260, Training accuracy 89.74\n",
            "\t Test loss 0.00328, Test accuracy 87.21\n",
            "-----------------------------------------------------\n",
            "Epoch: 581\n",
            "\t Training loss 0.00255, Training accuracy 90.99\n",
            "\t Test loss 0.00287, Test accuracy 88.70\n",
            "-----------------------------------------------------\n",
            "Epoch: 582\n",
            "\t Training loss 0.00251, Training accuracy 90.41\n",
            "\t Test loss 0.00332, Test accuracy 87.40\n",
            "-----------------------------------------------------\n",
            "Epoch: 583\n",
            "\t Training loss 0.00264, Training accuracy 89.62\n",
            "\t Test loss 0.00291, Test accuracy 88.33\n",
            "-----------------------------------------------------\n",
            "Epoch: 584\n",
            "\t Training loss 0.00262, Training accuracy 90.20\n",
            "\t Test loss 0.00282, Test accuracy 89.38\n",
            "-----------------------------------------------------\n",
            "Epoch: 585\n",
            "\t Training loss 0.00248, Training accuracy 90.47\n",
            "\t Test loss 0.00301, Test accuracy 88.39\n",
            "-----------------------------------------------------\n",
            "Epoch: 586\n",
            "\t Training loss 0.00275, Training accuracy 89.28\n",
            "\t Test loss 0.00290, Test accuracy 89.07\n",
            "-----------------------------------------------------\n",
            "Epoch: 587\n",
            "\t Training loss 0.00252, Training accuracy 90.47\n",
            "\t Test loss 0.00295, Test accuracy 89.07\n",
            "-----------------------------------------------------\n",
            "Epoch: 588\n",
            "\t Training loss 0.00266, Training accuracy 89.68\n",
            "\t Test loss 0.00329, Test accuracy 87.77\n",
            "-----------------------------------------------------\n",
            "Epoch: 589\n",
            "\t Training loss 0.00264, Training accuracy 90.44\n",
            "\t Test loss 0.00278, Test accuracy 88.39\n",
            "-----------------------------------------------------\n",
            "Epoch: 590\n",
            "\t Training loss 0.00261, Training accuracy 89.92\n",
            "\t Test loss 0.00329, Test accuracy 88.02\n",
            "-----------------------------------------------------\n",
            "Epoch: 591\n",
            "\t Training loss 0.00250, Training accuracy 90.72\n",
            "\t Test loss 0.00284, Test accuracy 88.26\n",
            "-----------------------------------------------------\n",
            "Epoch: 592\n",
            "\t Training loss 0.00266, Training accuracy 90.26\n",
            "\t Test loss 0.00302, Test accuracy 89.07\n",
            "-----------------------------------------------------\n",
            "Epoch: 593\n",
            "\t Training loss 0.00246, Training accuracy 91.08\n",
            "\t Test loss 0.00342, Test accuracy 86.97\n",
            "-----------------------------------------------------\n",
            "Epoch: 594\n",
            "\t Training loss 0.00260, Training accuracy 89.28\n",
            "\t Test loss 0.00308, Test accuracy 87.58\n",
            "-----------------------------------------------------\n",
            "Epoch: 595\n",
            "\t Training loss 0.00250, Training accuracy 90.41\n",
            "\t Test loss 0.00274, Test accuracy 89.50\n",
            "-----------------------------------------------------\n",
            "Epoch: 596\n",
            "\t Training loss 0.00248, Training accuracy 90.90\n",
            "\t Test loss 0.00294, Test accuracy 89.07\n",
            "-----------------------------------------------------\n",
            "Epoch: 597\n",
            "\t Training loss 0.00274, Training accuracy 89.65\n",
            "\t Test loss 0.00307, Test accuracy 88.26\n",
            "-----------------------------------------------------\n",
            "Epoch: 598\n",
            "\t Training loss 0.00271, Training accuracy 89.28\n",
            "\t Test loss 0.00341, Test accuracy 87.21\n",
            "-----------------------------------------------------\n",
            "Epoch: 599\n",
            "\t Training loss 0.00263, Training accuracy 90.41\n",
            "\t Test loss 0.00329, Test accuracy 87.03\n",
            "-----------------------------------------------------\n",
            "Epoch: 600\n",
            "\t Training loss 0.00239, Training accuracy 91.35\n",
            "\t Test loss 0.00285, Test accuracy 89.68\n",
            "-----------------------------------------------------\n",
            "Epoch: 601\n",
            "\t Training loss 0.00264, Training accuracy 90.47\n",
            "\t Test loss 0.00330, Test accuracy 87.65\n",
            "-----------------------------------------------------\n",
            "Epoch: 602\n",
            "\t Training loss 0.00263, Training accuracy 89.56\n",
            "\t Test loss 0.00314, Test accuracy 88.14\n",
            "-----------------------------------------------------\n",
            "Epoch: 603\n",
            "\t Training loss 0.00262, Training accuracy 90.02\n",
            "\t Test loss 0.00304, Test accuracy 87.89\n",
            "-----------------------------------------------------\n",
            "Epoch: 604\n",
            "\t Training loss 0.00244, Training accuracy 90.99\n",
            "\t Test loss 0.00272, Test accuracy 89.19\n",
            "-----------------------------------------------------\n",
            "Epoch: 605\n",
            "\t Training loss 0.00262, Training accuracy 89.92\n",
            "\t Test loss 0.00368, Test accuracy 86.47\n",
            "-----------------------------------------------------\n",
            "Epoch: 606\n",
            "\t Training loss 0.00255, Training accuracy 90.38\n",
            "\t Test loss 0.00284, Test accuracy 88.76\n",
            "-----------------------------------------------------\n",
            "Epoch: 607\n",
            "\t Training loss 0.00261, Training accuracy 90.68\n",
            "\t Test loss 0.00342, Test accuracy 86.60\n",
            "-----------------------------------------------------\n",
            "Epoch: 608\n",
            "\t Training loss 0.00245, Training accuracy 90.87\n",
            "\t Test loss 0.00302, Test accuracy 89.01\n",
            "-----------------------------------------------------\n",
            "Epoch: 609\n",
            "\t Training loss 0.00248, Training accuracy 90.87\n",
            "\t Test loss 0.00291, Test accuracy 89.31\n",
            "-----------------------------------------------------\n",
            "Epoch: 610\n",
            "\t Training loss 0.00244, Training accuracy 90.68\n",
            "\t Test loss 0.00342, Test accuracy 87.40\n",
            "-----------------------------------------------------\n",
            "Epoch: 611\n",
            "\t Training loss 0.00267, Training accuracy 89.56\n",
            "\t Test loss 0.00321, Test accuracy 87.52\n",
            "-----------------------------------------------------\n",
            "Epoch: 612\n",
            "\t Training loss 0.00259, Training accuracy 90.35\n",
            "\t Test loss 0.00289, Test accuracy 88.76\n",
            "-----------------------------------------------------\n",
            "Epoch: 613\n",
            "\t Training loss 0.00266, Training accuracy 90.23\n",
            "\t Test loss 0.00279, Test accuracy 88.76\n",
            "-----------------------------------------------------\n",
            "Epoch: 614\n",
            "\t Training loss 0.00252, Training accuracy 90.02\n",
            "\t Test loss 0.00296, Test accuracy 87.71\n",
            "-----------------------------------------------------\n",
            "Epoch: 615\n",
            "\t Training loss 0.00256, Training accuracy 90.26\n",
            "\t Test loss 0.00349, Test accuracy 87.71\n",
            "-----------------------------------------------------\n",
            "Epoch: 616\n",
            "\t Training loss 0.00239, Training accuracy 91.45\n",
            "\t Test loss 0.00286, Test accuracy 88.51\n",
            "-----------------------------------------------------\n",
            "Epoch: 617\n",
            "\t Training loss 0.00253, Training accuracy 91.32\n",
            "\t Test loss 0.00330, Test accuracy 86.84\n",
            "-----------------------------------------------------\n",
            "Epoch: 618\n",
            "\t Training loss 0.00260, Training accuracy 90.53\n",
            "\t Test loss 0.00374, Test accuracy 84.25\n",
            "-----------------------------------------------------\n",
            "Epoch: 619\n",
            "\t Training loss 0.00252, Training accuracy 89.95\n",
            "\t Test loss 0.00279, Test accuracy 89.07\n",
            "-----------------------------------------------------\n",
            "Epoch: 620\n",
            "\t Training loss 0.00245, Training accuracy 91.08\n",
            "\t Test loss 0.00292, Test accuracy 89.19\n",
            "-----------------------------------------------------\n",
            "Epoch: 621\n",
            "\t Training loss 0.00242, Training accuracy 90.62\n",
            "\t Test loss 0.00275, Test accuracy 89.68\n",
            "-----------------------------------------------------\n",
            "Epoch: 622\n",
            "\t Training loss 0.00253, Training accuracy 90.65\n",
            "\t Test loss 0.00326, Test accuracy 87.21\n",
            "-----------------------------------------------------\n",
            "Epoch: 623\n",
            "\t Training loss 0.00253, Training accuracy 91.17\n",
            "\t Test loss 0.00305, Test accuracy 88.14\n",
            "-----------------------------------------------------\n",
            "Epoch: 624\n",
            "\t Training loss 0.00262, Training accuracy 90.17\n",
            "\t Test loss 0.00278, Test accuracy 89.68\n",
            "-----------------------------------------------------\n",
            "Epoch: 625\n",
            "\t Training loss 0.00256, Training accuracy 90.50\n",
            "\t Test loss 0.00338, Test accuracy 87.58\n",
            "-----------------------------------------------------\n",
            "Epoch: 626\n",
            "\t Training loss 0.00265, Training accuracy 90.41\n",
            "\t Test loss 0.00325, Test accuracy 87.96\n",
            "-----------------------------------------------------\n",
            "Epoch: 627\n",
            "\t Training loss 0.00251, Training accuracy 89.95\n",
            "\t Test loss 0.00316, Test accuracy 87.77\n",
            "-----------------------------------------------------\n",
            "Epoch: 628\n",
            "\t Training loss 0.00255, Training accuracy 91.08\n",
            "\t Test loss 0.00328, Test accuracy 88.14\n",
            "-----------------------------------------------------\n",
            "Epoch: 629\n",
            "\t Training loss 0.00255, Training accuracy 91.23\n",
            "\t Test loss 0.00300, Test accuracy 88.76\n",
            "-----------------------------------------------------\n",
            "Epoch: 630\n",
            "\t Training loss 0.00253, Training accuracy 90.75\n",
            "\t Test loss 0.00296, Test accuracy 88.63\n",
            "-----------------------------------------------------\n",
            "Epoch: 631\n",
            "\t Training loss 0.00237, Training accuracy 91.48\n",
            "\t Test loss 0.00340, Test accuracy 88.39\n",
            "-----------------------------------------------------\n",
            "Epoch: 632\n",
            "\t Training loss 0.00253, Training accuracy 91.29\n",
            "\t Test loss 0.00306, Test accuracy 89.25\n",
            "-----------------------------------------------------\n",
            "Epoch: 633\n",
            "\t Training loss 0.00251, Training accuracy 90.11\n",
            "\t Test loss 0.00318, Test accuracy 87.52\n",
            "-----------------------------------------------------\n",
            "Epoch: 634\n",
            "\t Training loss 0.00264, Training accuracy 89.95\n",
            "\t Test loss 0.00300, Test accuracy 88.82\n",
            "-----------------------------------------------------\n",
            "Epoch: 635\n",
            "\t Training loss 0.00262, Training accuracy 89.98\n",
            "\t Test loss 0.00293, Test accuracy 88.94\n",
            "-----------------------------------------------------\n",
            "Epoch: 636\n",
            "\t Training loss 0.00252, Training accuracy 90.26\n",
            "\t Test loss 0.00315, Test accuracy 87.65\n",
            "-----------------------------------------------------\n",
            "Epoch: 637\n",
            "\t Training loss 0.00253, Training accuracy 90.47\n",
            "\t Test loss 0.00339, Test accuracy 86.41\n",
            "-----------------------------------------------------\n",
            "Epoch: 638\n",
            "\t Training loss 0.00263, Training accuracy 90.65\n",
            "\t Test loss 0.00352, Test accuracy 86.41\n",
            "-----------------------------------------------------\n",
            "Epoch: 639\n",
            "\t Training loss 0.00246, Training accuracy 91.26\n",
            "\t Test loss 0.00319, Test accuracy 87.96\n",
            "-----------------------------------------------------\n",
            "Epoch: 640\n",
            "\t Training loss 0.00241, Training accuracy 91.35\n",
            "\t Test loss 0.00293, Test accuracy 89.07\n",
            "-----------------------------------------------------\n",
            "Epoch: 641\n",
            "\t Training loss 0.00267, Training accuracy 89.95\n",
            "\t Test loss 0.00349, Test accuracy 87.34\n",
            "-----------------------------------------------------\n",
            "Epoch: 642\n",
            "\t Training loss 0.00269, Training accuracy 89.83\n",
            "\t Test loss 0.00357, Test accuracy 86.16\n",
            "-----------------------------------------------------\n",
            "Epoch: 643\n",
            "\t Training loss 0.00252, Training accuracy 90.84\n",
            "\t Test loss 0.00315, Test accuracy 88.57\n",
            "-----------------------------------------------------\n",
            "Epoch: 644\n",
            "\t Training loss 0.00241, Training accuracy 91.32\n",
            "\t Test loss 0.00309, Test accuracy 88.20\n",
            "-----------------------------------------------------\n",
            "Epoch: 645\n",
            "\t Training loss 0.00247, Training accuracy 91.72\n",
            "\t Test loss 0.00341, Test accuracy 86.66\n",
            "-----------------------------------------------------\n",
            "Epoch: 646\n",
            "\t Training loss 0.00231, Training accuracy 91.48\n",
            "\t Test loss 0.00320, Test accuracy 88.76\n",
            "-----------------------------------------------------\n",
            "Epoch: 647\n",
            "\t Training loss 0.00243, Training accuracy 90.02\n",
            "\t Test loss 0.00301, Test accuracy 88.82\n",
            "-----------------------------------------------------\n",
            "Epoch: 648\n",
            "\t Training loss 0.00256, Training accuracy 90.78\n",
            "\t Test loss 0.00310, Test accuracy 88.94\n",
            "-----------------------------------------------------\n",
            "Epoch: 649\n",
            "\t Training loss 0.00246, Training accuracy 90.87\n",
            "\t Test loss 0.00308, Test accuracy 88.08\n",
            "-----------------------------------------------------\n",
            "Epoch: 650\n",
            "\t Training loss 0.00242, Training accuracy 91.26\n",
            "\t Test loss 0.00307, Test accuracy 89.25\n",
            "-----------------------------------------------------\n",
            "Epoch: 651\n",
            "\t Training loss 0.00260, Training accuracy 90.23\n",
            "\t Test loss 0.00294, Test accuracy 88.88\n",
            "-----------------------------------------------------\n",
            "Epoch: 652\n",
            "\t Training loss 0.00249, Training accuracy 90.41\n",
            "\t Test loss 0.00305, Test accuracy 88.14\n",
            "-----------------------------------------------------\n",
            "Epoch: 653\n",
            "\t Training loss 0.00257, Training accuracy 89.83\n",
            "\t Test loss 0.00311, Test accuracy 88.51\n",
            "-----------------------------------------------------\n",
            "Epoch: 654\n",
            "\t Training loss 0.00255, Training accuracy 90.84\n",
            "\t Test loss 0.00287, Test accuracy 89.19\n",
            "-----------------------------------------------------\n",
            "Epoch: 655\n",
            "\t Training loss 0.00241, Training accuracy 91.20\n",
            "\t Test loss 0.00314, Test accuracy 87.77\n",
            "-----------------------------------------------------\n",
            "Epoch: 656\n",
            "\t Training loss 0.00246, Training accuracy 90.96\n",
            "\t Test loss 0.00274, Test accuracy 90.36\n",
            "-----------------------------------------------------\n",
            "Epoch: 657\n",
            "\t Training loss 0.00245, Training accuracy 90.65\n",
            "\t Test loss 0.00282, Test accuracy 89.62\n",
            "-----------------------------------------------------\n",
            "Epoch: 658\n",
            "\t Training loss 0.00239, Training accuracy 91.66\n",
            "\t Test loss 0.00314, Test accuracy 87.96\n",
            "-----------------------------------------------------\n",
            "Epoch: 659\n",
            "\t Training loss 0.00260, Training accuracy 90.41\n",
            "\t Test loss 0.00291, Test accuracy 89.38\n",
            "-----------------------------------------------------\n",
            "Epoch: 660\n",
            "\t Training loss 0.00253, Training accuracy 89.92\n",
            "\t Test loss 0.00318, Test accuracy 87.40\n",
            "-----------------------------------------------------\n",
            "Epoch: 661\n",
            "\t Training loss 0.00254, Training accuracy 91.20\n",
            "\t Test loss 0.00338, Test accuracy 87.71\n",
            "-----------------------------------------------------\n",
            "Epoch: 662\n",
            "\t Training loss 0.00248, Training accuracy 90.78\n",
            "\t Test loss 0.00291, Test accuracy 89.68\n",
            "-----------------------------------------------------\n",
            "Epoch: 663\n",
            "\t Training loss 0.00248, Training accuracy 90.38\n",
            "\t Test loss 0.00302, Test accuracy 88.39\n",
            "-----------------------------------------------------\n",
            "Epoch: 664\n",
            "\t Training loss 0.00248, Training accuracy 90.53\n",
            "\t Test loss 0.00273, Test accuracy 88.76\n",
            "-----------------------------------------------------\n",
            "Epoch: 665\n",
            "\t Training loss 0.00250, Training accuracy 91.08\n",
            "\t Test loss 0.00294, Test accuracy 89.87\n",
            "-----------------------------------------------------\n",
            "Epoch: 666\n",
            "\t Training loss 0.00254, Training accuracy 90.93\n",
            "\t Test loss 0.00309, Test accuracy 88.57\n",
            "-----------------------------------------------------\n",
            "Epoch: 667\n",
            "\t Training loss 0.00251, Training accuracy 90.90\n",
            "\t Test loss 0.00337, Test accuracy 85.79\n",
            "-----------------------------------------------------\n",
            "Epoch: 668\n",
            "\t Training loss 0.00263, Training accuracy 90.05\n",
            "\t Test loss 0.00305, Test accuracy 88.08\n",
            "-----------------------------------------------------\n",
            "Epoch: 669\n",
            "\t Training loss 0.00251, Training accuracy 90.32\n",
            "\t Test loss 0.00287, Test accuracy 88.45\n",
            "-----------------------------------------------------\n",
            "Epoch: 670\n",
            "\t Training loss 0.00246, Training accuracy 90.93\n",
            "\t Test loss 0.00289, Test accuracy 89.56\n",
            "-----------------------------------------------------\n",
            "Epoch: 671\n",
            "\t Training loss 0.00255, Training accuracy 90.53\n",
            "\t Test loss 0.00298, Test accuracy 89.07\n",
            "-----------------------------------------------------\n",
            "Epoch: 672\n",
            "\t Training loss 0.00250, Training accuracy 90.62\n",
            "\t Test loss 0.00303, Test accuracy 88.33\n",
            "-----------------------------------------------------\n",
            "Epoch: 673\n",
            "\t Training loss 0.00244, Training accuracy 90.81\n",
            "\t Test loss 0.00284, Test accuracy 89.01\n",
            "-----------------------------------------------------\n",
            "Epoch: 674\n",
            "\t Training loss 0.00259, Training accuracy 90.47\n",
            "\t Test loss 0.00318, Test accuracy 87.71\n",
            "-----------------------------------------------------\n",
            "Epoch: 675\n",
            "\t Training loss 0.00249, Training accuracy 90.93\n",
            "\t Test loss 0.00284, Test accuracy 89.38\n",
            "-----------------------------------------------------\n",
            "Epoch: 676\n",
            "\t Training loss 0.00249, Training accuracy 90.32\n",
            "\t Test loss 0.00307, Test accuracy 87.96\n",
            "-----------------------------------------------------\n",
            "Epoch: 677\n",
            "\t Training loss 0.00257, Training accuracy 90.56\n",
            "\t Test loss 0.00315, Test accuracy 88.14\n",
            "-----------------------------------------------------\n",
            "Epoch: 678\n",
            "\t Training loss 0.00246, Training accuracy 90.81\n",
            "\t Test loss 0.00300, Test accuracy 88.26\n",
            "-----------------------------------------------------\n",
            "Epoch: 679\n",
            "\t Training loss 0.00247, Training accuracy 91.02\n",
            "\t Test loss 0.00304, Test accuracy 87.89\n",
            "-----------------------------------------------------\n",
            "Epoch: 680\n",
            "\t Training loss 0.00237, Training accuracy 91.35\n",
            "\t Test loss 0.00262, Test accuracy 90.06\n",
            "-----------------------------------------------------\n",
            "Epoch: 681\n",
            "\t Training loss 0.00245, Training accuracy 90.50\n",
            "\t Test loss 0.00288, Test accuracy 88.82\n",
            "-----------------------------------------------------\n",
            "Epoch: 682\n",
            "\t Training loss 0.00245, Training accuracy 90.20\n",
            "\t Test loss 0.00272, Test accuracy 90.24\n",
            "-----------------------------------------------------\n",
            "Epoch: 683\n",
            "\t Training loss 0.00241, Training accuracy 90.59\n",
            "\t Test loss 0.00306, Test accuracy 87.83\n",
            "-----------------------------------------------------\n",
            "Epoch: 684\n",
            "\t Training loss 0.00246, Training accuracy 90.26\n",
            "\t Test loss 0.00300, Test accuracy 86.97\n",
            "-----------------------------------------------------\n",
            "Epoch: 685\n",
            "\t Training loss 0.00243, Training accuracy 91.14\n",
            "\t Test loss 0.00342, Test accuracy 86.47\n",
            "-----------------------------------------------------\n",
            "Epoch: 686\n",
            "\t Training loss 0.00242, Training accuracy 90.99\n",
            "\t Test loss 0.00283, Test accuracy 89.07\n",
            "-----------------------------------------------------\n",
            "Epoch: 687\n",
            "\t Training loss 0.00247, Training accuracy 91.14\n",
            "\t Test loss 0.00267, Test accuracy 90.43\n",
            "-----------------------------------------------------\n",
            "Epoch: 688\n",
            "\t Training loss 0.00258, Training accuracy 90.32\n",
            "\t Test loss 0.00282, Test accuracy 89.50\n",
            "-----------------------------------------------------\n",
            "Epoch: 689\n",
            "\t Training loss 0.00237, Training accuracy 91.57\n",
            "\t Test loss 0.00279, Test accuracy 89.68\n",
            "-----------------------------------------------------\n",
            "Epoch: 690\n",
            "\t Training loss 0.00249, Training accuracy 90.75\n",
            "\t Test loss 0.00274, Test accuracy 90.12\n",
            "-----------------------------------------------------\n",
            "Epoch: 691\n",
            "\t Training loss 0.00242, Training accuracy 91.26\n",
            "\t Test loss 0.00298, Test accuracy 89.13\n",
            "-----------------------------------------------------\n",
            "Epoch: 692\n",
            "\t Training loss 0.00238, Training accuracy 91.26\n",
            "\t Test loss 0.00277, Test accuracy 89.68\n",
            "-----------------------------------------------------\n",
            "Epoch: 693\n",
            "\t Training loss 0.00251, Training accuracy 89.80\n",
            "\t Test loss 0.00328, Test accuracy 87.46\n",
            "-----------------------------------------------------\n",
            "Epoch: 694\n",
            "\t Training loss 0.00240, Training accuracy 90.87\n",
            "\t Test loss 0.00288, Test accuracy 88.63\n",
            "-----------------------------------------------------\n",
            "Epoch: 695\n",
            "\t Training loss 0.00246, Training accuracy 90.78\n",
            "\t Test loss 0.00311, Test accuracy 87.46\n",
            "-----------------------------------------------------\n",
            "Epoch: 696\n",
            "\t Training loss 0.00250, Training accuracy 89.53\n",
            "\t Test loss 0.00314, Test accuracy 88.76\n",
            "-----------------------------------------------------\n",
            "Epoch: 697\n",
            "\t Training loss 0.00251, Training accuracy 90.32\n",
            "\t Test loss 0.00292, Test accuracy 88.51\n",
            "-----------------------------------------------------\n",
            "Epoch: 698\n",
            "\t Training loss 0.00247, Training accuracy 90.87\n",
            "\t Test loss 0.00315, Test accuracy 88.39\n",
            "-----------------------------------------------------\n",
            "Epoch: 699\n",
            "\t Training loss 0.00245, Training accuracy 90.38\n",
            "\t Test loss 0.00316, Test accuracy 88.26\n",
            "-----------------------------------------------------\n",
            "Epoch: 700\n",
            "\t Training loss 0.00246, Training accuracy 90.93\n",
            "\t Test loss 0.00281, Test accuracy 89.62\n",
            "-----------------------------------------------------\n",
            "Epoch: 701\n",
            "\t Training loss 0.00239, Training accuracy 90.90\n",
            "\t Test loss 0.00344, Test accuracy 87.77\n",
            "-----------------------------------------------------\n",
            "Epoch: 702\n",
            "\t Training loss 0.00235, Training accuracy 91.66\n",
            "\t Test loss 0.00268, Test accuracy 89.99\n",
            "-----------------------------------------------------\n",
            "Epoch: 703\n",
            "\t Training loss 0.00253, Training accuracy 89.62\n",
            "\t Test loss 0.00302, Test accuracy 87.96\n",
            "-----------------------------------------------------\n",
            "Epoch: 704\n",
            "\t Training loss 0.00239, Training accuracy 90.75\n",
            "\t Test loss 0.00328, Test accuracy 88.82\n",
            "-----------------------------------------------------\n",
            "Epoch: 705\n",
            "\t Training loss 0.00240, Training accuracy 91.96\n",
            "\t Test loss 0.00280, Test accuracy 89.87\n",
            "-----------------------------------------------------\n",
            "Epoch: 706\n",
            "\t Training loss 0.00243, Training accuracy 91.26\n",
            "\t Test loss 0.00315, Test accuracy 87.71\n",
            "-----------------------------------------------------\n",
            "Epoch: 707\n",
            "\t Training loss 0.00257, Training accuracy 90.23\n",
            "\t Test loss 0.00301, Test accuracy 88.14\n",
            "-----------------------------------------------------\n",
            "Epoch: 708\n",
            "\t Training loss 0.00233, Training accuracy 91.63\n",
            "\t Test loss 0.00271, Test accuracy 89.68\n",
            "-----------------------------------------------------\n",
            "Epoch: 709\n",
            "\t Training loss 0.00232, Training accuracy 91.32\n",
            "\t Test loss 0.00264, Test accuracy 89.93\n",
            "-----------------------------------------------------\n",
            "Epoch: 710\n",
            "\t Training loss 0.00258, Training accuracy 89.80\n",
            "\t Test loss 0.00257, Test accuracy 90.49\n",
            "-----------------------------------------------------\n",
            "Epoch: 711\n",
            "\t Training loss 0.00237, Training accuracy 90.65\n",
            "\t Test loss 0.00276, Test accuracy 89.81\n",
            "-----------------------------------------------------\n",
            "Epoch: 712\n",
            "\t Training loss 0.00231, Training accuracy 92.02\n",
            "\t Test loss 0.00278, Test accuracy 89.99\n",
            "-----------------------------------------------------\n",
            "Epoch: 713\n",
            "\t Training loss 0.00240, Training accuracy 91.29\n",
            "\t Test loss 0.00317, Test accuracy 87.34\n",
            "-----------------------------------------------------\n",
            "Epoch: 714\n",
            "\t Training loss 0.00262, Training accuracy 89.77\n",
            "\t Test loss 0.00310, Test accuracy 87.15\n",
            "-----------------------------------------------------\n",
            "Epoch: 715\n",
            "\t Training loss 0.00251, Training accuracy 90.78\n",
            "\t Test loss 0.00302, Test accuracy 88.51\n",
            "-----------------------------------------------------\n",
            "Epoch: 716\n",
            "\t Training loss 0.00261, Training accuracy 90.35\n",
            "\t Test loss 0.00294, Test accuracy 89.19\n",
            "-----------------------------------------------------\n",
            "Epoch: 717\n",
            "\t Training loss 0.00237, Training accuracy 91.39\n",
            "\t Test loss 0.00312, Test accuracy 88.76\n",
            "-----------------------------------------------------\n",
            "Epoch: 718\n",
            "\t Training loss 0.00243, Training accuracy 91.17\n",
            "\t Test loss 0.00293, Test accuracy 89.93\n",
            "-----------------------------------------------------\n",
            "Epoch: 719\n",
            "\t Training loss 0.00246, Training accuracy 91.17\n",
            "\t Test loss 0.00303, Test accuracy 89.25\n",
            "-----------------------------------------------------\n",
            "Epoch: 720\n",
            "\t Training loss 0.00251, Training accuracy 90.41\n",
            "\t Test loss 0.00272, Test accuracy 89.38\n",
            "-----------------------------------------------------\n",
            "Epoch: 721\n",
            "\t Training loss 0.00255, Training accuracy 89.89\n",
            "\t Test loss 0.00317, Test accuracy 88.14\n",
            "-----------------------------------------------------\n",
            "Epoch: 722\n",
            "\t Training loss 0.00252, Training accuracy 90.41\n",
            "\t Test loss 0.00640, Test accuracy 78.51\n",
            "-----------------------------------------------------\n",
            "Epoch: 723\n",
            "\t Training loss 0.00274, Training accuracy 89.50\n",
            "\t Test loss 0.00326, Test accuracy 87.52\n",
            "-----------------------------------------------------\n",
            "Epoch: 724\n",
            "\t Training loss 0.00253, Training accuracy 90.50\n",
            "\t Test loss 0.00307, Test accuracy 87.34\n",
            "-----------------------------------------------------\n",
            "Epoch: 725\n",
            "\t Training loss 0.00253, Training accuracy 90.35\n",
            "\t Test loss 0.00268, Test accuracy 89.62\n",
            "-----------------------------------------------------\n",
            "Epoch: 726\n",
            "\t Training loss 0.00260, Training accuracy 90.02\n",
            "\t Test loss 0.00273, Test accuracy 89.38\n",
            "-----------------------------------------------------\n",
            "Epoch: 727\n",
            "\t Training loss 0.00250, Training accuracy 90.47\n",
            "\t Test loss 0.00312, Test accuracy 88.08\n",
            "-----------------------------------------------------\n",
            "Epoch: 728\n",
            "\t Training loss 0.00248, Training accuracy 90.87\n",
            "\t Test loss 0.00420, Test accuracy 84.43\n",
            "-----------------------------------------------------\n",
            "Epoch: 729\n",
            "\t Training loss 0.00274, Training accuracy 89.86\n",
            "\t Test loss 0.00379, Test accuracy 84.68\n",
            "-----------------------------------------------------\n",
            "Epoch: 730\n",
            "\t Training loss 0.00250, Training accuracy 90.17\n",
            "\t Test loss 0.00265, Test accuracy 88.57\n",
            "-----------------------------------------------------\n",
            "Epoch: 731\n",
            "\t Training loss 0.00252, Training accuracy 90.78\n",
            "\t Test loss 0.00300, Test accuracy 89.07\n",
            "-----------------------------------------------------\n",
            "Epoch: 732\n",
            "\t Training loss 0.00247, Training accuracy 90.99\n",
            "\t Test loss 0.00281, Test accuracy 89.75\n",
            "-----------------------------------------------------\n",
            "Epoch: 733\n",
            "\t Training loss 0.00249, Training accuracy 91.05\n",
            "\t Test loss 0.00262, Test accuracy 90.67\n",
            "-----------------------------------------------------\n",
            "Epoch: 734\n",
            "\t Training loss 0.00250, Training accuracy 90.44\n",
            "\t Test loss 0.00302, Test accuracy 88.82\n",
            "-----------------------------------------------------\n",
            "Epoch: 735\n",
            "\t Training loss 0.00234, Training accuracy 91.29\n",
            "\t Test loss 0.00279, Test accuracy 90.12\n",
            "-----------------------------------------------------\n",
            "Epoch: 736\n",
            "\t Training loss 0.00239, Training accuracy 91.02\n",
            "\t Test loss 0.00277, Test accuracy 89.62\n",
            "-----------------------------------------------------\n",
            "Epoch: 737\n",
            "\t Training loss 0.00243, Training accuracy 91.45\n",
            "\t Test loss 0.00277, Test accuracy 88.82\n",
            "-----------------------------------------------------\n",
            "Epoch: 738\n",
            "\t Training loss 0.00234, Training accuracy 91.35\n",
            "\t Test loss 0.00271, Test accuracy 88.26\n",
            "-----------------------------------------------------\n",
            "Epoch: 739\n",
            "\t Training loss 0.00252, Training accuracy 90.75\n",
            "\t Test loss 0.00275, Test accuracy 89.31\n",
            "-----------------------------------------------------\n",
            "Epoch: 740\n",
            "\t Training loss 0.00271, Training accuracy 89.89\n",
            "\t Test loss 0.00325, Test accuracy 88.26\n",
            "-----------------------------------------------------\n",
            "Epoch: 741\n",
            "\t Training loss 0.00239, Training accuracy 91.51\n",
            "\t Test loss 0.00275, Test accuracy 89.99\n",
            "-----------------------------------------------------\n",
            "Epoch: 742\n",
            "\t Training loss 0.00254, Training accuracy 90.59\n",
            "\t Test loss 0.00265, Test accuracy 90.12\n",
            "-----------------------------------------------------\n",
            "Epoch: 743\n",
            "\t Training loss 0.00231, Training accuracy 91.75\n",
            "\t Test loss 0.00280, Test accuracy 89.31\n",
            "-----------------------------------------------------\n",
            "Epoch: 744\n",
            "\t Training loss 0.00240, Training accuracy 90.65\n",
            "\t Test loss 0.00277, Test accuracy 90.12\n",
            "-----------------------------------------------------\n",
            "Epoch: 745\n",
            "\t Training loss 0.00246, Training accuracy 90.87\n",
            "\t Test loss 0.00267, Test accuracy 90.67\n",
            "-----------------------------------------------------\n",
            "Epoch: 746\n",
            "\t Training loss 0.00231, Training accuracy 92.54\n",
            "\t Test loss 0.00274, Test accuracy 90.74\n",
            "-----------------------------------------------------\n",
            "Epoch: 747\n",
            "\t Training loss 0.00234, Training accuracy 91.32\n",
            "\t Test loss 0.00289, Test accuracy 90.43\n",
            "-----------------------------------------------------\n",
            "Epoch: 748\n",
            "\t Training loss 0.00242, Training accuracy 91.39\n",
            "\t Test loss 0.00270, Test accuracy 89.25\n",
            "-----------------------------------------------------\n",
            "Epoch: 749\n",
            "\t Training loss 0.00228, Training accuracy 91.54\n",
            "\t Test loss 0.00345, Test accuracy 87.46\n",
            "-----------------------------------------------------\n",
            "Epoch: 750\n",
            "\t Training loss 0.00243, Training accuracy 90.65\n",
            "\t Test loss 0.00283, Test accuracy 88.63\n",
            "-----------------------------------------------------\n",
            "Epoch: 751\n",
            "\t Training loss 0.00240, Training accuracy 90.68\n",
            "\t Test loss 0.00302, Test accuracy 88.14\n",
            "-----------------------------------------------------\n",
            "Epoch: 752\n",
            "\t Training loss 0.00241, Training accuracy 90.99\n",
            "\t Test loss 0.00273, Test accuracy 89.38\n",
            "-----------------------------------------------------\n",
            "Epoch: 753\n",
            "\t Training loss 0.00254, Training accuracy 90.44\n",
            "\t Test loss 0.00319, Test accuracy 86.78\n",
            "-----------------------------------------------------\n",
            "Epoch: 754\n",
            "\t Training loss 0.00243, Training accuracy 91.05\n",
            "\t Test loss 0.00306, Test accuracy 88.45\n",
            "-----------------------------------------------------\n",
            "Epoch: 755\n",
            "\t Training loss 0.00250, Training accuracy 90.65\n",
            "\t Test loss 0.00305, Test accuracy 88.82\n",
            "-----------------------------------------------------\n",
            "Epoch: 756\n",
            "\t Training loss 0.00248, Training accuracy 90.90\n",
            "\t Test loss 0.00278, Test accuracy 90.55\n",
            "-----------------------------------------------------\n",
            "Epoch: 757\n",
            "\t Training loss 0.00234, Training accuracy 91.48\n",
            "\t Test loss 0.00277, Test accuracy 89.93\n",
            "-----------------------------------------------------\n",
            "Epoch: 758\n",
            "\t Training loss 0.00231, Training accuracy 91.35\n",
            "\t Test loss 0.00274, Test accuracy 88.94\n",
            "-----------------------------------------------------\n",
            "Epoch: 759\n",
            "\t Training loss 0.00235, Training accuracy 91.11\n",
            "\t Test loss 0.00335, Test accuracy 87.58\n",
            "-----------------------------------------------------\n",
            "Epoch: 760\n",
            "\t Training loss 0.00251, Training accuracy 90.68\n",
            "\t Test loss 0.00303, Test accuracy 88.33\n",
            "-----------------------------------------------------\n",
            "Epoch: 761\n",
            "\t Training loss 0.00256, Training accuracy 91.05\n",
            "\t Test loss 0.00269, Test accuracy 89.38\n",
            "-----------------------------------------------------\n",
            "Epoch: 762\n",
            "\t Training loss 0.00250, Training accuracy 91.05\n",
            "\t Test loss 0.00295, Test accuracy 89.62\n",
            "-----------------------------------------------------\n",
            "Epoch: 763\n",
            "\t Training loss 0.00232, Training accuracy 91.63\n",
            "\t Test loss 0.00300, Test accuracy 89.68\n",
            "-----------------------------------------------------\n",
            "Epoch: 764\n",
            "\t Training loss 0.00245, Training accuracy 91.54\n",
            "\t Test loss 0.00306, Test accuracy 88.08\n",
            "-----------------------------------------------------\n",
            "Epoch: 765\n",
            "\t Training loss 0.00254, Training accuracy 90.75\n",
            "\t Test loss 0.00287, Test accuracy 88.14\n",
            "-----------------------------------------------------\n",
            "Epoch: 766\n",
            "\t Training loss 0.00239, Training accuracy 91.29\n",
            "\t Test loss 0.00291, Test accuracy 89.25\n",
            "-----------------------------------------------------\n",
            "Epoch: 767\n",
            "\t Training loss 0.00242, Training accuracy 91.69\n",
            "\t Test loss 0.00297, Test accuracy 89.38\n",
            "-----------------------------------------------------\n",
            "Epoch: 768\n",
            "\t Training loss 0.00231, Training accuracy 91.63\n",
            "\t Test loss 0.00283, Test accuracy 89.13\n",
            "-----------------------------------------------------\n",
            "Epoch: 769\n",
            "\t Training loss 0.00246, Training accuracy 90.87\n",
            "\t Test loss 0.00339, Test accuracy 87.21\n",
            "-----------------------------------------------------\n",
            "Epoch: 770\n",
            "\t Training loss 0.00243, Training accuracy 90.26\n",
            "\t Test loss 0.00273, Test accuracy 90.67\n",
            "-----------------------------------------------------\n",
            "Epoch: 771\n",
            "\t Training loss 0.00240, Training accuracy 90.93\n",
            "\t Test loss 0.00299, Test accuracy 89.13\n",
            "-----------------------------------------------------\n",
            "Epoch: 772\n",
            "\t Training loss 0.00244, Training accuracy 91.90\n",
            "\t Test loss 0.00283, Test accuracy 90.36\n",
            "-----------------------------------------------------\n",
            "Epoch: 773\n",
            "\t Training loss 0.00236, Training accuracy 91.32\n",
            "\t Test loss 0.00301, Test accuracy 89.81\n",
            "-----------------------------------------------------\n",
            "Epoch: 774\n",
            "\t Training loss 0.00241, Training accuracy 90.93\n",
            "\t Test loss 0.00286, Test accuracy 89.81\n",
            "-----------------------------------------------------\n",
            "Epoch: 775\n",
            "\t Training loss 0.00244, Training accuracy 91.72\n",
            "\t Test loss 0.00285, Test accuracy 89.25\n",
            "-----------------------------------------------------\n",
            "Epoch: 776\n",
            "\t Training loss 0.00246, Training accuracy 90.90\n",
            "\t Test loss 0.00317, Test accuracy 88.02\n",
            "-----------------------------------------------------\n",
            "Epoch: 777\n",
            "\t Training loss 0.00234, Training accuracy 91.93\n",
            "\t Test loss 0.00266, Test accuracy 89.38\n",
            "-----------------------------------------------------\n",
            "Epoch: 778\n",
            "\t Training loss 0.00231, Training accuracy 91.69\n",
            "\t Test loss 0.00305, Test accuracy 90.24\n",
            "-----------------------------------------------------\n",
            "Epoch: 779\n",
            "\t Training loss 0.00251, Training accuracy 90.44\n",
            "\t Test loss 0.00298, Test accuracy 89.38\n",
            "-----------------------------------------------------\n",
            "Epoch: 780\n",
            "\t Training loss 0.00249, Training accuracy 90.75\n",
            "\t Test loss 0.00296, Test accuracy 88.08\n",
            "-----------------------------------------------------\n",
            "Epoch: 781\n",
            "\t Training loss 0.00239, Training accuracy 91.54\n",
            "\t Test loss 0.00300, Test accuracy 89.87\n",
            "-----------------------------------------------------\n",
            "Epoch: 782\n",
            "\t Training loss 0.00239, Training accuracy 91.20\n",
            "\t Test loss 0.00271, Test accuracy 90.18\n",
            "-----------------------------------------------------\n",
            "Epoch: 783\n",
            "\t Training loss 0.00249, Training accuracy 90.75\n",
            "\t Test loss 0.00286, Test accuracy 89.50\n",
            "-----------------------------------------------------\n",
            "Epoch: 784\n",
            "\t Training loss 0.00247, Training accuracy 90.90\n",
            "\t Test loss 0.00389, Test accuracy 85.61\n",
            "-----------------------------------------------------\n",
            "Epoch: 785\n",
            "\t Training loss 0.00256, Training accuracy 90.93\n",
            "\t Test loss 0.00301, Test accuracy 88.94\n",
            "-----------------------------------------------------\n",
            "Epoch: 786\n",
            "\t Training loss 0.00245, Training accuracy 90.93\n",
            "\t Test loss 0.00283, Test accuracy 89.50\n",
            "-----------------------------------------------------\n",
            "Epoch: 787\n",
            "\t Training loss 0.00251, Training accuracy 90.75\n",
            "\t Test loss 0.00264, Test accuracy 90.61\n",
            "-----------------------------------------------------\n",
            "Epoch: 788\n",
            "\t Training loss 0.00223, Training accuracy 92.57\n",
            "\t Test loss 0.00298, Test accuracy 88.57\n",
            "-----------------------------------------------------\n",
            "Epoch: 789\n",
            "\t Training loss 0.00237, Training accuracy 91.32\n",
            "\t Test loss 0.00290, Test accuracy 89.99\n",
            "-----------------------------------------------------\n",
            "Epoch: 790\n",
            "\t Training loss 0.00248, Training accuracy 89.71\n",
            "\t Test loss 0.00290, Test accuracy 90.43\n",
            "-----------------------------------------------------\n",
            "Epoch: 791\n",
            "\t Training loss 0.00244, Training accuracy 90.56\n",
            "\t Test loss 0.00290, Test accuracy 88.51\n",
            "-----------------------------------------------------\n",
            "Epoch: 792\n",
            "\t Training loss 0.00236, Training accuracy 91.75\n",
            "\t Test loss 0.00278, Test accuracy 89.62\n",
            "-----------------------------------------------------\n",
            "Epoch: 793\n",
            "\t Training loss 0.00239, Training accuracy 91.05\n",
            "\t Test loss 0.00265, Test accuracy 90.55\n",
            "-----------------------------------------------------\n",
            "Epoch: 794\n",
            "\t Training loss 0.00247, Training accuracy 91.17\n",
            "\t Test loss 0.00291, Test accuracy 88.33\n",
            "-----------------------------------------------------\n",
            "Epoch: 795\n",
            "\t Training loss 0.00240, Training accuracy 91.14\n",
            "\t Test loss 0.00278, Test accuracy 90.18\n",
            "-----------------------------------------------------\n",
            "Epoch: 796\n",
            "\t Training loss 0.00246, Training accuracy 90.81\n",
            "\t Test loss 0.00311, Test accuracy 87.34\n",
            "-----------------------------------------------------\n",
            "Epoch: 797\n",
            "\t Training loss 0.00242, Training accuracy 91.11\n",
            "\t Test loss 0.00296, Test accuracy 88.94\n",
            "-----------------------------------------------------\n",
            "Epoch: 798\n",
            "\t Training loss 0.00245, Training accuracy 91.02\n",
            "\t Test loss 0.00286, Test accuracy 89.50\n",
            "-----------------------------------------------------\n",
            "Epoch: 799\n",
            "\t Training loss 0.00245, Training accuracy 91.35\n",
            "\t Test loss 0.00303, Test accuracy 89.01\n",
            "-----------------------------------------------------\n",
            "Epoch: 800\n",
            "\t Training loss 0.00238, Training accuracy 90.68\n",
            "\t Test loss 0.00280, Test accuracy 88.39\n",
            "-----------------------------------------------------\n",
            "Epoch: 801\n",
            "\t Training loss 0.00240, Training accuracy 91.02\n",
            "\t Test loss 0.00268, Test accuracy 90.80\n",
            "-----------------------------------------------------\n",
            "Epoch: 802\n",
            "\t Training loss 0.00232, Training accuracy 91.57\n",
            "\t Test loss 0.00269, Test accuracy 89.50\n",
            "-----------------------------------------------------\n",
            "Epoch: 803\n",
            "\t Training loss 0.00242, Training accuracy 90.93\n",
            "\t Test loss 0.00343, Test accuracy 86.53\n",
            "-----------------------------------------------------\n",
            "Epoch: 804\n",
            "\t Training loss 0.00256, Training accuracy 90.41\n",
            "\t Test loss 0.00282, Test accuracy 89.68\n",
            "-----------------------------------------------------\n",
            "Epoch: 805\n",
            "\t Training loss 0.00236, Training accuracy 91.11\n",
            "\t Test loss 0.00280, Test accuracy 89.87\n",
            "-----------------------------------------------------\n",
            "Epoch: 806\n",
            "\t Training loss 0.00228, Training accuracy 91.57\n",
            "\t Test loss 0.00297, Test accuracy 89.31\n",
            "-----------------------------------------------------\n",
            "Epoch: 807\n",
            "\t Training loss 0.00225, Training accuracy 92.12\n",
            "\t Test loss 0.00278, Test accuracy 90.61\n",
            "-----------------------------------------------------\n",
            "Epoch: 808\n",
            "\t Training loss 0.00231, Training accuracy 91.05\n",
            "\t Test loss 0.00273, Test accuracy 89.19\n",
            "-----------------------------------------------------\n",
            "Epoch: 809\n",
            "\t Training loss 0.00235, Training accuracy 91.42\n",
            "\t Test loss 0.00308, Test accuracy 89.56\n",
            "-----------------------------------------------------\n",
            "Epoch: 810\n",
            "\t Training loss 0.00235, Training accuracy 91.69\n",
            "\t Test loss 0.00272, Test accuracy 90.36\n",
            "-----------------------------------------------------\n",
            "Epoch: 811\n",
            "\t Training loss 0.00247, Training accuracy 90.78\n",
            "\t Test loss 0.00305, Test accuracy 89.93\n",
            "-----------------------------------------------------\n",
            "Epoch: 812\n",
            "\t Training loss 0.00253, Training accuracy 90.53\n",
            "\t Test loss 0.00353, Test accuracy 86.35\n",
            "-----------------------------------------------------\n",
            "Epoch: 813\n",
            "\t Training loss 0.00235, Training accuracy 91.75\n",
            "\t Test loss 0.00285, Test accuracy 90.06\n",
            "-----------------------------------------------------\n",
            "Epoch: 814\n",
            "\t Training loss 0.00247, Training accuracy 90.90\n",
            "\t Test loss 0.00285, Test accuracy 89.44\n",
            "-----------------------------------------------------\n",
            "Epoch: 815\n",
            "\t Training loss 0.00232, Training accuracy 92.02\n",
            "\t Test loss 0.00285, Test accuracy 88.70\n",
            "-----------------------------------------------------\n",
            "Epoch: 816\n",
            "\t Training loss 0.00237, Training accuracy 90.90\n",
            "\t Test loss 0.00299, Test accuracy 89.13\n",
            "-----------------------------------------------------\n",
            "Epoch: 817\n",
            "\t Training loss 0.00236, Training accuracy 90.99\n",
            "\t Test loss 0.00249, Test accuracy 90.55\n",
            "-----------------------------------------------------\n",
            "Epoch: 818\n",
            "\t Training loss 0.00235, Training accuracy 91.60\n",
            "\t Test loss 0.00284, Test accuracy 89.19\n",
            "-----------------------------------------------------\n",
            "Epoch: 819\n",
            "\t Training loss 0.00238, Training accuracy 90.96\n",
            "\t Test loss 0.00276, Test accuracy 89.68\n",
            "-----------------------------------------------------\n",
            "Epoch: 820\n",
            "\t Training loss 0.00246, Training accuracy 91.29\n",
            "\t Test loss 0.00278, Test accuracy 89.93\n",
            "-----------------------------------------------------\n",
            "Epoch: 821\n",
            "\t Training loss 0.00234, Training accuracy 91.20\n",
            "\t Test loss 0.00287, Test accuracy 89.19\n",
            "-----------------------------------------------------\n",
            "Epoch: 822\n",
            "\t Training loss 0.00243, Training accuracy 91.26\n",
            "\t Test loss 0.00309, Test accuracy 88.70\n",
            "-----------------------------------------------------\n",
            "Epoch: 823\n",
            "\t Training loss 0.00224, Training accuracy 92.27\n",
            "\t Test loss 0.00314, Test accuracy 87.52\n",
            "-----------------------------------------------------\n",
            "Epoch: 824\n",
            "\t Training loss 0.00253, Training accuracy 90.90\n",
            "\t Test loss 0.00276, Test accuracy 89.13\n",
            "-----------------------------------------------------\n",
            "Epoch: 825\n",
            "\t Training loss 0.00253, Training accuracy 90.96\n",
            "\t Test loss 0.00274, Test accuracy 89.99\n",
            "-----------------------------------------------------\n",
            "Epoch: 826\n",
            "\t Training loss 0.00238, Training accuracy 90.99\n",
            "\t Test loss 0.00296, Test accuracy 89.68\n",
            "-----------------------------------------------------\n",
            "Epoch: 827\n",
            "\t Training loss 0.00241, Training accuracy 91.02\n",
            "\t Test loss 0.00282, Test accuracy 90.24\n",
            "-----------------------------------------------------\n",
            "Epoch: 828\n",
            "\t Training loss 0.00236, Training accuracy 91.14\n",
            "\t Test loss 0.00281, Test accuracy 88.70\n",
            "-----------------------------------------------------\n",
            "Epoch: 829\n",
            "\t Training loss 0.00241, Training accuracy 91.05\n",
            "\t Test loss 0.00276, Test accuracy 89.62\n",
            "-----------------------------------------------------\n",
            "Epoch: 830\n",
            "\t Training loss 0.00243, Training accuracy 90.84\n",
            "\t Test loss 0.00274, Test accuracy 89.81\n",
            "-----------------------------------------------------\n",
            "Epoch: 831\n",
            "\t Training loss 0.00229, Training accuracy 91.75\n",
            "\t Test loss 0.00272, Test accuracy 90.24\n",
            "-----------------------------------------------------\n",
            "Epoch: 832\n",
            "\t Training loss 0.00230, Training accuracy 91.96\n",
            "\t Test loss 0.00254, Test accuracy 90.36\n",
            "-----------------------------------------------------\n",
            "Epoch: 833\n",
            "\t Training loss 0.00247, Training accuracy 90.93\n",
            "\t Test loss 0.00277, Test accuracy 89.44\n",
            "-----------------------------------------------------\n",
            "Epoch: 834\n",
            "\t Training loss 0.00235, Training accuracy 91.32\n",
            "\t Test loss 0.00283, Test accuracy 90.49\n",
            "-----------------------------------------------------\n",
            "Epoch: 835\n",
            "\t Training loss 0.00237, Training accuracy 91.96\n",
            "\t Test loss 0.00303, Test accuracy 89.50\n",
            "-----------------------------------------------------\n",
            "Epoch: 836\n",
            "\t Training loss 0.00260, Training accuracy 90.23\n",
            "\t Test loss 0.00338, Test accuracy 88.02\n",
            "-----------------------------------------------------\n",
            "Epoch: 837\n",
            "\t Training loss 0.00230, Training accuracy 91.54\n",
            "\t Test loss 0.00313, Test accuracy 88.08\n",
            "-----------------------------------------------------\n",
            "Epoch: 838\n",
            "\t Training loss 0.00245, Training accuracy 90.50\n",
            "\t Test loss 0.00297, Test accuracy 89.38\n",
            "-----------------------------------------------------\n",
            "Epoch: 839\n",
            "\t Training loss 0.00250, Training accuracy 90.50\n",
            "\t Test loss 0.00291, Test accuracy 88.88\n",
            "-----------------------------------------------------\n",
            "Epoch: 840\n",
            "\t Training loss 0.00257, Training accuracy 90.29\n",
            "\t Test loss 0.00322, Test accuracy 88.76\n",
            "-----------------------------------------------------\n",
            "Epoch: 841\n",
            "\t Training loss 0.00250, Training accuracy 90.68\n",
            "\t Test loss 0.00276, Test accuracy 90.49\n",
            "-----------------------------------------------------\n",
            "Epoch: 842\n",
            "\t Training loss 0.00238, Training accuracy 90.59\n",
            "\t Test loss 0.00263, Test accuracy 91.29\n",
            "-----------------------------------------------------\n",
            "Epoch: 843\n",
            "\t Training loss 0.00224, Training accuracy 90.75\n",
            "\t Test loss 0.00270, Test accuracy 90.36\n",
            "-----------------------------------------------------\n",
            "Epoch: 844\n",
            "\t Training loss 0.00248, Training accuracy 91.23\n",
            "\t Test loss 0.00281, Test accuracy 90.12\n",
            "-----------------------------------------------------\n",
            "Epoch: 845\n",
            "\t Training loss 0.00234, Training accuracy 91.35\n",
            "\t Test loss 0.00287, Test accuracy 89.25\n",
            "-----------------------------------------------------\n",
            "Epoch: 846\n",
            "\t Training loss 0.00247, Training accuracy 90.44\n",
            "\t Test loss 0.00284, Test accuracy 90.30\n",
            "-----------------------------------------------------\n",
            "Epoch: 847\n",
            "\t Training loss 0.00233, Training accuracy 91.29\n",
            "\t Test loss 0.00258, Test accuracy 90.67\n",
            "-----------------------------------------------------\n",
            "Epoch: 848\n",
            "\t Training loss 0.00229, Training accuracy 91.32\n",
            "\t Test loss 0.00298, Test accuracy 89.25\n",
            "-----------------------------------------------------\n",
            "Epoch: 849\n",
            "\t Training loss 0.00229, Training accuracy 91.51\n",
            "\t Test loss 0.00278, Test accuracy 90.74\n",
            "-----------------------------------------------------\n",
            "Epoch: 850\n",
            "\t Training loss 0.00227, Training accuracy 91.54\n",
            "\t Test loss 0.00301, Test accuracy 89.81\n",
            "-----------------------------------------------------\n",
            "Epoch: 851\n",
            "\t Training loss 0.00248, Training accuracy 90.68\n",
            "\t Test loss 0.00266, Test accuracy 90.24\n",
            "-----------------------------------------------------\n",
            "Epoch: 852\n",
            "\t Training loss 0.00224, Training accuracy 91.93\n",
            "\t Test loss 0.00256, Test accuracy 90.74\n",
            "-----------------------------------------------------\n",
            "Epoch: 853\n",
            "\t Training loss 0.00236, Training accuracy 91.54\n",
            "\t Test loss 0.00278, Test accuracy 90.12\n",
            "-----------------------------------------------------\n",
            "Epoch: 854\n",
            "\t Training loss 0.00219, Training accuracy 92.05\n",
            "\t Test loss 0.00278, Test accuracy 89.68\n",
            "-----------------------------------------------------\n",
            "Epoch: 855\n",
            "\t Training loss 0.00232, Training accuracy 91.11\n",
            "\t Test loss 0.00271, Test accuracy 89.75\n",
            "-----------------------------------------------------\n",
            "Epoch: 856\n",
            "\t Training loss 0.00217, Training accuracy 92.60\n",
            "\t Test loss 0.00275, Test accuracy 89.50\n",
            "-----------------------------------------------------\n",
            "Epoch: 857\n",
            "\t Training loss 0.00229, Training accuracy 91.57\n",
            "\t Test loss 0.00276, Test accuracy 89.75\n",
            "-----------------------------------------------------\n",
            "Epoch: 858\n",
            "\t Training loss 0.00250, Training accuracy 90.81\n",
            "\t Test loss 0.00292, Test accuracy 89.99\n",
            "-----------------------------------------------------\n",
            "Epoch: 859\n",
            "\t Training loss 0.00236, Training accuracy 91.39\n",
            "\t Test loss 0.00285, Test accuracy 88.51\n",
            "-----------------------------------------------------\n",
            "Epoch: 860\n",
            "\t Training loss 0.00239, Training accuracy 90.90\n",
            "\t Test loss 0.00290, Test accuracy 90.18\n",
            "-----------------------------------------------------\n",
            "Epoch: 861\n",
            "\t Training loss 0.00244, Training accuracy 90.75\n",
            "\t Test loss 0.00278, Test accuracy 89.99\n",
            "-----------------------------------------------------\n",
            "Epoch: 862\n",
            "\t Training loss 0.00256, Training accuracy 90.38\n",
            "\t Test loss 0.00265, Test accuracy 90.49\n",
            "-----------------------------------------------------\n",
            "Epoch: 863\n",
            "\t Training loss 0.00223, Training accuracy 91.72\n",
            "\t Test loss 0.00258, Test accuracy 91.85\n",
            "-----------------------------------------------------\n",
            "Epoch: 864\n",
            "\t Training loss 0.00232, Training accuracy 92.18\n",
            "\t Test loss 0.00265, Test accuracy 89.81\n",
            "-----------------------------------------------------\n",
            "Epoch: 865\n",
            "\t Training loss 0.00238, Training accuracy 91.48\n",
            "\t Test loss 0.00273, Test accuracy 90.06\n",
            "-----------------------------------------------------\n",
            "Epoch: 866\n",
            "\t Training loss 0.00227, Training accuracy 91.42\n",
            "\t Test loss 0.00245, Test accuracy 90.67\n",
            "-----------------------------------------------------\n",
            "Epoch: 867\n",
            "\t Training loss 0.00236, Training accuracy 91.78\n",
            "\t Test loss 0.00267, Test accuracy 89.87\n",
            "-----------------------------------------------------\n",
            "Epoch: 868\n",
            "\t Training loss 0.00239, Training accuracy 91.17\n",
            "\t Test loss 0.00255, Test accuracy 90.80\n",
            "-----------------------------------------------------\n",
            "Epoch: 869\n",
            "\t Training loss 0.00230, Training accuracy 91.42\n",
            "\t Test loss 0.00262, Test accuracy 89.68\n",
            "-----------------------------------------------------\n",
            "Epoch: 870\n",
            "\t Training loss 0.00218, Training accuracy 91.81\n",
            "\t Test loss 0.00317, Test accuracy 88.14\n",
            "-----------------------------------------------------\n",
            "Epoch: 871\n",
            "\t Training loss 0.00238, Training accuracy 90.93\n",
            "\t Test loss 0.00293, Test accuracy 87.96\n",
            "-----------------------------------------------------\n",
            "Epoch: 872\n",
            "\t Training loss 0.00240, Training accuracy 90.11\n",
            "\t Test loss 0.00248, Test accuracy 90.92\n",
            "-----------------------------------------------------\n",
            "Epoch: 873\n",
            "\t Training loss 0.00244, Training accuracy 90.26\n",
            "\t Test loss 0.00302, Test accuracy 88.70\n",
            "-----------------------------------------------------\n",
            "Epoch: 874\n",
            "\t Training loss 0.00239, Training accuracy 91.02\n",
            "\t Test loss 0.00286, Test accuracy 89.68\n",
            "-----------------------------------------------------\n",
            "Epoch: 875\n",
            "\t Training loss 0.00239, Training accuracy 91.32\n",
            "\t Test loss 0.00279, Test accuracy 90.86\n",
            "-----------------------------------------------------\n",
            "Epoch: 876\n",
            "\t Training loss 0.00243, Training accuracy 91.14\n",
            "\t Test loss 0.00278, Test accuracy 89.01\n",
            "-----------------------------------------------------\n",
            "Epoch: 877\n",
            "\t Training loss 0.00233, Training accuracy 91.87\n",
            "\t Test loss 0.00272, Test accuracy 89.50\n",
            "-----------------------------------------------------\n",
            "Epoch: 878\n",
            "\t Training loss 0.00237, Training accuracy 90.99\n",
            "\t Test loss 0.00260, Test accuracy 91.41\n",
            "-----------------------------------------------------\n",
            "Epoch: 879\n",
            "\t Training loss 0.00238, Training accuracy 91.63\n",
            "\t Test loss 0.00308, Test accuracy 89.75\n",
            "-----------------------------------------------------\n",
            "Epoch: 880\n",
            "\t Training loss 0.00234, Training accuracy 91.42\n",
            "\t Test loss 0.00261, Test accuracy 90.55\n",
            "-----------------------------------------------------\n",
            "Epoch: 881\n",
            "\t Training loss 0.00225, Training accuracy 91.84\n",
            "\t Test loss 0.00256, Test accuracy 91.66\n",
            "-----------------------------------------------------\n",
            "Epoch: 882\n",
            "\t Training loss 0.00237, Training accuracy 91.08\n",
            "\t Test loss 0.00280, Test accuracy 89.19\n",
            "-----------------------------------------------------\n",
            "Epoch: 883\n",
            "\t Training loss 0.00222, Training accuracy 92.48\n",
            "\t Test loss 0.00262, Test accuracy 90.30\n",
            "-----------------------------------------------------\n",
            "Epoch: 884\n",
            "\t Training loss 0.00241, Training accuracy 90.68\n",
            "\t Test loss 0.00273, Test accuracy 89.87\n",
            "-----------------------------------------------------\n",
            "Epoch: 885\n",
            "\t Training loss 0.00241, Training accuracy 90.96\n",
            "\t Test loss 0.00268, Test accuracy 89.75\n",
            "-----------------------------------------------------\n",
            "Epoch: 886\n",
            "\t Training loss 0.00232, Training accuracy 91.35\n",
            "\t Test loss 0.00265, Test accuracy 90.12\n",
            "-----------------------------------------------------\n",
            "Epoch: 887\n",
            "\t Training loss 0.00227, Training accuracy 92.02\n",
            "\t Test loss 0.00330, Test accuracy 87.40\n",
            "-----------------------------------------------------\n",
            "Epoch: 888\n",
            "\t Training loss 0.00243, Training accuracy 91.32\n",
            "\t Test loss 0.00271, Test accuracy 89.38\n",
            "-----------------------------------------------------\n",
            "Epoch: 889\n",
            "\t Training loss 0.00229, Training accuracy 91.57\n",
            "\t Test loss 0.00302, Test accuracy 89.44\n",
            "-----------------------------------------------------\n",
            "Epoch: 890\n",
            "\t Training loss 0.00231, Training accuracy 92.15\n",
            "\t Test loss 0.00240, Test accuracy 92.34\n",
            "-----------------------------------------------------\n",
            "Epoch: 891\n",
            "\t Training loss 0.00231, Training accuracy 91.23\n",
            "\t Test loss 0.00320, Test accuracy 88.63\n",
            "-----------------------------------------------------\n",
            "Epoch: 892\n",
            "\t Training loss 0.00231, Training accuracy 91.60\n",
            "\t Test loss 0.00255, Test accuracy 89.99\n",
            "-----------------------------------------------------\n",
            "Epoch: 893\n",
            "\t Training loss 0.00221, Training accuracy 92.09\n",
            "\t Test loss 0.00278, Test accuracy 90.98\n",
            "-----------------------------------------------------\n",
            "Epoch: 894\n",
            "\t Training loss 0.00244, Training accuracy 91.75\n",
            "\t Test loss 0.00280, Test accuracy 89.75\n",
            "-----------------------------------------------------\n",
            "Epoch: 895\n",
            "\t Training loss 0.00238, Training accuracy 91.20\n",
            "\t Test loss 0.00271, Test accuracy 90.67\n",
            "-----------------------------------------------------\n",
            "Epoch: 896\n",
            "\t Training loss 0.00229, Training accuracy 91.81\n",
            "\t Test loss 0.00273, Test accuracy 89.62\n",
            "-----------------------------------------------------\n",
            "Epoch: 897\n",
            "\t Training loss 0.00223, Training accuracy 91.48\n",
            "\t Test loss 0.00275, Test accuracy 89.87\n",
            "-----------------------------------------------------\n",
            "Epoch: 898\n",
            "\t Training loss 0.00237, Training accuracy 91.45\n",
            "\t Test loss 0.00280, Test accuracy 90.24\n",
            "-----------------------------------------------------\n",
            "Epoch: 899\n",
            "\t Training loss 0.00230, Training accuracy 91.72\n",
            "\t Test loss 0.00259, Test accuracy 90.98\n",
            "-----------------------------------------------------\n",
            "Epoch: 900\n",
            "\t Training loss 0.00234, Training accuracy 91.08\n",
            "\t Test loss 0.00274, Test accuracy 89.87\n",
            "-----------------------------------------------------\n",
            "Epoch: 901\n",
            "\t Training loss 0.00225, Training accuracy 91.39\n",
            "\t Test loss 0.00272, Test accuracy 89.75\n",
            "-----------------------------------------------------\n",
            "Epoch: 902\n",
            "\t Training loss 0.00216, Training accuracy 91.99\n",
            "\t Test loss 0.00279, Test accuracy 89.93\n",
            "-----------------------------------------------------\n",
            "Epoch: 903\n",
            "\t Training loss 0.00240, Training accuracy 91.23\n",
            "\t Test loss 0.00292, Test accuracy 89.25\n",
            "-----------------------------------------------------\n",
            "Epoch: 904\n",
            "\t Training loss 0.00233, Training accuracy 91.84\n",
            "\t Test loss 0.00282, Test accuracy 89.68\n",
            "-----------------------------------------------------\n",
            "Epoch: 905\n",
            "\t Training loss 0.00233, Training accuracy 91.02\n",
            "\t Test loss 0.00296, Test accuracy 89.19\n",
            "-----------------------------------------------------\n",
            "Epoch: 906\n",
            "\t Training loss 0.00227, Training accuracy 91.93\n",
            "\t Test loss 0.00270, Test accuracy 89.75\n",
            "-----------------------------------------------------\n",
            "Epoch: 907\n",
            "\t Training loss 0.00230, Training accuracy 91.90\n",
            "\t Test loss 0.00261, Test accuracy 89.56\n",
            "-----------------------------------------------------\n",
            "Epoch: 908\n",
            "\t Training loss 0.00234, Training accuracy 91.23\n",
            "\t Test loss 0.00303, Test accuracy 88.39\n",
            "-----------------------------------------------------\n",
            "Epoch: 909\n",
            "\t Training loss 0.00227, Training accuracy 91.45\n",
            "\t Test loss 0.00248, Test accuracy 90.80\n",
            "-----------------------------------------------------\n",
            "Epoch: 910\n",
            "\t Training loss 0.00223, Training accuracy 91.78\n",
            "\t Test loss 0.00281, Test accuracy 90.24\n",
            "-----------------------------------------------------\n",
            "Epoch: 911\n",
            "\t Training loss 0.00227, Training accuracy 91.17\n",
            "\t Test loss 0.00272, Test accuracy 90.49\n",
            "-----------------------------------------------------\n",
            "Epoch: 912\n",
            "\t Training loss 0.00243, Training accuracy 90.47\n",
            "\t Test loss 0.00268, Test accuracy 90.61\n",
            "-----------------------------------------------------\n",
            "Epoch: 913\n",
            "\t Training loss 0.00219, Training accuracy 91.78\n",
            "\t Test loss 0.00263, Test accuracy 89.81\n",
            "-----------------------------------------------------\n",
            "Epoch: 914\n",
            "\t Training loss 0.00246, Training accuracy 90.78\n",
            "\t Test loss 0.00265, Test accuracy 89.62\n",
            "-----------------------------------------------------\n",
            "Epoch: 915\n",
            "\t Training loss 0.00233, Training accuracy 91.39\n",
            "\t Test loss 0.00290, Test accuracy 89.56\n",
            "-----------------------------------------------------\n",
            "Epoch: 916\n",
            "\t Training loss 0.00235, Training accuracy 91.29\n",
            "\t Test loss 0.00299, Test accuracy 88.51\n",
            "-----------------------------------------------------\n",
            "Epoch: 917\n",
            "\t Training loss 0.00224, Training accuracy 91.96\n",
            "\t Test loss 0.00247, Test accuracy 91.23\n",
            "-----------------------------------------------------\n",
            "Epoch: 918\n",
            "\t Training loss 0.00227, Training accuracy 92.51\n",
            "\t Test loss 0.00276, Test accuracy 89.81\n",
            "-----------------------------------------------------\n",
            "Epoch: 919\n",
            "\t Training loss 0.00237, Training accuracy 91.69\n",
            "\t Test loss 0.00307, Test accuracy 89.31\n",
            "-----------------------------------------------------\n",
            "Epoch: 920\n",
            "\t Training loss 0.00229, Training accuracy 91.90\n",
            "\t Test loss 0.00248, Test accuracy 89.93\n",
            "-----------------------------------------------------\n",
            "Epoch: 921\n",
            "\t Training loss 0.00240, Training accuracy 91.93\n",
            "\t Test loss 0.00343, Test accuracy 88.94\n",
            "-----------------------------------------------------\n",
            "Epoch: 922\n",
            "\t Training loss 0.00237, Training accuracy 90.93\n",
            "\t Test loss 0.00314, Test accuracy 88.57\n",
            "-----------------------------------------------------\n",
            "Epoch: 923\n",
            "\t Training loss 0.00239, Training accuracy 90.96\n",
            "\t Test loss 0.00289, Test accuracy 88.94\n",
            "-----------------------------------------------------\n",
            "Epoch: 924\n",
            "\t Training loss 0.00228, Training accuracy 91.20\n",
            "\t Test loss 0.00263, Test accuracy 90.86\n",
            "-----------------------------------------------------\n",
            "Epoch: 925\n",
            "\t Training loss 0.00246, Training accuracy 90.90\n",
            "\t Test loss 0.00265, Test accuracy 89.38\n",
            "-----------------------------------------------------\n",
            "Epoch: 926\n",
            "\t Training loss 0.00242, Training accuracy 91.23\n",
            "\t Test loss 0.00280, Test accuracy 89.25\n",
            "-----------------------------------------------------\n",
            "Epoch: 927\n",
            "\t Training loss 0.00238, Training accuracy 91.54\n",
            "\t Test loss 0.00382, Test accuracy 87.15\n",
            "-----------------------------------------------------\n",
            "Epoch: 928\n",
            "\t Training loss 0.00230, Training accuracy 91.57\n",
            "\t Test loss 0.00267, Test accuracy 89.44\n",
            "-----------------------------------------------------\n",
            "Epoch: 929\n",
            "\t Training loss 0.00234, Training accuracy 91.42\n",
            "\t Test loss 0.00271, Test accuracy 89.75\n",
            "-----------------------------------------------------\n",
            "Epoch: 930\n",
            "\t Training loss 0.00239, Training accuracy 91.84\n",
            "\t Test loss 0.00277, Test accuracy 90.67\n",
            "-----------------------------------------------------\n",
            "Epoch: 931\n",
            "\t Training loss 0.00238, Training accuracy 91.17\n",
            "\t Test loss 0.00320, Test accuracy 88.45\n",
            "-----------------------------------------------------\n",
            "Epoch: 932\n",
            "\t Training loss 0.00236, Training accuracy 90.90\n",
            "\t Test loss 0.00267, Test accuracy 90.36\n",
            "-----------------------------------------------------\n",
            "Epoch: 933\n",
            "\t Training loss 0.00230, Training accuracy 91.84\n",
            "\t Test loss 0.00268, Test accuracy 90.18\n",
            "-----------------------------------------------------\n",
            "Epoch: 934\n",
            "\t Training loss 0.00233, Training accuracy 90.75\n",
            "\t Test loss 0.00267, Test accuracy 89.62\n",
            "-----------------------------------------------------\n",
            "Epoch: 935\n",
            "\t Training loss 0.00235, Training accuracy 91.72\n",
            "\t Test loss 0.00271, Test accuracy 89.38\n",
            "-----------------------------------------------------\n",
            "Epoch: 936\n",
            "\t Training loss 0.00237, Training accuracy 91.26\n",
            "\t Test loss 0.00287, Test accuracy 89.31\n",
            "-----------------------------------------------------\n",
            "Epoch: 937\n",
            "\t Training loss 0.00230, Training accuracy 91.23\n",
            "\t Test loss 0.00268, Test accuracy 89.38\n",
            "-----------------------------------------------------\n",
            "Epoch: 938\n",
            "\t Training loss 0.00219, Training accuracy 91.93\n",
            "\t Test loss 0.00288, Test accuracy 89.68\n",
            "-----------------------------------------------------\n",
            "Epoch: 939\n",
            "\t Training loss 0.00224, Training accuracy 91.90\n",
            "\t Test loss 0.00291, Test accuracy 90.24\n",
            "-----------------------------------------------------\n",
            "Epoch: 940\n",
            "\t Training loss 0.00236, Training accuracy 91.93\n",
            "\t Test loss 0.00265, Test accuracy 91.60\n",
            "-----------------------------------------------------\n",
            "Epoch: 941\n",
            "\t Training loss 0.00238, Training accuracy 91.11\n",
            "\t Test loss 0.00273, Test accuracy 89.50\n",
            "-----------------------------------------------------\n",
            "Epoch: 942\n",
            "\t Training loss 0.00233, Training accuracy 91.75\n",
            "\t Test loss 0.00271, Test accuracy 89.93\n",
            "-----------------------------------------------------\n",
            "Epoch: 943\n",
            "\t Training loss 0.00231, Training accuracy 91.96\n",
            "\t Test loss 0.00289, Test accuracy 88.33\n",
            "-----------------------------------------------------\n",
            "Epoch: 944\n",
            "\t Training loss 0.00225, Training accuracy 91.75\n",
            "\t Test loss 0.00255, Test accuracy 90.80\n",
            "-----------------------------------------------------\n",
            "Epoch: 945\n",
            "\t Training loss 0.00221, Training accuracy 91.87\n",
            "\t Test loss 0.00262, Test accuracy 90.30\n",
            "-----------------------------------------------------\n",
            "Epoch: 946\n",
            "\t Training loss 0.00237, Training accuracy 91.20\n",
            "\t Test loss 0.00281, Test accuracy 90.24\n",
            "-----------------------------------------------------\n",
            "Epoch: 947\n",
            "\t Training loss 0.00239, Training accuracy 91.45\n",
            "\t Test loss 0.00294, Test accuracy 89.50\n",
            "-----------------------------------------------------\n",
            "Epoch: 948\n",
            "\t Training loss 0.00231, Training accuracy 91.60\n",
            "\t Test loss 0.00287, Test accuracy 89.87\n",
            "-----------------------------------------------------\n",
            "Epoch: 949\n",
            "\t Training loss 0.00234, Training accuracy 91.99\n",
            "\t Test loss 0.00302, Test accuracy 89.01\n",
            "-----------------------------------------------------\n",
            "Epoch: 950\n",
            "\t Training loss 0.00238, Training accuracy 90.50\n",
            "\t Test loss 0.00278, Test accuracy 90.18\n",
            "-----------------------------------------------------\n",
            "Epoch: 951\n",
            "\t Training loss 0.00225, Training accuracy 92.21\n",
            "\t Test loss 0.00275, Test accuracy 89.44\n",
            "-----------------------------------------------------\n",
            "Epoch: 952\n",
            "\t Training loss 0.00221, Training accuracy 92.02\n",
            "\t Test loss 0.00309, Test accuracy 89.38\n",
            "-----------------------------------------------------\n",
            "Epoch: 953\n",
            "\t Training loss 0.00232, Training accuracy 91.51\n",
            "\t Test loss 0.00267, Test accuracy 90.49\n",
            "-----------------------------------------------------\n",
            "Epoch: 954\n",
            "\t Training loss 0.00229, Training accuracy 91.66\n",
            "\t Test loss 0.00274, Test accuracy 90.98\n",
            "-----------------------------------------------------\n",
            "Epoch: 955\n",
            "\t Training loss 0.00236, Training accuracy 91.69\n",
            "\t Test loss 0.00283, Test accuracy 87.83\n",
            "-----------------------------------------------------\n",
            "Epoch: 956\n",
            "\t Training loss 0.00229, Training accuracy 91.93\n",
            "\t Test loss 0.00262, Test accuracy 90.61\n",
            "-----------------------------------------------------\n",
            "Epoch: 957\n",
            "\t Training loss 0.00221, Training accuracy 91.72\n",
            "\t Test loss 0.00274, Test accuracy 90.55\n",
            "-----------------------------------------------------\n",
            "Epoch: 958\n",
            "\t Training loss 0.00221, Training accuracy 91.48\n",
            "\t Test loss 0.00297, Test accuracy 90.18\n",
            "-----------------------------------------------------\n",
            "Epoch: 959\n",
            "\t Training loss 0.00237, Training accuracy 91.84\n",
            "\t Test loss 0.00267, Test accuracy 89.81\n",
            "-----------------------------------------------------\n",
            "Epoch: 960\n",
            "\t Training loss 0.00224, Training accuracy 92.21\n",
            "\t Test loss 0.00319, Test accuracy 87.52\n",
            "-----------------------------------------------------\n",
            "Epoch: 961\n",
            "\t Training loss 0.00235, Training accuracy 92.12\n",
            "\t Test loss 0.00296, Test accuracy 90.18\n",
            "-----------------------------------------------------\n",
            "Epoch: 962\n",
            "\t Training loss 0.00229, Training accuracy 91.26\n",
            "\t Test loss 0.00258, Test accuracy 90.67\n",
            "-----------------------------------------------------\n",
            "Epoch: 963\n",
            "\t Training loss 0.00227, Training accuracy 92.15\n",
            "\t Test loss 0.00258, Test accuracy 90.55\n",
            "-----------------------------------------------------\n",
            "Epoch: 964\n",
            "\t Training loss 0.00240, Training accuracy 91.32\n",
            "\t Test loss 0.00287, Test accuracy 89.87\n",
            "-----------------------------------------------------\n",
            "Epoch: 965\n",
            "\t Training loss 0.00227, Training accuracy 91.84\n",
            "\t Test loss 0.00267, Test accuracy 90.30\n",
            "-----------------------------------------------------\n",
            "Epoch: 966\n",
            "\t Training loss 0.00221, Training accuracy 91.66\n",
            "\t Test loss 0.00295, Test accuracy 89.38\n",
            "-----------------------------------------------------\n",
            "Epoch: 967\n",
            "\t Training loss 0.00231, Training accuracy 91.35\n",
            "\t Test loss 0.00279, Test accuracy 89.50\n",
            "-----------------------------------------------------\n",
            "Epoch: 968\n",
            "\t Training loss 0.00234, Training accuracy 91.05\n",
            "\t Test loss 0.00314, Test accuracy 89.44\n",
            "-----------------------------------------------------\n",
            "Epoch: 969\n",
            "\t Training loss 0.00222, Training accuracy 92.18\n",
            "\t Test loss 0.00257, Test accuracy 90.92\n",
            "-----------------------------------------------------\n",
            "Epoch: 970\n",
            "\t Training loss 0.00233, Training accuracy 91.35\n",
            "\t Test loss 0.00264, Test accuracy 90.36\n",
            "-----------------------------------------------------\n",
            "Epoch: 971\n",
            "\t Training loss 0.00237, Training accuracy 90.53\n",
            "\t Test loss 0.00289, Test accuracy 89.68\n",
            "-----------------------------------------------------\n",
            "Epoch: 972\n",
            "\t Training loss 0.00230, Training accuracy 91.69\n",
            "\t Test loss 0.00248, Test accuracy 89.93\n",
            "-----------------------------------------------------\n",
            "Epoch: 973\n",
            "\t Training loss 0.00217, Training accuracy 91.78\n",
            "\t Test loss 0.00262, Test accuracy 91.04\n",
            "-----------------------------------------------------\n",
            "Epoch: 974\n",
            "\t Training loss 0.00223, Training accuracy 91.90\n",
            "\t Test loss 0.00295, Test accuracy 89.44\n",
            "-----------------------------------------------------\n",
            "Epoch: 975\n",
            "\t Training loss 0.00255, Training accuracy 91.32\n",
            "\t Test loss 0.00264, Test accuracy 90.67\n",
            "-----------------------------------------------------\n",
            "Epoch: 976\n",
            "\t Training loss 0.00224, Training accuracy 92.45\n",
            "\t Test loss 0.00297, Test accuracy 90.06\n",
            "-----------------------------------------------------\n",
            "Epoch: 977\n",
            "\t Training loss 0.00232, Training accuracy 91.60\n",
            "\t Test loss 0.00262, Test accuracy 91.54\n",
            "-----------------------------------------------------\n",
            "Epoch: 978\n",
            "\t Training loss 0.00229, Training accuracy 91.84\n",
            "\t Test loss 0.00264, Test accuracy 89.99\n",
            "-----------------------------------------------------\n",
            "Epoch: 979\n",
            "\t Training loss 0.00225, Training accuracy 91.42\n",
            "\t Test loss 0.00275, Test accuracy 90.43\n",
            "-----------------------------------------------------\n",
            "Epoch: 980\n",
            "\t Training loss 0.00217, Training accuracy 92.85\n",
            "\t Test loss 0.00291, Test accuracy 89.25\n",
            "-----------------------------------------------------\n",
            "Epoch: 981\n",
            "\t Training loss 0.00224, Training accuracy 91.57\n",
            "\t Test loss 0.00317, Test accuracy 88.45\n",
            "-----------------------------------------------------\n",
            "Epoch: 982\n",
            "\t Training loss 0.00241, Training accuracy 91.35\n",
            "\t Test loss 0.00307, Test accuracy 88.76\n",
            "-----------------------------------------------------\n",
            "Epoch: 983\n",
            "\t Training loss 0.00237, Training accuracy 90.96\n",
            "\t Test loss 0.00321, Test accuracy 88.26\n",
            "-----------------------------------------------------\n",
            "Epoch: 984\n",
            "\t Training loss 0.00240, Training accuracy 90.99\n",
            "\t Test loss 0.00281, Test accuracy 89.93\n",
            "-----------------------------------------------------\n",
            "Epoch: 985\n",
            "\t Training loss 0.00224, Training accuracy 91.81\n",
            "\t Test loss 0.00266, Test accuracy 91.60\n",
            "-----------------------------------------------------\n",
            "Epoch: 986\n",
            "\t Training loss 0.00223, Training accuracy 92.18\n",
            "\t Test loss 0.00279, Test accuracy 91.23\n",
            "-----------------------------------------------------\n",
            "Epoch: 987\n",
            "\t Training loss 0.00227, Training accuracy 92.21\n",
            "\t Test loss 0.00313, Test accuracy 90.12\n",
            "-----------------------------------------------------\n",
            "Epoch: 988\n",
            "\t Training loss 0.00231, Training accuracy 91.17\n",
            "\t Test loss 0.00284, Test accuracy 90.24\n",
            "-----------------------------------------------------\n",
            "Epoch: 989\n",
            "\t Training loss 0.00240, Training accuracy 91.23\n",
            "\t Test loss 0.00302, Test accuracy 88.94\n",
            "-----------------------------------------------------\n",
            "Epoch: 990\n",
            "\t Training loss 0.00242, Training accuracy 91.17\n",
            "\t Test loss 0.00298, Test accuracy 90.18\n",
            "-----------------------------------------------------\n",
            "Epoch: 991\n",
            "\t Training loss 0.00236, Training accuracy 91.45\n",
            "\t Test loss 0.00272, Test accuracy 89.44\n",
            "-----------------------------------------------------\n",
            "Epoch: 992\n",
            "\t Training loss 0.00217, Training accuracy 92.15\n",
            "\t Test loss 0.00295, Test accuracy 89.13\n",
            "-----------------------------------------------------\n",
            "Epoch: 993\n",
            "\t Training loss 0.00221, Training accuracy 91.93\n",
            "\t Test loss 0.00293, Test accuracy 89.38\n",
            "-----------------------------------------------------\n",
            "Epoch: 994\n",
            "\t Training loss 0.00228, Training accuracy 91.51\n",
            "\t Test loss 0.00324, Test accuracy 88.39\n",
            "-----------------------------------------------------\n",
            "Epoch: 995\n",
            "\t Training loss 0.00241, Training accuracy 91.57\n",
            "\t Test loss 0.00270, Test accuracy 90.06\n",
            "-----------------------------------------------------\n",
            "Epoch: 996\n",
            "\t Training loss 0.00238, Training accuracy 91.45\n",
            "\t Test loss 0.00301, Test accuracy 89.38\n",
            "-----------------------------------------------------\n",
            "Epoch: 997\n",
            "\t Training loss 0.00223, Training accuracy 92.05\n",
            "\t Test loss 0.00261, Test accuracy 91.11\n",
            "-----------------------------------------------------\n",
            "Epoch: 998\n",
            "\t Training loss 0.00217, Training accuracy 92.09\n",
            "\t Test loss 0.00259, Test accuracy 90.18\n",
            "-----------------------------------------------------\n",
            "Epoch: 999\n",
            "\t Training loss 0.00234, Training accuracy 91.45\n",
            "\t Test loss 0.00298, Test accuracy 90.24\n",
            "-----------------------------------------------------\n",
            "Epoch: 1000\n",
            "\t Training loss 0.00219, Training accuracy 92.72\n",
            "\t Test loss 0.00276, Test accuracy 89.75\n",
            "-----------------------------------------------------\n",
            "After training:\n",
            "\t Training loss 0.00062, Training accuracy 98.63\n",
            "\t Test loss 0.00276, Test accuracy 89.75\n",
            "-----------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SPIu5kykcNeY",
        "colab_type": "text"
      },
      "source": [
        "### Accuracy plot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dRnpmiQFaRQV",
        "colab_type": "code",
        "outputId": "03a4be11-833a-45ae-e0cc-853e311ebb82",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "x = range(0,num_epochs +1)\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "\n",
        "plt.plot(x, train_accuracy_list, label='Train Accuracy')\n",
        "plt.plot(x, test_accuracy_list, label='Test Accuracy')\n",
        "\n",
        "plt.xlabel('N epochs')\n",
        "plt.ylabel('Accuracy [%]')\n",
        "plt.title(\"Accuracy Plot\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "#plt.savefig('accuracy.png', format='png', dpi=1000)\n",
        "\n",
        "# Do the plot code\n",
        "fig.savefig('accuracy.png', format='png', dpi=1000)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngZlZxbkQmJlV3P8AqWOKHIiL4xAAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fndcel9EcRrz",
        "colab_type": "text"
      },
      "source": [
        "### Loss Plot\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oR3XyS-DcVYo",
        "colab_type": "code",
        "outputId": "ffea2cf8-4db6-4415-8181-f0e2fcd20a2d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "fig, ax = plt.subplots()\n",
        "plt.plot(x, train_loss_list, label='Train Loss')\n",
        "plt.plot(x, test_loss_list, label='Test Loss')\n",
        "\n",
        "plt.xlabel('N epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title(\"Loss Plot\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "fig.savefig('loss.png', format='png', dpi=1000)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZIAAAEWCAYAAABMoxE0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmJmZpX8F7OkjYFiJCsXAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pXI3rKb_Co0x",
        "colab_type": "text"
      },
      "source": [
        "## Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-57nC0ppCOm-",
        "colab_type": "code",
        "outputId": "6e0d1616-82be-46a2-9a18-83d1c0b506cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        }
      },
      "source": [
        "'''test_loader_copy = test_loader\n",
        "#val_loader_copy = val_loader\n",
        "train_loader_copy = train_loader'''\n",
        "net_copy = net\n",
        "samples = 0.\n",
        "cumulative_loss = 0.\n",
        "cumulative_accuracy = 0.\n",
        "\n",
        "net_copy.eval() # Strictly needed if network contains layers which has different behaviours between train and tes\n",
        "predicted_label = list()\n",
        "real_label = list()\n",
        "#imgPath = list()\n",
        "with torch.no_grad():\n",
        "    for batch_idx, (inputs, targets) in enumerate(test_loader):\n",
        "        # Load data into GPU\n",
        "        inputs = inputs.to(device)\n",
        "        targets = targets.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = net_copy(inputs)\n",
        "\n",
        "        # Apply the loss\n",
        "        loss = cost_function(outputs, targets)\n",
        "\n",
        "        # Better print something\n",
        "        samples+=inputs.shape[0]\n",
        "        cumulative_loss += loss.item() # Note: the .item() is needed to extract scalars from tensors\n",
        "        _, predicted = outputs.max(1)\n",
        "        predicted_label.extend(predicted.tolist())\n",
        "        real_label.extend(targets.tolist())\n",
        "        cumulative_accuracy += predicted.eq(targets).sum().item()\n",
        "        \n",
        "print(\"Final Accuracy: \", cumulative_accuracy/samples*100, \"%\")\n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Final Accuracy:  89.7467572575664 %\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:123: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:53: UserWarning: torch.range is deprecated in favor of torch.arange and will be removed in 0.5. Note that arange generates values in [start; end), not [start; end].\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:72: UserWarning: torch.range is deprecated in favor of torch.arange and will be removed in 0.5. Note that arange generates values in [start; end), not [start; end].\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V2__Lk3LCOSs",
        "colab_type": "code",
        "outputId": "b700a497-8424-40fe-f731-7e10439de48b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        }
      },
      "source": [
        "#print(__doc__)\n",
        "from sklearn.utils.multiclass import unique_labels\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "\n",
        "#Classes to be predicted\n",
        "class_names = ['neutral','calm','happy', 'sad','angry','fearful','disgust','surprised']\n",
        "\n",
        "def plot_confusion_matrix(y_true, y_pred, classes,\n",
        "                          normalize=False,\n",
        "                          title=None,\n",
        "                          cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    if not title:\n",
        "        if normalize:\n",
        "            title = 'Normalized confusion matrix'\n",
        "        else:\n",
        "            title = 'Confusion matrix, without normalization'\n",
        "\n",
        "    # Compute confusion matrix\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    # Only use the labels that appear in the data\n",
        "\n",
        "    fig, ax = plt.subplots()\n",
        "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    ax.figure.colorbar(im, ax=ax)\n",
        "    # We want to show all ticks...\n",
        "    ax.set(xticks=np.arange(cm.shape[1]),\n",
        "           yticks=np.arange(cm.shape[0]),\n",
        "           # ... and label them with the respective list entries\n",
        "           xticklabels=classes, yticklabels=classes,\n",
        "           title=title,\n",
        "           ylabel='True label',\n",
        "           xlabel='Predicted label')\n",
        "\n",
        "    # Rotate the tick labels and set their alignment.\n",
        "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
        "             rotation_mode=\"anchor\")\n",
        "\n",
        "    # Loop over data dimensions and create text annotations.\n",
        "    fmt = '.2f' if normalize else 'd'\n",
        "    thresh = cm.max() / 2.\n",
        "    for i in range(cm.shape[0]):\n",
        "        for j in range(cm.shape[1]):\n",
        "            ax.text(j, i, format(cm[i, j], fmt),\n",
        "                    ha=\"center\", va=\"center\",\n",
        "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "    fig.tight_layout()\n",
        "    return ax\n",
        "\n",
        "\n",
        "np.set_printoptions(precision=2)\n",
        "\n",
        "print(\"Test items: \", len(predicted_label))\n",
        "\n",
        "# Plot confusion matrix\n",
        "plot_confusion_matrix(real_label,predicted_label, classes=class_names,\n",
        "                      title='Confusion matrix, without normalization')\n",
        "plt.show()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test items:  1619\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUYAAAEYCAYAAAAgU193AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}